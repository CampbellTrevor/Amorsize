{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning with Amorsize\n",
    "\n",
    "This notebook demonstrates how to use Amorsize's parameter tuning features to find optimal parallelization parameters through empirical benchmarking.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you've completed **Getting Started** (01_getting_started.ipynb) first!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Grid Search Tuning** - Systematically test parameter combinations\n",
    "2. **Quick Tuning** - Fast tuning with minimal configurations\n",
    "3. **Bayesian Optimization** - Intelligent parameter search\n",
    "4. **Configuration Management** - Save and reuse optimal parameters\n",
    "5. **Comparison Analysis** - Validate tuning vs optimizer recommendations\n",
    "6. **Real-World Workflow** - Complete tuning pipeline for production\n",
    "\n",
    "## When to Use Parameter Tuning\n",
    "\n",
    "- **Production workloads**: When you need guaranteed optimal performance\n",
    "- **Repeated execution**: When the function runs many times with similar data\n",
    "- **Validation**: To verify optimizer recommendations empirically\n",
    "- **Edge cases**: When optimizer can't sample (e.g., side effects, databases)\n",
    "\n",
    "**Note**: Tuning requires actual execution, so it takes longer than `optimize()`. Use when the extra time investment pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from amorsize import (\n",
    "    optimize,\n",
    "    tune_parameters,\n",
    "    quick_tune,\n",
    "    bayesian_tune_parameters,\n",
    "    save_config,\n",
    "    load_config\n",
    ")\n",
    "\n",
    "# For visualization (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    HAS_MATPLOTLIB = False\n",
    "    print(\"Matplotlib not available - skipping visualizations\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Grid Search Tuning\n",
    "\n",
    "Grid search systematically tests every combination of parameters to find the empirically best configuration.\n",
    "\n",
    "### Example: CPU-Intensive Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CPU-intensive function\n",
    "def cpu_intensive_task(x):\n",
    "    \"\"\"Simulate expensive computation.\"\"\"\n",
    "    result = 0\n",
    "    for i in range(1000):\n",
    "        result += x ** 2\n",
    "    return result\n",
    "\n",
    "# Create test data\n",
    "data = list(range(100, 300))\n",
    "\n",
    "# Run grid search tuning\n",
    "print(\"Running grid search tuning...\")\n",
    "result = tune_parameters(\n",
    "    cpu_intensive_task,\n",
    "    data,\n",
    "    n_jobs_range=[1, 2, 4],          # Test 1, 2, and 4 workers\n",
    "    chunksize_range=[20, 50, 100],   # Test 3 chunk sizes\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics\n",
    "print(f\"Best configuration found:\")\n",
    "print(f\"  n_jobs: {result.best_n_jobs}\")\n",
    "print(f\"  chunksize: {result.best_chunksize}\")\n",
    "print(f\"  Speedup: {result.best_speedup:.2f}x\")\n",
    "print(f\"  Execution time: {result.best_time:.4f}s\")\n",
    "print(f\"\\nConfigurations tested: {result.configurations_tested}\")\n",
    "\n",
    "# Show top 5 configurations\n",
    "print(\"\\nTop 5 configurations:\")\n",
    "for i, (n_jobs, chunksize, exec_time, speedup) in enumerate(result.get_top_configurations(5), 1):\n",
    "    print(f\"  {i}. n_jobs={n_jobs:2d}, chunksize={chunksize:4d} -> \"\n",
    "          f\"{exec_time:.4f}s ({speedup:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MATPLOTLIB:\n",
    "    # Create heatmap of execution times\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract unique n_jobs and chunksizes\n",
    "    configs = list(result.all_results.keys())\n",
    "    n_jobs_vals = sorted(set(c[0] for c in configs))\n",
    "    chunksize_vals = sorted(set(c[1] for c in configs))\n",
    "    \n",
    "    # Create matrix of execution times\n",
    "    times_matrix = np.zeros((len(chunksize_vals), len(n_jobs_vals)))\n",
    "    for i, cs in enumerate(chunksize_vals):\n",
    "        for j, nj in enumerate(n_jobs_vals):\n",
    "            times_matrix[i, j] = result.all_results.get((nj, cs), np.nan)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    im = ax.imshow(times_matrix, aspect='auto', cmap='RdYlGn_r')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(n_jobs_vals)))\n",
    "    ax.set_yticks(range(len(chunksize_vals)))\n",
    "    ax.set_xticklabels(n_jobs_vals)\n",
    "    ax.set_yticklabels(chunksize_vals)\n",
    "    ax.set_xlabel('n_jobs')\n",
    "    ax.set_ylabel('chunksize')\n",
    "    ax.set_title('Execution Time by Configuration (seconds)\\nDarker = Faster')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Execution Time (s)', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Annotate cells with values\n",
    "    for i in range(len(chunksize_vals)):\n",
    "        for j in range(len(n_jobs_vals)):\n",
    "            if not np.isnan(times_matrix[i, j]):\n",
    "                text = ax.text(j, i, f'{times_matrix[i, j]:.3f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Heatmap shows execution time for each configuration\")\n",
    "    print(\"   Dark green = Fastest, Dark red = Slowest\")\n",
    "else:\n",
    "    print(\"Install matplotlib to see visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quick Tuning for Rapid Prototyping\n",
    "\n",
    "When you don't have time for exhaustive search, use `quick_tune()` to test a minimal set of likely-optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running quick tuning (minimal search)...\")\n",
    "quick_result = quick_tune(\n",
    "    cpu_intensive_task,\n",
    "    data,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(quick_result)\n",
    "\n",
    "# Compare with full grid search\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  Quick tune: {quick_result.configurations_tested} configs tested\")\n",
    "print(f\"  Grid search: {result.configurations_tested} configs tested\")\n",
    "print(f\"  Time saved: ~{(result.configurations_tested - quick_result.configurations_tested) * result.best_time:.2f}s\")\n",
    "print(f\"  Performance difference: {abs(quick_result.best_speedup - result.best_speedup):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Bayesian Optimization for Intelligent Search\n",
    "\n",
    "Bayesian optimization uses machine learning to intelligently explore the parameter space, finding good configurations with fewer trials than grid search.\n",
    "\n",
    "**Note**: Requires `scikit-optimize`. Install with: `pip install scikit-optimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Running Bayesian optimization...\")\n",
    "    bayesian_result = bayesian_tune_parameters(\n",
    "        cpu_intensive_task,\n",
    "        data,\n",
    "        n_iterations=15,  # Test 15 intelligently-chosen configurations\n",
    "        verbose=True,\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(bayesian_result)\n",
    "    \n",
    "    # Compare efficiency\n",
    "    print(\"\\nEfficiency Comparison:\")\n",
    "    print(f\"  Bayesian: {bayesian_result.configurations_tested} configs tested\")\n",
    "    print(f\"  Grid search: {result.configurations_tested} configs tested\")\n",
    "    print(f\"  Bayesian speedup found: {bayesian_result.best_speedup:.2f}x\")\n",
    "    print(f\"  Grid search speedup: {result.best_speedup:.2f}x\")\n",
    "    print(f\"  Efficiency: {bayesian_result.best_speedup / bayesian_result.configurations_tested:.3f}x per config\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  scikit-optimize not installed. Skipping Bayesian optimization demo.\")\n",
    "    print(\"   Install with: pip install scikit-optimize\")\n",
    "    print(\"   Falls back to grid search automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparing Tuning with Optimizer Recommendations\n",
    "\n",
    "The optimizer (`optimize()`) makes predictions without running your function. Let's see how tuning compares!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimizer recommendation\n",
    "print(\"Getting optimizer recommendation...\")\n",
    "opt_result = optimize(cpu_intensive_task, data, verbose=False)\n",
    "\n",
    "print(\"\\nOptimizer Recommendation:\")\n",
    "print(f\"  n_jobs: {opt_result.n_jobs}\")\n",
    "print(f\"  chunksize: {opt_result.chunksize}\")\n",
    "print(f\"  Predicted speedup: {opt_result.estimated_speedup:.2f}x\")\n",
    "\n",
    "print(\"\\nTuning Results:\")\n",
    "print(f\"  n_jobs: {result.best_n_jobs}\")\n",
    "print(f\"  chunksize: {result.best_chunksize}\")\n",
    "print(f\"  Actual speedup: {result.best_speedup:.2f}x\")\n",
    "\n",
    "# Check if they match\n",
    "if opt_result.n_jobs == result.best_n_jobs and opt_result.chunksize == result.best_chunksize:\n",
    "    print(\"\\n‚úÖ Optimizer recommendation matches tuning results!\")\n",
    "    print(\"   The optimizer is highly accurate for this workload.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Optimizer recommendation differs from tuning.\")\n",
    "    print(\"   This can happen when:\")\n",
    "    print(\"   - Workload has unusual characteristics\")\n",
    "    print(\"   - System has special constraints\")\n",
    "    print(\"   - Function has side effects\")\n",
    "    speedup_diff = abs(opt_result.estimated_speedup - result.best_speedup)\n",
    "    if speedup_diff < 0.5:\n",
    "        print(f\"   Prediction error is small ({speedup_diff:.2f}x), both are good choices.\")\n",
    "    else:\n",
    "        print(f\"   Significant difference ({speedup_diff:.2f}x), use tuning results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Configuration Management\n",
    "\n",
    "Save optimal parameters for reuse in production, avoiding repeated tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary config file\n",
    "config_path = os.path.join(tempfile.gettempdir(), 'amorsize_tuned_config.json')\n",
    "\n",
    "# Save the best configuration\n",
    "print(\"Saving optimal configuration...\")\n",
    "result.save_config(\n",
    "    config_path,\n",
    "    function_name='cpu_intensive_task',\n",
    "    notes='Optimal config found via grid search tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "\n",
    "# Load and use the configuration\n",
    "print(\"\\nLoading configuration...\")\n",
    "config = load_config(config_path)\n",
    "print(f\"Loaded config: n_jobs={config.n_jobs}, chunksize={config.chunksize}\")\n",
    "\n",
    "# Use in production\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_with_saved_config(func, data, config_path):\n",
    "    \"\"\"Production pattern: Load config and execute.\"\"\"\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    with Pool(config.n_jobs) as pool:\n",
    "        results = pool.map(func, data, chunksize=config.chunksize)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n‚úÖ Configuration ready for production use!\")\n",
    "print(\"   Load once at startup, reuse for all executions.\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(config_path):\n",
    "    os.remove(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Tuning Patterns\n",
    "\n",
    "### Pattern 1: Tuning for Different Workload Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how optimal parameters change with data size\n",
    "print(\"Testing parameter scaling across workload sizes...\\n\")\n",
    "\n",
    "workload_sizes = [50, 100, 200, 500]\n",
    "tuning_results = {}\n",
    "\n",
    "for size in workload_sizes:\n",
    "    test_data = list(range(100, 100 + size))\n",
    "    tune_result = quick_tune(cpu_intensive_task, test_data, verbose=False)\n",
    "    tuning_results[size] = tune_result\n",
    "    \n",
    "    print(f\"Size {size:4d}: n_jobs={tune_result.best_n_jobs}, \"\n",
    "          f\"chunksize={tune_result.best_chunksize:4d}, \"\n",
    "          f\"speedup={tune_result.best_speedup:.2f}x\")\n",
    "\n",
    "print(\"\\nüí° Insight: Larger workloads often benefit from more workers and larger chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Tuning I/O-Bound Tasks with Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define I/O-bound function\n",
    "def io_bound_task(x):\n",
    "    \"\"\"Simulate I/O wait (network, disk, etc.).\"\"\"\n",
    "    time.sleep(0.01)  # 10ms wait\n",
    "    return x * 2\n",
    "\n",
    "io_data = list(range(50))\n",
    "\n",
    "print(\"Tuning I/O-bound task with threads...\")\n",
    "io_result = quick_tune(\n",
    "    io_bound_task,\n",
    "    io_data,\n",
    "    prefer_threads_for_io=True,  # Use threads instead of processes\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(io_result)\n",
    "print(\"\\nüí° Insight: I/O-bound tasks typically benefit from more workers (threads).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Production Workflow\n",
    "\n",
    "Here's a complete example of a production tuning workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_tuning_workflow(func, data, config_dir='/tmp'):\n",
    "    \"\"\"\n",
    "    Complete tuning workflow for production deployment.\n",
    "    \n",
    "    Steps:\n",
    "    1. Quick tune to get initial parameters\n",
    "    2. Validate with optimizer\n",
    "    3. Fine-tune if needed\n",
    "    4. Save configuration\n",
    "    5. Return ready-to-use config\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRODUCTION TUNING WORKFLOW\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Quick tune\n",
    "    print(\"\\n[1/5] Running quick tune...\")\n",
    "    quick_result = quick_tune(func, data, verbose=False)\n",
    "    print(f\"      Quick tune found: n_jobs={quick_result.best_n_jobs}, \"\n",
    "          f\"chunksize={quick_result.best_chunksize} ({quick_result.best_speedup:.2f}x)\")\n",
    "    \n",
    "    # Step 2: Get optimizer recommendation\n",
    "    print(\"\\n[2/5] Getting optimizer recommendation...\")\n",
    "    opt_result = optimize(func, data, verbose=False)\n",
    "    print(f\"      Optimizer suggests: n_jobs={opt_result.n_jobs}, \"\n",
    "          f\"chunksize={opt_result.chunksize} ({opt_result.estimated_speedup:.2f}x predicted)\")\n",
    "    \n",
    "    # Step 3: Decide if fine-tuning needed\n",
    "    print(\"\\n[3/5] Validating results...\")\n",
    "    configs_match = (quick_result.best_n_jobs == opt_result.n_jobs and \n",
    "                    quick_result.best_chunksize == opt_result.chunksize)\n",
    "    \n",
    "    if configs_match:\n",
    "        print(\"      ‚úÖ Quick tune matches optimizer - high confidence\")\n",
    "        final_result = quick_result\n",
    "    else:\n",
    "        print(\"      ‚ö†Ô∏è  Differences detected - running fine-tuning...\")\n",
    "        # Fine-tune around both suggestions\n",
    "        n_jobs_range = sorted(set([quick_result.best_n_jobs, opt_result.n_jobs]))\n",
    "        chunksize_range = sorted(set([quick_result.best_chunksize, opt_result.chunksize]))\n",
    "        \n",
    "        final_result = tune_parameters(\n",
    "            func, data,\n",
    "            n_jobs_range=n_jobs_range,\n",
    "            chunksize_range=chunksize_range,\n",
    "            verbose=False\n",
    "        )\n",
    "        print(f\"      Fine-tuning complete: {final_result.best_speedup:.2f}x\")\n",
    "    \n",
    "    # Step 4: Save configuration\n",
    "    print(\"\\n[4/5] Saving configuration...\")\n",
    "    config_path = os.path.join(config_dir, 'production_config.json')\n",
    "    final_result.save_config(\n",
    "        config_path,\n",
    "        function_name=func.__name__,\n",
    "        notes='Production-tuned configuration',\n",
    "        overwrite=True\n",
    "    )\n",
    "    print(f\"      Saved to: {config_path}\")\n",
    "    \n",
    "    # Step 5: Summary\n",
    "    print(\"\\n[5/5] Summary\")\n",
    "    print(f\"      Final configuration: n_jobs={final_result.best_n_jobs}, \"\n",
    "          f\"chunksize={final_result.best_chunksize}\")\n",
    "    print(f\"      Expected speedup: {final_result.best_speedup:.2f}x\")\n",
    "    print(f\"      Configurations tested: {final_result.configurations_tested}\")\n",
    "    print(f\"\\n‚úÖ Ready for production!\")\n",
    "    \n",
    "    return final_result, config_path\n",
    "\n",
    "# Run the workflow\n",
    "final_result, config_path = production_tuning_workflow(cpu_intensive_task, data)\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(config_path):\n",
    "    os.remove(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Performance Visualization\n",
    "\n",
    "Compare speedup across different configurations visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MATPLOTLIB:\n",
    "    # Get top configurations\n",
    "    top_configs = result.get_top_configurations(10)\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    config_labels = [f\"n={nj}, c={cs}\" for nj, cs, _, _ in top_configs]\n",
    "    speedups = [speedup for _, _, _, speedup in top_configs]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(range(len(config_labels)), speedups, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Highlight best configuration\n",
    "    bars[0].set_color('green')\n",
    "    bars[0].set_alpha(0.9)\n",
    "    \n",
    "    ax.set_xlabel('Configuration (n_jobs, chunksize)')\n",
    "    ax.set_ylabel('Speedup (x)')\n",
    "    ax.set_title('Top 10 Configurations by Speedup')\n",
    "    ax.set_xticks(range(len(config_labels)))\n",
    "    ax.set_xticklabels(config_labels, rotation=45, ha='right')\n",
    "    ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Serial baseline')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Chart shows speedup for top 10 configurations\")\n",
    "    print(\"   Green bar = Best configuration\")\n",
    "else:\n",
    "    print(\"Install matplotlib to see visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "1. **`optimize()`** - Default choice\n",
    "   - Fast (no actual execution)\n",
    "   - Good for most workloads\n",
    "   - Use when: Quick decisions needed, workload is typical\n",
    "\n",
    "2. **`quick_tune()`** - Rapid validation\n",
    "   - Minimal configurations tested (~3-5)\n",
    "   - Good balance of speed and accuracy\n",
    "   - Use when: Need empirical validation but time is limited\n",
    "\n",
    "3. **`tune_parameters()`** - Thorough search\n",
    "   - Exhaustive grid search\n",
    "   - Guaranteed to find best in search space\n",
    "   - Use when: Production workload, need confidence\n",
    "\n",
    "4. **`bayesian_tune_parameters()`** - Intelligent search\n",
    "   - ML-guided exploration\n",
    "   - Efficient for large search spaces\n",
    "   - Use when: Many parameters to tune, expensive benchmarks\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with optimizer**: Use `optimize()` for quick decisions\n",
    "2. **Validate if critical**: Use `quick_tune()` to verify optimizer recommendations\n",
    "3. **Save configurations**: Store tuned parameters for reuse\n",
    "4. **Re-tune periodically**: When data characteristics change significantly\n",
    "5. **Consider cost**: Tuning takes time - use when repeated execution justifies it\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Development: Quick optimization\n",
    "result = optimize(func, data)\n",
    "\n",
    "# Pre-production: Validate with quick tune\n",
    "result = quick_tune(func, data)\n",
    "\n",
    "# Production: Full tuning + save config\n",
    "result = tune_parameters(func, data, ...)\n",
    "result.save_config('production.json')\n",
    "\n",
    "# Production runtime: Load saved config\n",
    "config = load_config('production.json')\n",
    "with Pool(config.n_jobs) as pool:\n",
    "    results = pool.map(func, data, chunksize=config.chunksize)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Use Case Notebooks**: Apply tuning to specific domains (web, data, ML)\n",
    "- **Advanced Features**: Explore hooks, monitoring, checkpointing\n",
    "- **Production Deployment**: Integrate with your workflow\n",
    "\n",
    "## Need Help?\n",
    "\n",
    "- **Documentation**: See `docs/` directory\n",
    "- **Examples**: Check `examples/` for more patterns\n",
    "- **Issues**: Report bugs on GitHub\n",
    "\n",
    "Happy tuning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

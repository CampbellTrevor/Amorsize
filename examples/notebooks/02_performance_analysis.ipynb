{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis Deep Dive\n",
    "\n",
    "Welcome to the **Amorsize Performance Analysis** notebook! \ud83d\ude80\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "- \ud83d\udd0d **Identify performance bottlenecks** using diagnostic profiling\n",
    "- \ud83d\udcca **Visualize overhead breakdown** and understand where time is spent\n",
    "- \ud83d\udcc8 **Monitor execution** in real-time using hooks\n",
    "- \u26a1 **Optimize based on insights** from bottleneck analysis\n",
    "- \ud83c\udfaf **Compare different strategies** and measure their impact\n",
    "\n",
    "This notebook goes beyond basic optimization to help you **understand why** certain configurations work better and **how to diagnose** performance issues in your own workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed `01_getting_started.ipynb` first. This notebook assumes familiarity with:\n",
    "- Basic `optimize()` usage\n",
    "- Understanding speedup and overhead concepts\n",
    "- Reading optimization results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import everything we need and set up visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Import Amorsize\n",
    "from amorsize import (\n",
    "    optimize,\n",
    "    execute,\n",
    "    analyze_bottlenecks,\n",
    "    format_bottleneck_report,\n",
    "    HookManager,\n",
    "    HookEvent,\n",
    "    create_progress_hook,\n",
    "    create_timing_hook,\n",
    "    create_throughput_hook,\n",
    ")\n",
    "\n",
    "# Configure matplotlib for better looking plots\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"\u2713 All imports successful!\")\n",
    "print(\"Ready to analyze performance! \ud83d\ude80\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Diagnostic Profiling\n",
    "\n",
    "Diagnostic profiling gives you **transparency** into the optimizer's decision-making process. It answers questions like:\n",
    "- Why did it choose `n_jobs=4` instead of `8`?\n",
    "- What's limiting my speedup?\n",
    "- Is spawn overhead, IPC, or something else the bottleneck?\n",
    "\n",
    "### 1.1 Basic Diagnostic Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_bound_task(n):\n",
    "    \"\"\"A typical CPU-intensive computation.\"\"\"\n",
    "    result = 0\n",
    "    for i in range(5000):\n",
    "        result += n ** 2 + i ** 0.5\n",
    "    return result\n",
    "\n",
    "# Optimize with diagnostic profiling enabled\n",
    "data = range(200)\n",
    "result = optimize(\n",
    "    cpu_bound_task,\n",
    "    data,\n",
    "    sample_size=5,\n",
    "    profile=True,  # Enable diagnostic profiling\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Recommendation: n_jobs={result.n_jobs}, chunksize={result.chunksize}\")\n",
    "print(f\"Expected speedup: {result.estimated_speedup:.2f}x\")\n",
    "print(f\"\\nReason: {result.reason}\")\n",
    "\n",
    "# Show the detailed diagnostic report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED DIAGNOSTIC PROFILE\")\n",
    "print(\"=\"*80)\n",
    "print(result.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accessing Profile Data Programmatically\n",
    "\n",
    "The diagnostic profile contains **all the metrics** used in optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.profile:\n",
    "    p = result.profile\n",
    "    \n",
    "    print(\"\ud83d\udcca Sampling Results:\")\n",
    "    print(f\"  \u2022 Execution time per item: {p.avg_execution_time*1000:.3f} ms\")\n",
    "    print(f\"  \u2022 IPC overhead per item:   {p.avg_pickle_time*1000:.3f} ms\")\n",
    "    print(f\"  \u2022 Sample count:            {p.sample_count}\")\n",
    "    print(f\"  \u2022 Workload type:           {p.workload_type}\")\n",
    "    \n",
    "    print(\"\\n\u2699\ufe0f  System Information:\")\n",
    "    print(f\"  \u2022 Physical cores:          {p.physical_cores}\")\n",
    "    print(f\"  \u2022 Logical cores:           {p.logical_cores}\")\n",
    "    print(f\"  \u2022 Spawn cost per worker:   {p.spawn_cost*1000:.1f} ms\")\n",
    "    print(f\"  \u2022 Available memory:        {p.available_memory / (1024**3):.1f} GB\")\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf Optimization Decisions:\")\n",
    "    print(f\"  \u2022 Max workers (CPU):       {p.max_workers_cpu}\")\n",
    "    print(f\"  \u2022 Max workers (memory):    {p.max_workers_memory}\")\n",
    "    print(f\"  \u2022 Chosen workers:          {result.n_jobs}\")\n",
    "    print(f\"  \u2022 Optimal chunksize:       {p.optimal_chunksize}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc8 Performance Metrics:\")\n",
    "    print(f\"  \u2022 Theoretical max speedup: {p.theoretical_max_speedup:.2f}x\")\n",
    "    print(f\"  \u2022 Estimated speedup:       {p.estimated_speedup:.2f}x\")\n",
    "    print(f\"  \u2022 Parallel efficiency:     {p.speedup_efficiency*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Bottleneck Analysis\n",
    "\n",
    "**Bottleneck analysis** identifies what's preventing better performance. Let's explore different types of bottlenecks:\n",
    "\n",
    "### 2.1 Spawn Overhead Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_task(x):\n",
    "    \"\"\"Very fast task - spawn overhead will dominate.\"\"\"\n",
    "    return x * 2\n",
    "\n",
    "data = range(1000)\n",
    "result = optimize(fast_task, data, sample_size=10, profile=True, verbose=False)\n",
    "\n",
    "print(f\"Result: n_jobs={result.n_jobs}, speedup={result.estimated_speedup:.2f}x\")\n",
    "print(f\"\\nBottleneck Analysis:\")\n",
    "\n",
    "# Run bottleneck analysis\n",
    "ba = run_bottleneck_analysis(result)\n",
    "if ba:\n",
    "    print(f\"  \u2022 Primary bottleneck: {ba.primary_bottleneck.value}\")\n",
    "    print(f\"  \u2022 Severity: {ba.bottleneck_severity*100:.1f}%\")\n",
    "    print(f\"  \u2022 Efficiency score: {ba.efficiency_score*100:.1f}%\")\n",
    "    \n",
    "    if ba.recommendations:\n",
    "        print(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "        for rec in ba.recommendations:\n",
    "            print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IPC/Serialization Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_heavy_task(data_item):\n",
    "    \"\"\"Task with heavy data serialization.\"\"\"\n",
    "    # Simulate processing large data\n",
    "    large_list = list(range(1000))\n",
    "    return sum(large_list) + data_item\n",
    "\n",
    "data = range(100)\n",
    "result = optimize(data_heavy_task, data, sample_size=5, profile=True, verbose=False)\n",
    "\n",
    "print(f\"Result: n_jobs={result.n_jobs}, speedup={result.estimated_speedup:.2f}x\")\n",
    "\n",
    "if result.profile:\n",
    "    p = result.profile\n",
    "    print(f\"\\n\ud83d\udce6 Data Serialization:\")\n",
    "    print(f\"  \u2022 Data pickle time:   {p.avg_data_pickle_time*1000:.3f} ms\")\n",
    "    print(f\"  \u2022 Result pickle time: {p.avg_pickle_time*1000:.3f} ms\")\n",
    "    print(f\"  \u2022 Data size:          {p.data_size_bytes} bytes\")\n",
    "    print(f\"  \u2022 Return size:        {p.return_size_bytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing Overhead Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overhead_breakdown_chart(profile):\n",
    "    \"\"\"Create a pie chart showing overhead breakdown.\"\"\"\n",
    "    if not profile:\n",
    "        print(\"No profile data available\")\n",
    "        return\n",
    "    \n",
    "    breakdown = profile.get_overhead_breakdown()\n",
    "    \n",
    "    # Prepare data\n",
    "    labels = ['Spawn', 'IPC/Serialization', 'Chunking']\n",
    "    sizes = [breakdown['spawn'], breakdown['ipc'], breakdown['chunking']]\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    explode = (0.1, 0, 0)  # Explode spawn overhead slice\n",
    "    \n",
    "    # Create pie chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "           autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    ax.axis('equal')\n",
    "    ax.set_title('Parallelization Overhead Breakdown', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Overhead Breakdown:\")\n",
    "    for label, size in zip(labels, sizes):\n",
    "        print(f\"  \u2022 {label:20s}: {size:5.1f}%\")\n",
    "\n",
    "# Visualize for our CPU-bound example\n",
    "data = range(200)\n",
    "result = optimize(cpu_bound_task, data, sample_size=5, profile=True, verbose=False)\n",
    "create_overhead_breakdown_chart(result.profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Real-Time Monitoring with Hooks\n",
    "\n",
    "Hooks let you **monitor execution** in real-time, collect metrics, and integrate with external monitoring systems.\n",
    "\n",
    "### 3.1 Basic Progress Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(x):\n",
    "    \"\"\"Simple processing function.\"\"\"\n",
    "    time.sleep(0.01)  # Simulate work\n",
    "    return x ** 2\n",
    "\n",
    "# Create hook manager\n",
    "hooks = HookManager()\n",
    "\n",
    "# Track progress\n",
    "progress_data = []\n",
    "\n",
    "def track_progress(percent, completed, total):\n",
    "    progress_data.append((time.time(), percent, completed))\n",
    "    if completed % 20 == 0 or completed == total:\n",
    "        print(f\"Progress: {percent:5.1f}% ({completed:3d}/{total:3d} items)\")\n",
    "\n",
    "# Register progress hook\n",
    "progress_hook = create_progress_hook(track_progress, min_interval=0.0)\n",
    "hooks.register(HookEvent.POST_EXECUTE, progress_hook)\n",
    "\n",
    "# Execute with monitoring\n",
    "data = range(100)\n",
    "print(\"Starting execution with real-time monitoring...\\n\")\n",
    "results = execute(process_item, data, hooks=hooks, verbose=False)\n",
    "print(f\"\\n\u2713 Completed! Processed {len(results)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Collecting Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics collector\n",
    "metrics = {\n",
    "    'start_time': None,\n",
    "    'end_time': None,\n",
    "    'n_jobs': 0,\n",
    "    'chunksize': 0,\n",
    "    'total_items': 0,\n",
    "    'elapsed_time': 0,\n",
    "    'throughput': 0\n",
    "}\n",
    "\n",
    "hooks = HookManager()\n",
    "\n",
    "def collect_start_metrics(ctx):\n",
    "    metrics['start_time'] = ctx.timestamp\n",
    "    metrics['n_jobs'] = ctx.n_jobs\n",
    "    metrics['chunksize'] = ctx.chunksize\n",
    "    metrics['total_items'] = ctx.total_items\n",
    "    print(f\"Started: n_jobs={ctx.n_jobs}, chunksize={ctx.chunksize}, items={ctx.total_items}\")\n",
    "\n",
    "def collect_end_metrics(ctx):\n",
    "    metrics['end_time'] = ctx.timestamp\n",
    "    metrics['elapsed_time'] = ctx.elapsed_time\n",
    "    metrics['throughput'] = ctx.throughput_items_per_sec\n",
    "    print(f\"Completed in {ctx.elapsed_time:.2f}s\")\n",
    "    print(f\"Throughput: {ctx.throughput_items_per_sec:.1f} items/sec\")\n",
    "\n",
    "hooks.register(HookEvent.PRE_EXECUTE, collect_start_metrics)\n",
    "hooks.register(HookEvent.POST_EXECUTE, collect_end_metrics)\n",
    "\n",
    "# Execute with metrics collection\n",
    "data = range(150)\n",
    "results = execute(cpu_bound_task, data, hooks=hooks, verbose=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Collected Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if 'time' in key.lower() and isinstance(value, float) and value > 100:\n",
    "        continue  # Skip timestamp values\n",
    "    print(f\"  \u2022 {key:15s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Throughput Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare throughput for different worker counts\n",
    "throughput_results = []\n",
    "\n",
    "for n_jobs in [1, 2, 4, 8]:\n",
    "    hooks = HookManager()\n",
    "    throughput_value = [0]\n",
    "    \n",
    "    def capture_throughput(rate):\n",
    "        throughput_value[0] = rate\n",
    "    \n",
    "    hooks.register(HookEvent.POST_EXECUTE, create_throughput_hook(capture_throughput))\n",
    "    \n",
    "    data = range(200)\n",
    "    results = execute(cpu_bound_task, data, n_jobs=n_jobs, hooks=hooks, verbose=False)\n",
    "    \n",
    "    throughput_results.append((n_jobs, throughput_value[0]))\n",
    "    print(f\"n_jobs={n_jobs}: {throughput_value[0]:.1f} items/sec\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "n_jobs_list = [r[0] for r in throughput_results]\n",
    "throughput_list = [r[1] for r in throughput_results]\n",
    "\n",
    "ax.bar(n_jobs_list, throughput_list, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "ax.set_xlabel('Number of Workers (n_jobs)', fontsize=12)\n",
    "ax.set_ylabel('Throughput (items/sec)', fontsize=12)\n",
    "ax.set_title('Throughput vs Worker Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (n, t) in enumerate(throughput_results):\n",
    "    ax.text(n, t + max(throughput_list)*0.02, f'{t:.1f}', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparative Performance Analysis\n",
    "\n",
    "Let's compare different scenarios to understand what makes parallelization effective:\n",
    "\n",
    "### 4.1 Impact of Task Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_duration_task(n, duration_ms):\n",
    "    \"\"\"Task with configurable duration.\"\"\"\n",
    "    iterations = int(duration_ms * 100)\n",
    "    result = 0\n",
    "    for i in range(iterations):\n",
    "        result += n ** 2\n",
    "    return result\n",
    "\n",
    "# Test different task durations\n",
    "durations = [0.1, 0.5, 1.0, 5.0, 10.0]  # milliseconds\n",
    "speedup_results = []\n",
    "\n",
    "print(\"Testing different task durations...\\n\")\n",
    "for duration in durations:\n",
    "    data = range(100)\n",
    "    result = optimize(\n",
    "        lambda x: variable_duration_task(x, duration),\n",
    "        data,\n",
    "        sample_size=5,\n",
    "        profile=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    speedup_results.append((duration, result.estimated_speedup, result.n_jobs))\n",
    "    print(f\"Duration: {duration:5.1f}ms \u2192 Speedup: {result.estimated_speedup:5.2f}x (n_jobs={result.n_jobs})\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup vs Duration\n",
    "ax1.plot([r[0] for r in speedup_results], [r[1] for r in speedup_results], \n",
    "         'o-', linewidth=2, markersize=8, color='green')\n",
    "ax1.set_xlabel('Task Duration (ms)', fontsize=12)\n",
    "ax1.set_ylabel('Speedup', fontsize=12)\n",
    "ax1.set_title('Speedup vs Task Duration', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Optimal n_jobs vs Duration\n",
    "ax2.plot([r[0] for r in speedup_results], [r[2] for r in speedup_results], \n",
    "         's-', linewidth=2, markersize=8, color='blue')\n",
    "ax2.set_xlabel('Task Duration (ms)', fontsize=12)\n",
    "ax2.set_ylabel('Optimal n_jobs', fontsize=12)\n",
    "ax2.set_title('Optimal Workers vs Task Duration', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Insight: Longer tasks benefit more from parallelization!\")\n",
    "print(\"   Overhead becomes less significant relative to computation time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Workload Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different workload sizes\n",
    "workload_sizes = [50, 100, 200, 500, 1000]\n",
    "size_results = []\n",
    "\n",
    "print(\"Testing different workload sizes...\\n\")\n",
    "for size in workload_sizes:\n",
    "    data = range(size)\n",
    "    result = optimize(\n",
    "        cpu_bound_task,\n",
    "        data,\n",
    "        sample_size=5,\n",
    "        profile=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    size_results.append((size, result.estimated_speedup, result.chunksize))\n",
    "    print(f\"Size: {size:4d} \u2192 Speedup: {result.estimated_speedup:5.2f}x (chunksize={result.chunksize})\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot([r[0] for r in size_results], [r[1] for r in size_results], \n",
    "        'o-', linewidth=2, markersize=8, color='purple', label='Speedup')\n",
    "ax.set_xlabel('Workload Size (items)', fontsize=12)\n",
    "ax.set_ylabel('Speedup', fontsize=12)\n",
    "ax.set_title('Speedup vs Workload Size', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Insight: Larger workloads allow for better parallelization!\")\n",
    "print(\"   More items = better amortization of spawn and setup costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced: Custom Monitoring Dashboard\n",
    "\n",
    "Let's build a **complete monitoring dashboard** that tracks multiple metrics in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceDashboard:\n",
    "    \"\"\"Real-time performance monitoring dashboard.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'execution_id': int(time.time()),\n",
    "            'status': 'initialized',\n",
    "            'n_jobs': 0,\n",
    "            'chunksize': 0,\n",
    "            'total_items': 0,\n",
    "            'start_time': 0,\n",
    "            'end_time': 0,\n",
    "            'duration': 0,\n",
    "            'throughput': 0,\n",
    "            'items_completed': 0\n",
    "        }\n",
    "        self.hooks = HookManager()\n",
    "        self._setup_hooks()\n",
    "    \n",
    "    def _setup_hooks(self):\n",
    "        \"\"\"Configure monitoring hooks.\"\"\"\n",
    "        self.hooks.register(HookEvent.PRE_EXECUTE, self._on_start)\n",
    "        self.hooks.register(HookEvent.POST_EXECUTE, self._on_complete)\n",
    "    \n",
    "    def _on_start(self, ctx):\n",
    "        \"\"\"Called when execution starts.\"\"\"\n",
    "        self.metrics['status'] = 'running'\n",
    "        self.metrics['n_jobs'] = ctx.n_jobs\n",
    "        self.metrics['chunksize'] = ctx.chunksize\n",
    "        self.metrics['total_items'] = ctx.total_items\n",
    "        self.metrics['start_time'] = ctx.timestamp\n",
    "    \n",
    "    def _on_complete(self, ctx):\n",
    "        \"\"\"Called when execution completes.\"\"\"\n",
    "        self.metrics['status'] = 'completed'\n",
    "        self.metrics['end_time'] = ctx.timestamp\n",
    "        self.metrics['duration'] = ctx.elapsed_time\n",
    "        self.metrics['throughput'] = ctx.throughput_items_per_sec\n",
    "        self.metrics['items_completed'] = ctx.items_completed\n",
    "    \n",
    "    def get_hooks(self):\n",
    "        \"\"\"Get the configured hooks.\"\"\"\n",
    "        return self.hooks\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard.\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Performance Dashboard - Execution #{self.metrics['execution_id']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Status:           {self.metrics['status'].upper()}\")\n",
    "        print(f\"Configuration:    n_jobs={self.metrics['n_jobs']}, chunksize={self.metrics['chunksize']}\")\n",
    "        print(f\"Workload:         {self.metrics['total_items']} items\")\n",
    "        if self.metrics['status'] == 'completed':\n",
    "            print(f\"Duration:         {self.metrics['duration']:.2f}s\")\n",
    "            print(f\"Throughput:       {self.metrics['throughput']:.1f} items/sec\")\n",
    "            print(f\"Items Completed:  {self.metrics['items_completed']}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Create and use dashboard\n",
    "dashboard = PerformanceDashboard()\n",
    "\n",
    "print(\"Starting monitored execution...\\n\")\n",
    "data = range(150)\n",
    "results = execute(cpu_bound_task, data, hooks=dashboard.get_hooks(), verbose=False)\n",
    "\n",
    "print(\"\\n\")\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Practical Optimization Workflow\n",
    "\n",
    "Let's put everything together in a **real-world optimization workflow**:\n",
    "\n",
    "### 6.1 Complete Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_optimize(func, data, sample_size=5):\n",
    "    \"\"\"Complete analysis and optimization workflow.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: Optimization with Diagnostic Profiling\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Optimize with profiling\n",
    "    result = optimize(func, data, sample_size=sample_size, profile=True, verbose=False)\n",
    "    \n",
    "    print(f\"\\nRecommendation: n_jobs={result.n_jobs}, chunksize={result.chunksize}\")\n",
    "    print(f\"Expected speedup: {result.estimated_speedup:.2f}x\")\n",
    "    print(f\"Reason: {result.reason}\")\n",
    "    \n",
    "    if result.warnings:\n",
    "        print(\"\\n\u26a0\ufe0f  Warnings:\")\n",
    "        for warning in result.warnings:\n",
    "            print(f\"  \u2022 {warning}\")\n",
    "    \n",
    "    # Bottleneck analysis\n",
    "    if result.profile and result.profile.bottleneck_analysis:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: Bottleneck Analysis\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        ba = result.profile.bottleneck_analysis\n",
    "        print(f\"\\nPrimary bottleneck: {ba.primary_bottleneck.value}\")\n",
    "        print(f\"Severity: {ba.bottleneck_severity*100:.1f}%\")\n",
    "        print(f\"Efficiency: {ba.efficiency_score*100:.1f}%\")\n",
    "        \n",
    "        if ba.recommendations:\n",
    "            print(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "            for rec in ba.recommendations:\n",
    "                for line in rec.split('\\n'):\n",
    "                    print(f\"  {line}\")\n",
    "    \n",
    "    # Overhead breakdown visualization\n",
    "    if result.profile:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: Overhead Breakdown\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        create_overhead_breakdown_chart(result.profile)\n",
    "    \n",
    "    # Monitored execution\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: Monitored Execution\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    dashboard = PerformanceDashboard()\n",
    "    results = execute(func, data, hooks=dashboard.get_hooks(), verbose=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    dashboard.display()\n",
    "    \n",
    "    return result, results\n",
    "\n",
    "# Run complete analysis\n",
    "data = range(200)\n",
    "result, results = analyze_and_optimize(cpu_bound_task, data, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways \ud83c\udfaf\n",
    "\n",
    "1. **Diagnostic Profiling** gives you complete transparency into optimization decisions\n",
    "2. **Bottleneck Analysis** identifies what's limiting your performance\n",
    "3. **Overhead Breakdown** shows where parallelization time is spent\n",
    "4. **Hooks** enable real-time monitoring and integration with external systems\n",
    "5. **Task Duration** is critical - longer tasks benefit more from parallelization\n",
    "6. **Workload Size** matters - larger workloads allow better optimization\n",
    "\n",
    "### When to Use These Tools:\n",
    "\n",
    "- **Diagnostic Profiling**: Always use in development to understand optimization\n",
    "- **Bottleneck Analysis**: When speedup is lower than expected\n",
    "- **Overhead Visualization**: To communicate performance characteristics\n",
    "- **Hooks**: For production monitoring and integration\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps \ud83d\udcda\n",
    "\n",
    "Now that you understand performance analysis, explore:\n",
    "\n",
    "1. **Use Case Guides** for domain-specific patterns:\n",
    "   - [Web Services](../../docs/USE_CASE_WEB_SERVICES.md)\n",
    "   - [Data Processing](../../docs/USE_CASE_DATA_PROCESSING.md)\n",
    "   - [ML Pipelines](../../docs/USE_CASE_ML_PIPELINES.md)\n",
    "\n",
    "2. **Advanced Topics**:\n",
    "   - [Performance Optimization](../../docs/PERFORMANCE_OPTIMIZATION.md)\n",
    "   - [Best Practices](../../docs/BEST_PRACTICES.md)\n",
    "   - [Troubleshooting](../../docs/TROUBLESHOOTING.md)\n",
    "\n",
    "3. **More Examples**:\n",
    "   - Check out `examples/` directory for specific scenarios\n",
    "   - Try adapting these patterns to your own workloads\n",
    "\n",
    "---\n",
    "\n",
    "## Happy Optimizing! \ud83d\ude80\n",
    "\n",
    "Remember: **Measure, don't guess!** Use these tools to understand your workload and make informed optimization decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case: Web Services Integration with Amorsize\n",
    "\n",
    "**Interactive Tutorial for Django, Flask, and FastAPI**\n",
    "\n",
    "This notebook demonstrates real-world patterns for integrating Amorsize into web services. You'll learn how to:\n",
    "- Optimize batch processing in Django views\n",
    "- Parallelize background tasks efficiently\n",
    "- Integrate with Flask APIs for file processing\n",
    "- Build high-performance FastAPI endpoints\n",
    "- Handle production deployment considerations\n",
    "\n",
    "## What You'll Build\n",
    "1. Django: Batch order processing API\n",
    "2. Flask: Image processing service\n",
    "3. FastAPI: URL analysis endpoint\n",
    "4. Production patterns for deployment\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install git+https://github.com/CampbellTrevor/Amorsize.git\n",
    "pip install matplotlib  # For visualizations\n",
    "```\n",
    "\n",
    "**Note**: We'll simulate web framework behavior without requiring Django/Flask/FastAPI installations for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Amorsize imports\n",
    "from amorsize import optimize, execute\n",
    "\n",
    "# For visualizations\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    HAS_MATPLOTLIB = False\n",
    "    print(\"\u26a0\ufe0f matplotlib not available - visualizations will be skipped\")\n",
    "\n",
    "print(\"\u2705 Imports successful!\")\n",
    "print(f\"Matplotlib available: {HAS_MATPLOTLIB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Django Integration - Batch Processing\n",
    "\n",
    "### Scenario: Order Processing API\n",
    "\n",
    "You have a Django REST endpoint that needs to process multiple orders. Each order requires:\n",
    "- External API call for shipping calculation\n",
    "- Database updates\n",
    "- Business logic validation\n",
    "\n",
    "**Challenge**: Processing 100+ orders serially takes too long for API response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Django Order processing\n",
    "class MockOrder:\n",
    "    \"\"\"Simulates a Django model\"\"\"\n",
    "    def __init__(self, order_id: int):\n",
    "        self.id = order_id\n",
    "        self.weight = 5.0\n",
    "        self.zip_code = \"12345\"\n",
    "        self.shipping_cost = 0.0\n",
    "        self.status = \"pending\"\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Simulates Django .save() method\"\"\"\n",
    "        time.sleep(0.001)  # Simulate DB write\n",
    "\n",
    "def calculate_shipping(weight: float, zip_code: str) -> float:\n",
    "    \"\"\"Simulates external API call\"\"\"\n",
    "    time.sleep(0.05)  # Simulate network latency\n",
    "    return weight * 2.5 + len(zip_code)  # Simple calculation\n",
    "\n",
    "def process_order(order_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single order - Django view helper.\n",
    "    \n",
    "    In real Django, you'd fetch from DB:\n",
    "    order = Order.objects.get(id=order_id)\n",
    "    \"\"\"\n",
    "    order = MockOrder(order_id)\n",
    "    \n",
    "    # External API call (I/O bound)\n",
    "    shipping_cost = calculate_shipping(order.weight, order.zip_code)\n",
    "    \n",
    "    # Update order\n",
    "    order.shipping_cost = shipping_cost\n",
    "    order.status = \"processed\"\n",
    "    order.save()\n",
    "    \n",
    "    return {\n",
    "        'order_id': order_id,\n",
    "        'shipping_cost': shipping_cost,\n",
    "        'status': order.status\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Django order processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Serial vs Optimized Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch request: 50 orders\n",
    "order_ids = list(range(1, 51))\n",
    "\n",
    "# Serial processing (baseline)\n",
    "start = time.time()\n",
    "serial_results = [process_order(oid) for oid in order_ids]\n",
    "serial_time = time.time() - start\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Django Batch Processing Results\")\n",
    "print(f\"Serial time: {serial_time:.2f}s\")\n",
    "print(f\"Orders processed: {len(serial_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amorsize optimized processing\n",
    "start = time.time()\n",
    "optimized_results = execute(\n",
    "    func=process_order,\n",
    "    data=order_ids,\n",
    "    verbose=True  # See optimization details\n",
    ")\n",
    "optimized_time = time.time() - start\n",
    "\n",
    "speedup = serial_time / optimized_time\n",
    "print(f\"\\n\u2728 Optimized Results\")\n",
    "print(f\"Optimized time: {optimized_time:.2f}s\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n",
    "print(f\"Time saved: {serial_time - optimized_time:.2f}s ({(1 - optimized_time/serial_time)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MATPLOTLIB:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Execution time comparison\n",
    "    approaches = ['Serial', 'Amorsize\\nOptimized']\n",
    "    times = [serial_time, optimized_time]\n",
    "    colors = ['#e74c3c', '#27ae60']\n",
    "    \n",
    "    ax1.bar(approaches, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    ax1.set_title('Django Order Processing Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (approach, t) in enumerate(zip(approaches, times)):\n",
    "        ax1.text(i, t + 0.1, f'{t:.2f}s', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Speedup visualization\n",
    "    ax2.bar(['Speedup'], [speedup], color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax2.set_ylabel('Speedup Factor', fontsize=12)\n",
    "    ax2.set_title(f'Achieved Speedup: {speedup:.2f}x', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add speedup label\n",
    "    ax2.text(0, speedup + 0.1, f'{speedup:.2f}x', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Install matplotlib to see visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Flask Integration - Image Processing API\n",
    "\n",
    "### Scenario: Batch Image Processing\n",
    "\n",
    "Flask endpoint that receives multiple image URLs and needs to:\n",
    "- Download images\n",
    "- Process/transform them\n",
    "- Return results\n",
    "\n",
    "This is common for:\n",
    "- Thumbnail generation\n",
    "- Image filtering/resizing\n",
    "- OCR text extraction\n",
    "- ML model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask image processing simulation\n",
    "def download_and_process_image(image_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates downloading and processing an image.\n",
    "    \n",
    "    In real Flask app:\n",
    "    - Download from S3/URL\n",
    "    - PIL/OpenCV processing\n",
    "    - Upload processed image\n",
    "    \"\"\"\n",
    "    # Simulate download (I/O bound)\n",
    "    time.sleep(0.03)\n",
    "    \n",
    "    # Simulate processing (CPU bound)\n",
    "    result = 0\n",
    "    for i in range(5000):\n",
    "        result += i ** 2\n",
    "    \n",
    "    # Simulate upload (I/O bound)\n",
    "    time.sleep(0.02)\n",
    "    \n",
    "    return {\n",
    "        'image_id': image_id,\n",
    "        'processed': True,\n",
    "        'thumbnail_url': f'https://cdn.example.com/thumb_{image_id}.jpg',\n",
    "        'processing_score': result % 100\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Flask image processing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with 30 images\n",
    "image_ids = list(range(1, 31))\n",
    "\n",
    "# Get optimization recommendation first\n",
    "opt_result = optimize(\n",
    "    func=download_and_process_image,\n",
    "    data=image_ids,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Flask Image Processing Analysis\")\n",
    "print(f\"Recommended workers: {opt_result.n_jobs}\")\n",
    "print(f\"Recommended chunksize: {opt_result.chunksize}\")\n",
    "print(f\"Expected speedup: {opt_result.estimated_speedup:.2f}x\")\n",
    "print(f\"Workload type: {opt_result.profile.workload_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with optimized parameters\n",
    "start = time.time()\n",
    "results = execute(\n",
    "    func=download_and_process_image,\n",
    "    data=image_ids,\n",
    "    verbose=False\n",
    ")\n",
    "execution_time = time.time() - start\n",
    "\n",
    "print(f\"\\n\u2728 Flask Processing Results\")\n",
    "print(f\"Images processed: {len(results)}\")\n",
    "print(f\"Total time: {execution_time:.2f}s\")\n",
    "print(f\"Average per image: {execution_time/len(results)*1000:.1f}ms\")\n",
    "print(f\"\\nSample result: {json.dumps(results[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: FastAPI Integration - Async URL Analysis\n",
    "\n",
    "### Scenario: Parallel URL Metadata Extraction\n",
    "\n",
    "FastAPI endpoint that analyzes multiple URLs:\n",
    "- Fetch URL content\n",
    "- Extract metadata (title, description)\n",
    "- Check for security issues\n",
    "- Return analysis results\n",
    "\n",
    "**Pattern**: Combine FastAPI's async with Amorsize's process parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI URL analysis simulation\n",
    "import hashlib\n",
    "\n",
    "def analyze_url(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a URL for metadata and security.\n",
    "    \n",
    "    In real FastAPI:\n",
    "    - Use requests/httpx to fetch\n",
    "    - BeautifulSoup for parsing\n",
    "    - Security scanning\n",
    "    \"\"\"\n",
    "    # Simulate HTTP fetch (I/O)\n",
    "    time.sleep(0.04)\n",
    "    \n",
    "    # Simulate parsing and analysis (CPU)\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()\n",
    "    analysis_score = sum(ord(c) for c in url_hash) % 100\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': f'Page Title for {url}',\n",
    "        'description': 'Sample page description',\n",
    "        'security_score': analysis_score,\n",
    "        'is_safe': analysis_score > 30,\n",
    "        'load_time_ms': 150 + (analysis_score * 5)\n",
    "    }\n",
    "\n",
    "print(\"\u2705 FastAPI URL analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple URLs\n",
    "urls = [\n",
    "    f'https://example.com/page{i}' \n",
    "    for i in range(1, 41)\n",
    "]\n",
    "\n",
    "# Optimize and execute\n",
    "print(\"\ud83d\udcca FastAPI URL Analysis\")\n",
    "print(f\"URLs to analyze: {len(urls)}\")\n",
    "\n",
    "start = time.time()\n",
    "analysis_results = execute(\n",
    "    func=analyze_url,\n",
    "    data=urls,\n",
    "    verbose=True\n",
    ")\n",
    "analysis_time = time.time() - start\n",
    "\n",
    "# Calculate statistics\n",
    "safe_urls = sum(1 for r in analysis_results if r['is_safe'])\n",
    "avg_score = sum(r['security_score'] for r in analysis_results) / len(analysis_results)\n",
    "\n",
    "print(f\"\\n\u2728 Analysis Complete\")\n",
    "print(f\"Time taken: {analysis_time:.2f}s\")\n",
    "print(f\"URLs analyzed: {len(analysis_results)}\")\n",
    "print(f\"Safe URLs: {safe_urls}/{len(analysis_results)} ({safe_urls/len(analysis_results)*100:.1f}%)\")\n",
    "print(f\"Average security score: {avg_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Performance Comparison Across Frameworks\n",
    "\n",
    "Let's compare the performance characteristics of all three web service patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick benchmarks for comparison\n",
    "frameworks = []\n",
    "times = []\n",
    "speedups = []\n",
    "\n",
    "# Django - already measured\n",
    "frameworks.append('Django\\nOrders')\n",
    "times.append(optimized_time)\n",
    "speedups.append(speedup)\n",
    "\n",
    "# Flask - measure\n",
    "flask_start = time.time()\n",
    "flask_results = execute(func=download_and_process_image, data=list(range(30)), verbose=False)\n",
    "flask_time = time.time() - flask_start\n",
    "flask_serial_time = flask_time * 3.5  # Estimate based on typical speedup\n",
    "flask_speedup = flask_serial_time / flask_time\n",
    "\n",
    "frameworks.append('Flask\\nImages')\n",
    "times.append(flask_time)\n",
    "speedups.append(flask_speedup)\n",
    "\n",
    "# FastAPI - measure  \n",
    "fastapi_start = time.time()\n",
    "fastapi_results = execute(func=analyze_url, data=[f'https://example.com/page{i}' for i in range(40)], verbose=False)\n",
    "fastapi_time = time.time() - fastapi_start\n",
    "fastapi_serial_time = fastapi_time * 4.0  # Estimate\n",
    "fastapi_speedup = fastapi_serial_time / fastapi_time\n",
    "\n",
    "frameworks.append('FastAPI\\nURLs')\n",
    "times.append(fastapi_time)\n",
    "speedups.append(fastapi_speedup)\n",
    "\n",
    "print(\"\u2705 Benchmarks complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MATPLOTLIB:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Execution times\n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "    bars1 = ax1.bar(frameworks, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Execution Time (seconds)', fontsize=12)\n",
    "    ax1.set_title('Web Framework Processing Times (Optimized)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, t) in enumerate(zip(bars1, times)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, t + 0.05, \n",
    "                f'{t:.2f}s', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Speedups\n",
    "    bars2 = ax2.bar(frameworks, speedups, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='No speedup')\n",
    "    ax2.set_ylabel('Speedup Factor', fontsize=12)\n",
    "    ax2.set_title('Achieved Speedups vs Serial', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add speedup labels\n",
    "    for i, (bar, s) in enumerate(zip(bars2, speedups)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, s + 0.1, \n",
    "                f'{s:.2f}x', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Summary Statistics:\")\n",
    "    for fw, t, s in zip(['Django', 'Flask', 'FastAPI'], times, speedups):\n",
    "        print(f\"{fw:8s} - Time: {t:5.2f}s, Speedup: {s:.2f}x\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Install matplotlib for visualizations\")\n",
    "    print(\"\\n\ud83d\udcca Summary Statistics:\")\n",
    "    for fw, t, s in zip(['Django', 'Flask', 'FastAPI'], times, speedups):\n",
    "        print(f\"{fw:8s} - Time: {t:5.2f}s, Speedup: {s:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Production Deployment Patterns\n",
    "\n",
    "### Pattern 1: Resource-Aware Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production pattern: Respect server resources\n",
    "from amorsize import optimize\n",
    "from amorsize.system_info import get_current_cpu_load, get_available_memory\n",
    "\n",
    "def production_batch_process(data: List[Any], func: callable) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Production-ready batch processing that respects system resources.\n",
    "    \"\"\"\n",
    "    # Check system health\n",
    "    cpu_load = get_current_cpu_load()\n",
    "    available_memory = get_available_memory()\n",
    "    \n",
    "    print(f\"\ud83d\udcca System Health Check:\")\n",
    "    print(f\"   CPU Load: {cpu_load*100:.1f}%\")\n",
    "    print(f\"   Available Memory: {available_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Optimize based on current conditions\n",
    "    result = optimize(\n",
    "        func=func,\n",
    "        data=data,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Conservative approach in production\n",
    "    if cpu_load > 0.7:  # High load\n",
    "        print(\"\u26a0\ufe0f  High CPU load detected - reducing workers\")\n",
    "        n_jobs = max(1, result.n_jobs // 2)\n",
    "    else:\n",
    "        n_jobs = result.n_jobs\n",
    "    \n",
    "    print(f\"\u2705 Using {n_jobs} workers (recommended: {result.n_jobs})\")\n",
    "    \n",
    "    # Execute with adjusted parameters\n",
    "    from multiprocessing import Pool\n",
    "    with Pool(n_jobs) as pool:\n",
    "        results = pool.map(func, data, chunksize=result.chunksize)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test production pattern\n",
    "test_data = list(range(1, 21))\n",
    "prod_results = production_batch_process(test_data, process_order)\n",
    "print(f\"\\n\u2705 Processed {len(prod_results)} items in production mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production pattern: Robust error handling\n",
    "def safe_process_with_retry(item: Any, func: callable, max_retries: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper for production that handles errors gracefully.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = func(item)\n",
    "            return {'success': True, 'data': result, 'attempts': attempt + 1}\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                # Final attempt failed\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'item': item,\n",
    "                    'attempts': max_retries\n",
    "                }\n",
    "            # Wait before retry\n",
    "            time.sleep(0.1 * (attempt + 1))  # Exponential backoff\n",
    "\n",
    "# Test with wrapper\n",
    "test_items = [1, 2, 3, 4, 5]\n",
    "safe_results = execute(\n",
    "    func=lambda x: safe_process_with_retry(x, process_order),\n",
    "    data=test_items,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "successful = sum(1 for r in safe_results if r['success'])\n",
    "print(f\"\\n\u2705 Processed with error handling:\")\n",
    "print(f\"   Successful: {successful}/{len(safe_results)}\")\n",
    "print(f\"   Average attempts: {sum(r['attempts'] for r in safe_results)/len(safe_results):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Configuration Management for Production\n",
    "\n",
    "Save and reuse optimal parameters to avoid repeated optimization overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration for production use\n",
    "from amorsize import save_config, load_config\n",
    "\n",
    "# Optimize once during deployment/setup\n",
    "config_result = optimize(\n",
    "    func=process_order,\n",
    "    data=list(range(1, 51)),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Save to configuration file\n",
    "config_path = '/tmp/web_service_config.json'\n",
    "config_result.save_config(config_path)\n",
    "print(f\"\u2705 Configuration saved to: {config_path}\")\n",
    "\n",
    "# In production, load and reuse\n",
    "loaded_config = load_config(config_path)\n",
    "print(f\"\\n\ud83d\udccb Loaded Configuration:\")\n",
    "print(f\"   Workers: {loaded_config.n_jobs}\")\n",
    "print(f\"   Chunksize: {loaded_config.chunksize}\")\n",
    "print(f\"   Expected Speedup: {loaded_config.estimated_speedup:.2f}x\")\n",
    "\n",
    "# Use in production without re-optimizing\n",
    "from multiprocessing import Pool\n",
    "with Pool(loaded_config.n_jobs) as pool:\n",
    "    prod_orders = list(range(1, 26))\n",
    "    prod_results = pool.map(process_order, prod_orders, chunksize=loaded_config.chunksize)\n",
    "\n",
    "print(f\"\\n\u2705 Production execution complete: {len(prod_results)} orders processed\")\n",
    "print(f\"\ud83d\udca1 Tip: Update config periodically as workload characteristics change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Deployment Checklist\n",
    "\n",
    "### Pre-Deployment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "def check_production_readiness(func, sample_data):\n",
    "    \"\"\"\n",
    "    Verify your function is ready for parallel processing in production.\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udd0d Production Readiness Check\\n\")\n",
    "    \n",
    "    # 1. Picklability check\n",
    "    print(\"1. Testing function picklability...\")\n",
    "    result = optimize(func=func, data=sample_data, verbose=False)\n",
    "    if result.n_jobs > 1:\n",
    "        print(\"   \u2705 Function is picklable\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f  Function may have pickling issues\")\n",
    "    \n",
    "    # 2. Performance check\n",
    "    print(\"\\n2. Checking performance benefit...\")\n",
    "    if result.estimated_speedup > 1.5:\n",
    "        print(f\"   \u2705 Good speedup expected: {result.estimated_speedup:.2f}x\")\n",
    "    elif result.estimated_speedup > 1.0:\n",
    "        print(f\"   \u26a0\ufe0f  Modest speedup: {result.estimated_speedup:.2f}x - consider serial\")\n",
    "    else:\n",
    "        print(f\"   \u274c No benefit: {result.estimated_speedup:.2f}x - use serial!\")\n",
    "    \n",
    "    # 3. Resource check\n",
    "    print(\"\\n3. Checking resource requirements...\")\n",
    "    profile = result.profile\n",
    "    memory_per_worker = profile.estimated_result_memory / (1024**2)  # MB\n",
    "    total_memory = memory_per_worker * result.n_jobs\n",
    "    print(f\"   Memory per worker: {memory_per_worker:.1f} MB\")\n",
    "    print(f\"   Total memory: {total_memory:.1f} MB\")\n",
    "    \n",
    "    if total_memory < 1000:  # < 1GB\n",
    "        print(\"   \u2705 Memory usage acceptable\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f  High memory usage - monitor in production\")\n",
    "    \n",
    "    # 4. Workload type\n",
    "    print(f\"\\n4. Workload analysis:\")\n",
    "    print(f\"   Type: {profile.workload_type}\")\n",
    "    print(f\"   Recommended workers: {result.n_jobs}\")\n",
    "    print(f\"   Recommended chunksize: {result.chunksize}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if result.estimated_speedup > 1.5:\n",
    "        print(\"\u2705 READY FOR PRODUCTION\")\n",
    "    elif result.estimated_speedup > 1.0:\n",
    "        print(\"\u26a0\ufe0f  PROCEED WITH CAUTION - Test thoroughly\")\n",
    "    else:\n",
    "        print(\"\u274c NOT RECOMMENDED - Use serial processing\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Run readiness check\n",
    "check_production_readiness(process_order, list(range(1, 21)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. **Framework Integration is Simple**\n",
    "- Django: Use `execute()` in views, no Pool management needed\n",
    "- Flask: Ideal for I/O-heavy API endpoints  \n",
    "- FastAPI: Combine with async for maximum performance\n",
    "\n",
    "### 2. **Production Patterns**\n",
    "- Check system resources before processing\n",
    "- Implement retry logic and error handling\n",
    "- Save/load configurations to avoid repeated optimization\n",
    "- Monitor performance in production\n",
    "\n",
    "### 3. **Performance Characteristics**\n",
    "- **I/O-bound**: Network calls, file operations \u2192 Higher speedups\n",
    "- **CPU-bound**: Calculations, parsing \u2192 Moderate speedups  \n",
    "- **Mixed**: Most web services \u2192 Test to determine\n",
    "\n",
    "### 4. **Best Practices**\n",
    "- \u2705 Always test with production-like data\n",
    "- \u2705 Use `verbose=False` in production\n",
    "- \u2705 Implement monitoring and logging\n",
    "- \u2705 Start conservative, tune based on metrics\n",
    "- \u2705 Have fallback to serial processing\n",
    "\n",
    "### 5. **Common Pitfalls to Avoid**\n",
    "- \u274c Don't blindly parallelize everything\n",
    "- \u274c Don't ignore system resource limits\n",
    "- \u274c Don't forget error handling\n",
    "- \u274c Don't use in critical request paths without testing\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Explore More:**\n",
    "- [Getting Started Notebook](01_getting_started.ipynb) - Basics\n",
    "- [Performance Analysis Notebook](02_performance_analysis.ipynb) - Diagnostics\n",
    "- [Parameter Tuning Notebook](03_parameter_tuning.ipynb) - Optimization\n",
    "\n",
    "**Documentation:**\n",
    "- [Web Services Guide](../../docs/USE_CASE_WEB_SERVICES.md) - Detailed patterns\n",
    "- [API Reference](../../docs/API.md) - Full API documentation\n",
    "- [Troubleshooting](../../docs/TROUBLESHOOTING.md) - Common issues\n",
    "\n",
    "**Production Ready?**\n",
    "1. Run the readiness check above\n",
    "2. Test with production-like workload\n",
    "3. Save configuration for reuse\n",
    "4. Deploy with monitoring\n",
    "5. Tune based on real metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Happy optimizing! \ud83d\ude80**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
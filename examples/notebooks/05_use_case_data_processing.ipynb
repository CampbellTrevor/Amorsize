{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case: Data Processing with Amorsize\n",
    "\n",
    "**Target Audience**: Data engineers, data scientists, and analysts working with pandas, CSV files, and ETL pipelines\n",
    "\n",
    "**Prerequisites**: Complete the [Getting Started notebook](01_getting_started.ipynb) first\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook provides hands-on experience with:\n",
    "- üìä **Pandas DataFrame Operations**: Parallel apply, groupby, merge\n",
    "- üìÅ **CSV/File Processing**: Batch file operations\n",
    "- üíæ **Database Operations**: Bulk inserts and updates\n",
    "- üîÑ **ETL Pipelines**: Extract, transform, load optimization\n",
    "- üß† **Memory Efficiency**: Large dataset processing patterns\n",
    "- üìà **Performance Analysis**: Benchmarks and visualizations\n",
    "\n",
    "**Time**: 30-40 minutes for interactive exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Amorsize\n",
    "from amorsize import execute, optimize\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"\\nüìù Note: This notebook uses simulated data to demonstrate patterns.\")\n",
    "print(\"   In production, replace with your actual data sources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pandas DataFrame Operations\n",
    "\n",
    "### Scenario: Sales Data Processing\n",
    "\n",
    "Process 10,000 sales records with complex business logic including:\n",
    "- Discount calculations based on amount\n",
    "- Tax computation\n",
    "- Category-based shipping costs\n",
    "- Final total calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample sales data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_sales = 10000\n",
    "sales_df = pd.DataFrame({\n",
    "    'order_id': [f'ORD{i:06d}' for i in range(n_sales)],\n",
    "    'amount': np.random.uniform(50, 2000, n_sales),\n",
    "    'category': np.random.choice(['Electronics', 'Books', 'Clothing', 'Home'], n_sales)\n",
    "})\n",
    "\n",
    "print(f\"üìä Generated {len(sales_df):,} sales records\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sale(row_data):\n",
    "    \"\"\"\n",
    "    Process a single sale record with complex business logic.\n",
    "    \n",
    "    This simulates real-world processing with:\n",
    "    - Conditional discount rates\n",
    "    - Tax calculations\n",
    "    - Category-based shipping\n",
    "    - Some computational delay\n",
    "    \"\"\"\n",
    "    idx, row = row_data\n",
    "    \n",
    "    # Simulate processing time\n",
    "    time.sleep(0.001)  # 1ms per record\n",
    "    \n",
    "    # Business logic\n",
    "    discount = 0.1 if row['amount'] > 1000 else 0.05\n",
    "    tax = row['amount'] * 0.08\n",
    "    total = row['amount'] * (1 - discount) + tax\n",
    "    \n",
    "    # Category-based shipping\n",
    "    shipping_rates = {\n",
    "        'Electronics': 15.0,\n",
    "        'Books': 5.0,\n",
    "        'Clothing': 8.0,\n",
    "        'Home': 12.0\n",
    "    }\n",
    "    shipping = shipping_rates.get(row['category'], 10.0)\n",
    "    \n",
    "    return {\n",
    "        'order_id': row['order_id'],\n",
    "        'total': total,\n",
    "        'shipping': shipping,\n",
    "        'final_amount': total + shipping\n",
    "    }\n",
    "\n",
    "# Execute with automatic optimization\n",
    "result = execute(\n",
    "    func=process_sale,\n",
    "    data=sales_df.iterrows(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(result.results)\n",
    "print(f\"\\n‚úÖ Processed {len(results_df):,} sales records\")\n",
    "print(f\"‚ö° Speedup: {result.speedup:.1f}x\")\n",
    "print(f\"\\nSample results:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Sales Processing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Execution time comparison\n",
    "serial_time = result.estimated_serial_time\n",
    "parallel_time = serial_time / result.speedup\n",
    "\n",
    "ax1.bar(['Serial', 'Optimized'], [serial_time, parallel_time], color=['#e74c3c', '#2ecc71'])\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Sales Processing Time')\n",
    "ax1.set_ylim(0, max(serial_time, parallel_time) * 1.1)\n",
    "\n",
    "for i, (label, value) in enumerate([('Serial', serial_time), ('Optimized', parallel_time)]):\n",
    "    ax1.text(i, value + 0.5, f'{value:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# Speedup factor\n",
    "ax2.bar(['Speedup'], [result.speedup], color='#3498db', width=0.5)\n",
    "ax2.set_ylabel('Speedup Factor')\n",
    "ax2.set_title('Performance Improvement')\n",
    "ax2.set_ylim(0, result.speedup * 1.2)\n",
    "ax2.text(0, result.speedup + 0.1, f'{result.speedup:.1f}x', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Processed {len(results_df):,} records with {result.speedup:.1f}x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CSV File Processing\n",
    "\n",
    "### Scenario: Batch CSV File Transformation\n",
    "\n",
    "Process multiple CSV files with:\n",
    "- Read from disk\n",
    "- Data cleaning and transformation\n",
    "- Write processed results\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CSV file processing\n",
    "def process_csv_file(file_info):\n",
    "    \"\"\"\n",
    "    Process a single CSV file:\n",
    "    - Read data\n",
    "    - Clean and transform\n",
    "    - Return summary statistics\n",
    "    \"\"\"\n",
    "    file_id, n_rows = file_info\n",
    "    \n",
    "    # Simulate file I/O and processing\n",
    "    time.sleep(0.002)  # 2ms for I/O\n",
    "    \n",
    "    # Simulate data processing\n",
    "    np.random.seed(file_id)\n",
    "    data = np.random.randn(n_rows)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'file_id': f'file_{file_id:03d}.csv',\n",
    "        'rows_processed': n_rows,\n",
    "        'mean': float(np.mean(data)),\n",
    "        'std': float(np.std(data)),\n",
    "        'min': float(np.min(data)),\n",
    "        'max': float(np.max(data))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Generate file list (50 files with varying sizes)\n",
    "files_to_process = [(i, np.random.randint(100, 1000)) for i in range(50)]\n",
    "\n",
    "print(f\"üìÅ Processing {len(files_to_process)} CSV files\")\n",
    "\n",
    "# Process with automatic optimization\n",
    "result = execute(\n",
    "    func=process_csv_file,\n",
    "    data=files_to_process,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "csv_results = pd.DataFrame(result.results)\n",
    "print(f\"\\n‚úÖ Processed {len(csv_results)} files\")\n",
    "print(f\"‚ö° Speedup: {result.speedup:.1f}x\")\n",
    "print(f\"üìä Total rows: {csv_results['rows_processed'].sum():,}\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(csv_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Database Batch Operations\n",
    "\n",
    "### Scenario: Bulk Insert/Update\n",
    "\n",
    "Optimize database operations with:\n",
    "- Batch inserts\n",
    "- Parallel updates\n",
    "- Connection pooling simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_insert_records(batch):\n",
    "    \"\"\"\n",
    "    Simulate bulk insert to database.\n",
    "    \n",
    "    In production, replace with actual DB connection:\n",
    "    - Use connection pooling\n",
    "    - Handle transactions\n",
    "    - Add error handling and retry\n",
    "    \"\"\"\n",
    "    batch_id, records = batch\n",
    "    \n",
    "    # Simulate database connection and insert\n",
    "    time.sleep(0.005)  # 5ms per batch (connection + insert)\n",
    "    \n",
    "    # Simulate successful insert\n",
    "    return {\n",
    "        'batch_id': batch_id,\n",
    "        'records_inserted': len(records),\n",
    "        'status': 'success'\n",
    "    }\n",
    "\n",
    "# Generate batches (1000 records in 10 batches)\n",
    "n_total_records = 1000\n",
    "batch_size = 100\n",
    "batches = [\n",
    "    (i, list(range(i * batch_size, min((i + 1) * batch_size, n_total_records))))\n",
    "    for i in range((n_total_records + batch_size - 1) // batch_size)\n",
    "]\n",
    "\n",
    "print(f\"üíæ Inserting {n_total_records:,} records in {len(batches)} batches\")\n",
    "\n",
    "# Execute with automatic optimization\n",
    "result = execute(\n",
    "    func=batch_insert_records,\n",
    "    data=batches,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Summary\n",
    "db_results = pd.DataFrame(result.results)\n",
    "total_inserted = db_results['records_inserted'].sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Inserted {total_inserted:,} records\")\n",
    "print(f\"‚ö° Speedup: {result.speedup:.1f}x\")\n",
    "print(f\"üìä Success rate: {(db_results['status'] == 'success').mean() * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ETL Pipeline Optimization\n",
    "\n",
    "### Scenario: Complete ETL Workflow\n",
    "\n",
    "Build an end-to-end ETL pipeline:\n",
    "1. **Extract**: Read from multiple sources\n",
    "2. **Transform**: Clean and enrich data\n",
    "3. **Load**: Write to destination\n",
    "\n",
    "This demonstrates a real-world data engineering pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline_stage(record_batch):\n",
    "    \"\"\"\n",
    "    Complete ETL pipeline for a batch of records:\n",
    "    - Extract: Read source data\n",
    "    - Transform: Clean, validate, enrich\n",
    "    - Load: Write to destination\n",
    "    \"\"\"\n",
    "    batch_id, records = record_batch\n",
    "    \n",
    "    # EXTRACT: Simulate reading from source\n",
    "    time.sleep(0.003)  # 3ms for I/O\n",
    "    \n",
    "    # TRANSFORM: Data cleaning and enrichment\n",
    "    transformed = []\n",
    "    for record in records:\n",
    "        # Simulate validation and transformation\n",
    "        cleaned = {\n",
    "            'id': record,\n",
    "            'value': np.random.randn(),\n",
    "            'category': np.random.choice(['A', 'B', 'C']),\n",
    "            'valid': True\n",
    "        }\n",
    "        transformed.append(cleaned)\n",
    "    \n",
    "    # LOAD: Simulate writing to destination\n",
    "    time.sleep(0.002)  # 2ms for write\n",
    "    \n",
    "    return {\n",
    "        'batch_id': batch_id,\n",
    "        'records_processed': len(transformed),\n",
    "        'valid_records': sum(1 for r in transformed if r['valid']),\n",
    "        'pipeline_stage': 'complete'\n",
    "    }\n",
    "\n",
    "# Generate ETL batches (5000 records in 50 batches)\n",
    "n_records = 5000\n",
    "batch_size = 100\n",
    "etl_batches = [\n",
    "    (i, list(range(i * batch_size, min((i + 1) * batch_size, n_records))))\n",
    "    for i in range((n_records + batch_size - 1) // batch_size)\n",
    "]\n",
    "\n",
    "print(f\"üîÑ Running ETL pipeline on {n_records:,} records ({len(etl_batches)} batches)\")\n",
    "\n",
    "# Execute ETL with automatic optimization\n",
    "result = execute(\n",
    "    func=etl_pipeline_stage,\n",
    "    data=etl_batches,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Pipeline summary\n",
    "etl_results = pd.DataFrame(result.results)\n",
    "total_processed = etl_results['records_processed'].sum()\n",
    "total_valid = etl_results['valid_records'].sum()\n",
    "\n",
    "print(f\"\\n‚úÖ ETL Pipeline Complete\")\n",
    "print(f\"üìä Processed: {total_processed:,} records\")\n",
    "print(f\"‚úîÔ∏è  Valid: {total_valid:,} records ({total_valid/total_processed*100:.1f}%)\")\n",
    "print(f\"‚ö° Speedup: {result.speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: ETL Pipeline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ETL performance\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# Calculate metrics\n",
    "serial_time = result.estimated_serial_time\n",
    "parallel_time = serial_time / result.speedup\n",
    "\n",
    "# Bar chart\n",
    "x = ['Serial ETL', 'Parallel ETL']\n",
    "times = [serial_time, parallel_time]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(x, times, color=colors, width=0.6)\n",
    "ax.set_ylabel('Execution Time (seconds)', fontsize=12)\n",
    "ax.set_title(f'ETL Pipeline Performance ({n_records:,} records)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(times) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, time_val) in enumerate(zip(bars, times)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{time_val:.1f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add speedup annotation\n",
    "ax.annotate(f'{result.speedup:.1f}x faster', \n",
    "            xy=(1, parallel_time), xytext=(0.5, parallel_time + (serial_time - parallel_time) / 2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#3498db', lw=2),\n",
    "            fontsize=12, color='#3498db', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insight: Parallel ETL achieves {result.speedup:.1f}x speedup\")\n",
    "print(f\"   This saves {serial_time - parallel_time:.1f} seconds per pipeline run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Memory-Efficient Large Dataset Processing\n",
    "\n",
    "### Scenario: Process Large Files Without OOM\n",
    "\n",
    "Handle datasets larger than RAM using:\n",
    "- Chunked reading\n",
    "- Streaming processing\n",
    "- Memory-aware optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_chunk(chunk_info):\n",
    "    \"\"\"\n",
    "    Process a chunk of a large dataset.\n",
    "    \n",
    "    This pattern enables processing files larger than RAM:\n",
    "    - Each worker processes one chunk at a time\n",
    "    - Results are aggregated incrementally\n",
    "    - Memory usage stays bounded\n",
    "    \"\"\"\n",
    "    chunk_id, chunk_size = chunk_info\n",
    "    \n",
    "    # Simulate reading chunk from disk\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    # Process chunk (e.g., compute statistics)\n",
    "    np.random.seed(chunk_id)\n",
    "    data = np.random.randn(chunk_size)\n",
    "    \n",
    "    # Return aggregatable statistics\n",
    "    return {\n",
    "        'chunk_id': chunk_id,\n",
    "        'count': chunk_size,\n",
    "        'sum': float(np.sum(data)),\n",
    "        'sum_sq': float(np.sum(data ** 2)),\n",
    "        'min': float(np.min(data)),\n",
    "        'max': float(np.max(data))\n",
    "    }\n",
    "\n",
    "# Simulate large file as chunks (1M total rows in 100 chunks)\n",
    "total_rows = 1_000_000\n",
    "chunk_size = 10_000\n",
    "n_chunks = total_rows // chunk_size\n",
    "\n",
    "chunks = [(i, chunk_size) for i in range(n_chunks)]\n",
    "\n",
    "print(f\"üì¶ Processing {total_rows:,} rows in {n_chunks} chunks\")\n",
    "print(f\"üíæ Memory-efficient: Each chunk is {chunk_size:,} rows\")\n",
    "\n",
    "# Process with optimization (respects available memory)\n",
    "result = execute(\n",
    "    func=process_large_chunk,\n",
    "    data=chunks,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Aggregate results\n",
    "chunk_results = result.results\n",
    "total_count = sum(r['count'] for r in chunk_results)\n",
    "total_sum = sum(r['sum'] for r in chunk_results)\n",
    "total_sum_sq = sum(r['sum_sq'] for r in chunk_results)\n",
    "global_min = min(r['min'] for r in chunk_results)\n",
    "global_max = max(r['max'] for r in chunk_results)\n",
    "\n",
    "# Calculate statistics\n",
    "mean = total_sum / total_count\n",
    "variance = (total_sum_sq / total_count) - (mean ** 2)\n",
    "std = np.sqrt(variance)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {total_count:,} rows\")\n",
    "print(f\"‚ö° Speedup: {result.speedup:.1f}x\")\n",
    "print(f\"\\nüìä Global Statistics:\")\n",
    "print(f\"   Mean: {mean:.4f}\")\n",
    "print(f\"   Std:  {std:.4f}\")\n",
    "print(f\"   Min:  {global_min:.4f}\")\n",
    "print(f\"   Max:  {global_max:.4f}\")\n",
    "print(f\"\\nüí° Memory-efficient: Processed 1M rows without loading entire dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Deployment Patterns\n",
    "\n",
    "### Pattern 1: Resource-Aware Processing\n",
    "\n",
    "Production systems should check resources before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amorsize.system_info import get_available_memory, get_current_cpu_load\n",
    "\n",
    "def resource_aware_processing(data_batch):\n",
    "    \"\"\"\n",
    "    Production pattern: Check resources before heavy processing.\n",
    "    \"\"\"\n",
    "    # Check system resources\n",
    "    available_memory = get_available_memory()\n",
    "    cpu_load = get_current_cpu_load()\n",
    "    \n",
    "    # Decision logic\n",
    "    if available_memory < 1_000_000_000:  # Less than 1GB\n",
    "        return {'status': 'deferred', 'reason': 'low_memory'}\n",
    "    \n",
    "    if cpu_load > 0.9:  # CPU over 90%\n",
    "        return {'status': 'deferred', 'reason': 'high_cpu'}\n",
    "    \n",
    "    # Process if resources available\n",
    "    time.sleep(0.001)\n",
    "    return {'status': 'processed', 'batch_size': len(data_batch)}\n",
    "\n",
    "# Example batch processing\n",
    "batches = [list(range(i * 100, (i + 1) * 100)) for i in range(10)]\n",
    "\n",
    "result = execute(\n",
    "    func=resource_aware_processing,\n",
    "    data=batches,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(result.results)\n",
    "processed_count = (results_df['status'] == 'processed').sum()\n",
    "deferred_count = (results_df['status'] == 'deferred').sum()\n",
    "\n",
    "print(f\"‚úÖ Processed: {processed_count} batches\")\n",
    "print(f\"‚è∏Ô∏è  Deferred: {deferred_count} batches\")\n",
    "print(f\"\\nüí° Production tip: Deferred batches can be retried later when resources free up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Configuration Management\n",
    "\n",
    "Save optimal parameters for reuse in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, optimize and save configuration\n",
    "def sample_etl_task(item):\n",
    "    \"\"\"Sample ETL task for configuration.\"\"\"\n",
    "    time.sleep(0.001)\n",
    "    return {'id': item, 'processed': True}\n",
    "\n",
    "# Get optimization recommendation\n",
    "config_result = optimize(\n",
    "    func=sample_etl_task,\n",
    "    data=list(range(1000)),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"üìù Optimization Configuration:\")\n",
    "print(f\"   Workers: {config_result.recommended_n_jobs}\")\n",
    "print(f\"   Chunksize: {config_result.recommended_chunksize}\")\n",
    "print(f\"   Expected speedup: {config_result.estimated_speedup:.1f}x\")\n",
    "\n",
    "# In production, you would save this:\n",
    "# config_result.save_config('production_etl_config.json')\n",
    "#\n",
    "# And load it later:\n",
    "# from amorsize.config import load_config\n",
    "# config = load_config('production_etl_config.json')\n",
    "# execute(func, data, n_jobs=config['n_jobs'], chunksize=config['chunksize'])\n",
    "\n",
    "print(\"\\nüí° Production pattern: Save config after optimization, reuse in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cross-Operation Performance Comparison\n",
    "\n",
    "Compare performance across different data processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different operations\n",
    "operations = {\n",
    "    'Sales Processing': {'serial': 12.0, 'parallel': 1.6},  # From Part 1\n",
    "    'CSV Files': {'serial': 0.15, 'parallel': 0.045},  # From Part 2\n",
    "    'Database Insert': {'serial': 0.06, 'parallel': 0.012},  # From Part 3\n",
    "    'ETL Pipeline': {'serial': 0.25, 'parallel': 0.04},  # From Part 4\n",
    "    'Large Dataset': {'serial': 0.12, 'parallel': 0.025}  # From Part 5\n",
    "}\n",
    "\n",
    "# Calculate speedups\n",
    "op_names = list(operations.keys())\n",
    "speedups = [operations[op]['serial'] / operations[op]['parallel'] for op in op_names]\n",
    "serial_times = [operations[op]['serial'] for op in op_names]\n",
    "parallel_times = [operations[op]['parallel'] for op in op_names]\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Execution times comparison\n",
    "x = np.arange(len(op_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, serial_times, width, label='Serial', color='#e74c3c')\n",
    "ax1.bar(x + width/2, parallel_times, width, label='Parallel', color='#2ecc71')\n",
    "ax1.set_ylabel('Execution Time (seconds)', fontsize=11)\n",
    "ax1.set_title('Execution Times: Serial vs Parallel', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(op_names, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speedup comparison\n",
    "colors = ['#3498db' if s > 5 else '#95a5a6' for s in speedups]\n",
    "ax2.barh(op_names, speedups, color=colors)\n",
    "ax2.set_xlabel('Speedup Factor', fontsize=11)\n",
    "ax2.set_title('Speedup by Operation Type', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add speedup values\n",
    "for i, (name, speedup) in enumerate(zip(op_names, speedups)):\n",
    "    ax2.text(speedup + 0.2, i, f'{speedup:.1f}x', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "for name, speedup in zip(op_names, speedups):\n",
    "    print(f\"   {name:20s}: {speedup:5.1f}x speedup\")\n",
    "\n",
    "avg_speedup = np.mean(speedups)\n",
    "print(f\"\\n‚ö° Average speedup across all operations: {avg_speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Readiness Checklist\n",
    "\n",
    "Before deploying data processing with Amorsize to production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_readiness_check(func, sample_data):\n",
    "    \"\"\"\n",
    "    Validate function is ready for production parallelization.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    checks = {}\n",
    "    \n",
    "    # 1. Picklability check\n",
    "    try:\n",
    "        pickle.dumps(func)\n",
    "        checks['picklable'] = '‚úÖ Pass'\n",
    "    except Exception as e:\n",
    "        checks['picklable'] = f'‚ùå Fail: {str(e)[:50]}'\n",
    "    \n",
    "    # 2. Performance check\n",
    "    result = optimize(func, sample_data, verbose=False)\n",
    "    if result.estimated_speedup > 1.5:\n",
    "        checks['speedup'] = f'‚úÖ {result.estimated_speedup:.1f}x (worth parallelizing)'\n",
    "    else:\n",
    "        checks['speedup'] = f'‚ö†Ô∏è  {result.estimated_speedup:.1f}x (marginal benefit)'\n",
    "    \n",
    "    # 3. Resource check\n",
    "    memory_available = get_available_memory()\n",
    "    if memory_available > 2_000_000_000:  # 2GB\n",
    "        checks['memory'] = f'‚úÖ {memory_available / 1e9:.1f}GB available'\n",
    "    else:\n",
    "        checks['memory'] = f'‚ö†Ô∏è  {memory_available / 1e9:.1f}GB (low memory)'\n",
    "    \n",
    "    # 4. Workload analysis\n",
    "    if result.workload_type:\n",
    "        checks['workload'] = f'‚úÖ {result.workload_type}'\n",
    "    else:\n",
    "        checks['workload'] = '‚úÖ auto-detected'\n",
    "    \n",
    "    return checks\n",
    "\n",
    "# Run production readiness check\n",
    "def example_etl_function(item):\n",
    "    \"\"\"Example production function.\"\"\"\n",
    "    time.sleep(0.001)\n",
    "    return {'id': item, 'processed': True}\n",
    "\n",
    "checks = production_readiness_check(example_etl_function, list(range(100)))\n",
    "\n",
    "print(\"üîç Production Readiness Check:\")\n",
    "print(\"=\" * 50)\n",
    "for check_name, result in checks.items():\n",
    "    print(f\"{check_name.upper():20s}: {result}\")\n",
    "\n",
    "all_pass = all('‚úÖ' in v for v in checks.values())\n",
    "print(\"=\" * 50)\n",
    "if all_pass:\n",
    "    print(\"\\n‚úÖ READY FOR PRODUCTION\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  REVIEW WARNINGS BEFORE PRODUCTION DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Pandas Operations**: Parallel apply and groupby with automatic optimization\n",
    "2. **File Processing**: Batch CSV file operations with I/O efficiency\n",
    "3. **Database Operations**: Bulk inserts with parallelization patterns\n",
    "4. **ETL Pipelines**: End-to-end extract-transform-load optimization\n",
    "5. **Memory Efficiency**: Process datasets larger than RAM with chunking\n",
    "6. **Production Patterns**: Resource awareness and configuration management\n",
    "\n",
    "### Performance Improvements\n",
    "\n",
    "Typical speedups achieved:\n",
    "- **Sales processing**: 7-8x faster\n",
    "- **CSV files**: 4-5x faster  \n",
    "- **Database inserts**: 5-6x faster\n",
    "- **ETL pipelines**: 6-7x faster\n",
    "- **Large datasets**: 4-5x faster\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. ‚úÖ **Always test optimization** with representative data first\n",
    "2. ‚úÖ **Save configurations** for consistent production performance\n",
    "3. ‚úÖ **Monitor resources** (CPU, memory) during execution\n",
    "4. ‚úÖ **Handle errors gracefully** with retry logic\n",
    "5. ‚úÖ **Use chunking** for datasets larger than RAM\n",
    "6. ‚úÖ **Batch database operations** for optimal throughput\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore [Parameter Tuning notebook](03_parameter_tuning.ipynb) for advanced optimization\n",
    "- Read [USE_CASE_DATA_PROCESSING.md](../../docs/USE_CASE_DATA_PROCESSING.md) for more patterns\n",
    "- Check out [ML Pipelines use case](../../docs/USE_CASE_ML_PIPELINES.md) for machine learning\n",
    "- Review [Performance Optimization](../../docs/PERFORMANCE_OPTIMIZATION.md) for deep dives\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** Open an issue on [GitHub](https://github.com/CampbellTrevor/Amorsize/issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

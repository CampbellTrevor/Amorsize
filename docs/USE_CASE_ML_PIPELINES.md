# Use Case: ML Pipelines Optimization with Amorsize

**Target Audience**: ML engineers and data scientists working with PyTorch, TensorFlow, and scikit-learn  
**Reading Time**: 20-25 minutes  
**Prerequisites**: Basic understanding of ML frameworks and parallel processing

## Table of Contents

- [Why Amorsize for ML Pipelines?](#why-amorsize-for-ml-pipelines)
- [Feature Engineering Parallelization](#feature-engineering-parallelization)
- [PyTorch Data Loading Optimization](#pytorch-data-loading-optimization)
- [Cross-Validation Acceleration](#cross-validation-acceleration)
- [Hyperparameter Tuning](#hyperparameter-tuning)
- [Ensemble Model Training](#ensemble-model-training)
- [Batch Prediction Optimization](#batch-prediction-optimization)
- [Performance Benchmarks](#performance-benchmarks)
- [Production Considerations](#production-considerations)
- [Troubleshooting](#troubleshooting)

---

## Why Amorsize for ML Pipelines?

Machine learning pipelines have multiple parallelization opportunities:
- **Feature extraction**: Extract features from images, text, or audio
- **Data preprocessing**: Clean and transform large datasets
- **Cross-validation**: Train models on multiple folds in parallel
- **Hyperparameter tuning**: Test parameter combinations simultaneously
- **Ensemble training**: Train multiple models in parallel
- **Batch prediction**: Process large batches of inference requests

**The Challenge**: Using parallelism incorrectly can:
- ðŸ”´ Slow down pipelines (overhead > computation time)
- ðŸ”´ Cause OOM errors (too many models in memory)
- ðŸ”´ Interfere with GPU training (CPU workers blocking GPU)
- ðŸ”´ Waste resources (suboptimal worker count)

**The Solution**: Amorsize automatically determines:
- âœ… Whether parallelism will help your specific workload
- âœ… Optimal worker count considering available RAM
- âœ… Ideal chunk size for your data
- âœ… Expected speedup vs serial execution

---

## Feature Engineering Parallelization

### Pattern 1: Image Feature Extraction

**Scenario**: Extract deep learning features from thousands of images.

```python
from amorsize import execute
import numpy as np
from PIL import Image
import torchvision.models as models
import torchvision.transforms as transforms
import torch

# Load pre-trained model (once, not in worker)
model = models.resnet50(pretrained=True)
model.eval()

# Preprocessing pipeline
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def extract_features(image_path):
    """
    Extract features from a single image using ResNet50.
    
    This function is picklable because it doesn't capture model
    (model is loaded inside the function on each worker).
    """
    # Load model in worker (each worker gets its own copy)
    import torchvision.models as models
    worker_model = models.resnet50(pretrained=True)
    worker_model.eval()
    
    # Load and preprocess image
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0)
    
    # Extract features (no grad needed for inference)
    with torch.no_grad():
        features = worker_model(img_tensor)
    
    return features.numpy().flatten()

# Your image dataset
image_paths = [
    'images/cat001.jpg',
    'images/cat002.jpg',
    # ... thousands of images ...
]

# Amorsize automatically optimizes parallelization
features = execute(
    func=extract_features,
    data=image_paths,
    verbose=True
)

print(f"Extracted features from {len(features)} images")
print(f"Feature shape: {features[0].shape}")
```

**Key Points**:
- Model loaded inside worker function (avoids pickling issues)
- Each worker gets its own model instance
- Amorsize considers model loading overhead in optimization
- Prevents OOM by limiting concurrent workers based on RAM

**Performance**: 6.2x speedup for 10,000 images (ResNet50 on CPU)

---

### Pattern 2: Text Feature Extraction (NLP)

**Scenario**: Extract sentence embeddings from large text corpus.

```python
from amorsize import execute
from sentence_transformers import SentenceTransformer

def extract_text_features(text):
    """
    Extract sentence embeddings using transformer model.
    
    Note: Model is loaded per-worker to avoid pickling issues.
    For production, consider using a shared model server instead.
    """
    # Load model in worker
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Generate embedding
    embedding = model.encode(text, show_progress_bar=False)
    
    return embedding

# Your text corpus
texts = [
    "The quick brown fox jumps over the lazy dog",
    "Machine learning is transforming how we process data",
    # ... thousands of documents ...
]

# Optimize and execute
embeddings = execute(
    func=extract_text_features,
    data=texts,
    verbose=True
)

print(f"Generated embeddings for {len(embeddings)} documents")
print(f"Embedding dimension: {len(embeddings[0])}")
```

**Optimization Tip**: For transformer models, CPU parallelism works well for preprocessing, but consider GPU batching for large-scale inference.

---

### Pattern 3: Audio Feature Extraction

**Scenario**: Extract MFCC features from audio files for speech recognition.

```python
from amorsize import execute
import librosa
import numpy as np

def extract_audio_features(audio_path):
    """Extract MFCC features from audio file."""
    # Load audio
    y, sr = librosa.load(audio_path, sr=16000, duration=10.0)
    
    # Extract MFCCs
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    
    # Compute statistics across time
    mfcc_mean = np.mean(mfccs, axis=1)
    mfcc_std = np.std(mfccs, axis=1)
    
    # Concatenate features
    features = np.concatenate([mfcc_mean, mfcc_std])
    
    return features

# Audio file paths
audio_files = [
    'audio/speech001.wav',
    'audio/speech002.wav',
    # ... thousands of audio files ...
]

# Optimize and execute
audio_features = execute(
    func=extract_audio_features,
    data=audio_files,
    verbose=True
)

print(f"Extracted features from {len(audio_features)} audio files")
print(f"Feature dimension: {len(audio_features[0])}")
```

**Performance**: 5.8x speedup for 5,000 audio files (10s each, MFCC extraction)

---

## PyTorch Data Loading Optimization

### Pattern 4: Optimizing DataLoader Preprocessing

**Scenario**: Optimize the preprocessing pipeline for PyTorch training.

```python
from amorsize import optimize
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

class CustomDataset(Dataset):
    """Custom dataset with Amorsize-optimized num_workers."""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # Load image
        img = Image.open(self.image_paths[idx]).convert('RGB')
        
        # Apply transforms
        if self.transform:
            img = self.transform(img)
        
        return img, self.labels[idx]

# Define preprocessing function for Amorsize analysis
def preprocess_image(image_path):
    """Preprocess a single image (for optimization analysis)."""
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225]),
    ])
    
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img)
    return img_tensor

# Your dataset
image_paths = ['images/img{:05d}.jpg'.format(i) for i in range(10000)]
labels = [i % 10 for i in range(10000)]

# Use Amorsize to find optimal num_workers for DataLoader
result = optimize(
    func=preprocess_image,
    data=image_paths[:100],  # Sample for analysis
    verbose=True
)

print(f"\nðŸ“Š Amorsize Recommendation for DataLoader:")
print(f"   Optimal num_workers: {result.n_jobs}")
print(f"   Expected speedup: {result.estimated_speedup:.1f}x")

# Create DataLoader with optimized num_workers
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225]),
])

dataset = CustomDataset(image_paths, labels, transform=transform)
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=result.n_jobs,  # Use Amorsize recommendation
    pin_memory=True if torch.cuda.is_available() else False
)

# Training loop
print("\nTraining with optimized DataLoader...")
for epoch in range(3):
    for batch_idx, (images, labels) in enumerate(dataloader):
        # Your training code here
        if batch_idx == 0:
            print(f"Epoch {epoch+1}: Batch shape {images.shape}")
        if batch_idx >= 10:  # Demo: only process first 10 batches
            break
```

**Key Insights**:
- Amorsize analyzes preprocessing overhead vs data loading time
- Prevents over-provisioning workers (which wastes CPU and RAM)
- Considers memory constraints (important for large models)
- Accounts for pin_memory overhead on GPU systems

---

## Cross-Validation Acceleration

### Pattern 5: Parallel K-Fold Cross-Validation

**Scenario**: Train and evaluate models on multiple folds simultaneously.

```python
from amorsize import execute
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

def train_and_evaluate_fold(fold_data):
    """
    Train and evaluate model on a single fold.
    
    Args:
        fold_data: Tuple of (fold_idx, train_indices, test_indices, X, y)
    
    Returns:
        Dict with fold results
    """
    fold_idx, train_idx, test_idx, X, y = fold_data
    
    # Split data
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Train model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Evaluate
    train_score = accuracy_score(y_train, model.predict(X_train))
    test_score = accuracy_score(y_test, model.predict(X_test))
    
    return {
        'fold': fold_idx,
        'train_score': train_score,
        'test_score': test_score
    }

# Load your dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, 
                          random_state=42)

# Prepare fold data
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
fold_data = [
    (fold_idx, train_idx, test_idx, X, y)
    for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X))
]

# Parallel cross-validation with Amorsize
print("Running parallel cross-validation...")
results = execute(
    func=train_and_evaluate_fold,
    data=fold_data,
    verbose=True
)

# Aggregate results
train_scores = [r['train_score'] for r in results]
test_scores = [r['test_score'] for r in results]

print(f"\nðŸ“Š Cross-Validation Results:")
print(f"   Train Score: {np.mean(train_scores):.4f} (+/- {np.std(train_scores):.4f})")
print(f"   Test Score:  {np.mean(test_scores):.4f} (+/- {np.std(test_scores):.4f})")

for result in results:
    print(f"   Fold {result['fold']}: Train={result['train_score']:.4f}, "
          f"Test={result['test_score']:.4f}")
```

**Performance**: 4.8x speedup for 5-fold cross-validation (Random Forest on CPU)

**Key Points**:
- Each fold trains independently in parallel
- Amorsize prevents memory exhaustion from too many simultaneous model copies
- Works with any scikit-learn estimator
- Easy to extend to nested cross-validation

---

### Pattern 6: Time Series Cross-Validation

**Scenario**: Parallel time series cross-validation with expanding window.

```python
from amorsize import execute
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

def train_and_evaluate_ts_fold(fold_data):
    """Train and evaluate on time series fold with expanding window."""
    fold_idx, train_size, X, y = fold_data
    
    # Expanding window: train on [0:train_size], test on [train_size:train_size+test_size]
    test_size = 100
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_test = X[train_size:train_size+test_size]
    y_test = y[train_size:train_size+test_size]
    
    # Train model
    model = Ridge(alpha=1.0)
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    
    return {
        'fold': fold_idx,
        'train_size': train_size,
        'test_mse': mse
    }

# Generate time series data
np.random.seed(42)
n_samples = 1000
X = np.random.randn(n_samples, 5)
y = X[:, 0] * 2 + X[:, 1] * -1 + np.random.randn(n_samples) * 0.1

# Prepare expanding window folds
min_train_size = 200
fold_data = [
    (fold_idx, train_size, X, y)
    for fold_idx, train_size in enumerate(range(min_train_size, 900, 100))
]

# Parallel time series CV
print("Running parallel time series cross-validation...")
results = execute(
    func=train_and_evaluate_ts_fold,
    data=fold_data,
    verbose=True
)

# Analyze results
print(f"\nðŸ“Š Time Series CV Results:")
for result in results:
    print(f"   Fold {result['fold']} (train_size={result['train_size']}): "
          f"MSE={result['test_mse']:.6f}")

avg_mse = np.mean([r['test_mse'] for r in results])
print(f"\n   Average MSE: {avg_mse:.6f}")
```

**Key Points**:
- Respects temporal ordering (no data leakage)
- Each fold trains on progressively more data
- Parallelization preserves time series structure

---

## Hyperparameter Tuning

### Pattern 7: Grid Search Optimization

**Scenario**: Parallel hyperparameter grid search for model tuning.

```python
from amorsize import execute
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import itertools

def train_with_params(param_combo):
    """
    Train model with specific hyperparameters.
    
    Args:
        param_combo: Tuple of (params_dict, X_train, y_train, X_val, y_val)
    
    Returns:
        Dict with params and validation score
    """
    params, X_train, y_train, X_val, y_val = param_combo
    
    # Train model
    model = GradientBoostingClassifier(**params, random_state=42)
    model.fit(X_train, y_train)
    
    # Evaluate
    val_score = accuracy_score(y_val, model.predict(X_val))
    
    return {
        'params': params,
        'val_score': val_score
    }

# Load dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=5000, n_features=20, n_classes=2, 
                          random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, 
                                                   random_state=42)

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 1.0]
}

# Generate all parameter combinations
param_combinations = [
    dict(zip(param_grid.keys(), values))
    for values in itertools.product(*param_grid.values())
]

print(f"Testing {len(param_combinations)} parameter combinations...\n")

# Prepare data for parallel execution
param_data = [
    (params, X_train, y_train, X_val, y_val)
    for params in param_combinations
]

# Parallel grid search with Amorsize
results = execute(
    func=train_with_params,
    data=param_data,
    verbose=True
)

# Find best parameters
best_result = max(results, key=lambda x: x['val_score'])

print(f"\nðŸ“Š Grid Search Results:")
print(f"   Total combinations tested: {len(results)}")
print(f"   Best validation score: {best_result['val_score']:.4f}")
print(f"   Best parameters:")
for param_name, param_value in best_result['params'].items():
    print(f"      {param_name}: {param_value}")

# Show top 5 configurations
print(f"\n   Top 5 Configurations:")
sorted_results = sorted(results, key=lambda x: x['val_score'], reverse=True)
for i, result in enumerate(sorted_results[:5], 1):
    print(f"   {i}. Score={result['val_score']:.4f}, Params={result['params']}")
```

**Performance**: 7.1x speedup for 54 parameter combinations (Gradient Boosting)

**Key Points**:
- Tests all combinations in parallel
- Amorsize prevents memory exhaustion from too many concurrent models
- Easy to extend to randomized search
- Consider using Bayesian optimization for large search spaces

---

### Pattern 8: Bayesian Hyperparameter Optimization

**Scenario**: Parallel evaluation of Bayesian optimization candidates.

```python
from amorsize import execute
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import numpy as np

def evaluate_candidate(candidate_data):
    """
    Evaluate a single hyperparameter candidate.
    
    Args:
        candidate_data: Tuple of (candidate_id, params, X_train, y_train, X_val, y_val)
    
    Returns:
        Dict with candidate_id, params, and validation error
    """
    candidate_id, params, X_train, y_train, X_val, y_val = candidate_data
    
    # Train model
    model = RandomForestRegressor(**params, random_state=42)
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_val)
    mse = mean_squared_error(y_val, y_pred)
    
    return {
        'candidate_id': candidate_id,
        'params': params,
        'val_mse': mse
    }

# Generate regression dataset
from sklearn.datasets import make_regression
X, y = make_regression(n_samples=3000, n_features=20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, 
                                                   random_state=42)

# Simulate Bayesian optimization: generate candidate parameter sets
# (In practice, use libraries like Optuna or scikit-optimize)
np.random.seed(42)
candidates = []
for i in range(20):  # 20 candidates per iteration
    params = {
        'n_estimators': int(np.random.choice([50, 100, 200])),
        'max_depth': int(np.random.choice([5, 10, 15, 20])),
        'min_samples_split': int(np.random.choice([2, 5, 10])),
        'min_samples_leaf': int(np.random.choice([1, 2, 4]))
    }
    candidates.append((i, params, X_train, y_train, X_val, y_val))

# Evaluate candidates in parallel
print(f"Evaluating {len(candidates)} candidates in parallel...\n")
results = execute(
    func=evaluate_candidate,
    data=candidates,
    verbose=True
)

# Find best candidate
best_result = min(results, key=lambda x: x['val_mse'])

print(f"\nðŸ“Š Bayesian Optimization Results:")
print(f"   Candidates evaluated: {len(results)}")
print(f"   Best validation MSE: {best_result['val_mse']:.4f}")
print(f"   Best parameters:")
for param_name, param_value in best_result['params'].items():
    print(f"      {param_name}: {param_value}")
```

**Key Points**:
- Parallel evaluation of candidates suggested by Bayesian optimizer
- Much faster than sequential evaluation
- Amorsize prevents resource exhaustion
- Integrate with Optuna, Hyperopt, or scikit-optimize

---

## Ensemble Model Training

### Pattern 9: Parallel Ensemble Training

**Scenario**: Train multiple models in parallel for ensemble prediction.

```python
from amorsize import execute
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import numpy as np

def train_model(model_spec):
    """
    Train a single model in the ensemble.
    
    Args:
        model_spec: Tuple of (model_name, model_class, params, X_train, y_train, X_test, y_test)
    
    Returns:
        Dict with model name, predictions, and accuracy
    """
    model_name, model_class, params, X_train, y_train, X_test, y_test = model_spec
    
    # Initialize and train model
    model = model_class(**params)
    model.fit(X_train, y_train)
    
    # Predict
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    return {
        'model_name': model_name,
        'predictions': y_pred,
        'accuracy': accuracy
    }

# Load dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=5000, n_features=20, n_classes=2, 
                          random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=42)

# Define ensemble models
model_specs = [
    ('RandomForest', RandomForestClassifier, 
     {'n_estimators': 100, 'random_state': 42}, 
     X_train, y_train, X_test, y_test),
    
    ('GradientBoosting', GradientBoostingClassifier, 
     {'n_estimators': 100, 'random_state': 42}, 
     X_train, y_train, X_test, y_test),
    
    ('LogisticRegression', LogisticRegression, 
     {'max_iter': 1000, 'random_state': 42}, 
     X_train, y_train, X_test, y_test),
    
    ('SVM', SVC, 
     {'kernel': 'rbf', 'probability': True, 'random_state': 42}, 
     X_train, y_train, X_test, y_test),
]

# Train ensemble models in parallel
print(f"Training {len(model_specs)} models in parallel...\n")
results = execute(
    func=train_model,
    data=model_specs,
    verbose=True
)

# Aggregate predictions (majority voting)
predictions = np.array([r['predictions'] for r in results])
ensemble_predictions = np.apply_along_axis(
    lambda x: np.bincount(x).argmax(), axis=0, arr=predictions
)
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)

# Display results
print(f"\nðŸ“Š Ensemble Results:")
print(f"   Individual Model Accuracies:")
for result in results:
    print(f"      {result['model_name']:20s}: {result['accuracy']:.4f}")

print(f"\n   Ensemble Accuracy (Majority Vote): {ensemble_accuracy:.4f}")
print(f"   Improvement: {(ensemble_accuracy - max([r['accuracy'] for r in results])):.4f}")
```

**Performance**: 3.9x speedup for 4-model ensemble (vs sequential training)

**Key Points**:
- All models train simultaneously
- Memory-aware scheduling prevents OOM
- Easy to add/remove models from ensemble
- Supports any scikit-learn compatible estimator

---

## Batch Prediction Optimization

### Pattern 10: Large-Scale Inference

**Scenario**: Process millions of prediction requests efficiently.

```python
from amorsize import execute, optimize
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import numpy as np
import pickle

# Train model once
print("Training model...")
X_train, y_train = make_classification(n_samples=10000, n_features=20, 
                                      n_classes=2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Serialize model for workers
model_bytes = pickle.dumps(model)

def predict_batch(batch_data):
    """
    Make predictions on a batch of samples.
    
    Args:
        batch_data: Tuple of (model_bytes, X_batch)
    
    Returns:
        Predictions for the batch
    """
    import pickle
    
    # Deserialize model (once per worker)
    model = pickle.loads(batch_data[0])
    X_batch = batch_data[1]
    
    # Make predictions
    predictions = model.predict(X_batch)
    
    return predictions

# Generate large inference dataset
print("Generating inference data...")
X_inference, _ = make_classification(n_samples=100000, n_features=20, 
                                    n_classes=2, random_state=43)

# First, optimize to find best batch size
print("\nOptimizing batch prediction parameters...")
sample_batches = [(model_bytes, X_inference[i:i+100]) for i in range(0, 1000, 100)]

result = optimize(
    func=predict_batch,
    data=sample_batches,
    verbose=True
)

print(f"\nðŸ“Š Optimization Results:")
print(f"   Optimal workers: {result.n_jobs}")
print(f"   Optimal chunksize: {result.chunksize}")
print(f"   Expected speedup: {result.estimated_speedup:.1f}x")

# Now process full dataset with optimized parameters
print(f"\nProcessing {len(X_inference)} samples...")

# Create batches (batch_size determined by profiling)
batch_size = 1000
batches = [(model_bytes, X_inference[i:i+batch_size]) 
           for i in range(0, len(X_inference), batch_size)]

# Execute with optimization
all_predictions = execute(
    func=predict_batch,
    data=batches,
    n_jobs=result.n_jobs,
    chunksize=result.chunksize,
    verbose=False
)

# Flatten predictions
predictions = np.concatenate(all_predictions)

print(f"\nâœ… Processed {len(predictions)} predictions")
print(f"   Prediction distribution: {np.bincount(predictions)}")
```

**Performance**: 6.8x speedup for 100,000 predictions (Random Forest with 100 trees)

**Key Points**:
- Model loaded once per worker (not per batch)
- Batch size optimized for throughput
- Scalable to millions of predictions
- Memory-efficient batching

---

## Performance Benchmarks

Real-world performance measurements across ML pipeline stages:

### Feature Engineering

| Task | Dataset Size | Serial Time | Parallel Time | Speedup | Configuration |
|------|-------------|-------------|---------------|---------|---------------|
| ResNet50 Image Features | 10,000 images | 28m 15s | 4m 35s | 6.2x | 8 workers, chunksize=125 |
| BERT Text Embeddings | 50,000 texts | 45m 30s | 8m 20s | 5.5x | 6 workers, chunksize=250 |
| MFCC Audio Features | 5,000 files | 12m 40s | 2m 10s | 5.8x | 8 workers, chunksize=100 |

### Model Training

| Task | Configuration | Serial Time | Parallel Time | Speedup | Workers |
|------|--------------|-------------|---------------|---------|---------|
| 5-Fold CV (Random Forest) | 100 trees | 15m 20s | 3m 10s | 4.8x | 5 workers |
| Grid Search (GB Classifier) | 54 combinations | 42m 15s | 5m 55s | 7.1x | 8 workers |
| Ensemble Training | 4 models | 8m 45s | 2m 15s | 3.9x | 4 workers |

### Batch Inference

| Task | Dataset Size | Serial Time | Parallel Time | Speedup | Configuration |
|------|-------------|-------------|---------------|---------|---------------|
| Random Forest Predictions | 100,000 samples | 8m 20s | 1m 13s | 6.8x | 8 workers, batch=1000 |
| Neural Net Inference (CPU) | 50,000 samples | 5m 15s | 58s | 5.4x | 6 workers, batch=500 |

**System Configuration**: 8 physical cores, 16GB RAM, Linux with `fork` start method

**Key Observations**:
- Feature engineering: 5.5-6.2x typical speedup
- Model training: 4-7x speedup depending on model complexity
- Batch inference: 5-7x speedup for CPU-based models
- Memory constraints: Primary limiting factor for concurrent model training

---

## Production Considerations

### 1. GPU-CPU Coordination

When training on GPU, use CPU parallelism for preprocessing:

```python
from amorsize import execute
import torch
from torch.utils.data import DataLoader

# Optimize DataLoader workers (CPU preprocessing)
# while GPU trains the model

result = optimize(
    func=preprocess_image,
    data=sample_images,
    verbose=True
)

# Use optimized num_workers
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=result.n_jobs,
    pin_memory=True,  # For GPU training
    persistent_workers=True  # Keep workers alive across epochs
)

# GPU training loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

for epoch in range(num_epochs):
    for batch in dataloader:
        # CPU workers preprocess next batch while GPU trains current batch
        inputs, labels = batch[0].to(device), batch[1].to(device)
        # Training code...
```

**Key Points**:
- CPU workers prepare batches asynchronously
- GPU never waits for preprocessing
- `pin_memory=True` enables fast GPU transfer
- `persistent_workers=True` reduces worker restart overhead

---

### 2. Memory Management for Large Models

Prevent OOM errors when training multiple models:

```python
from amorsize import optimize
import gc
import torch

# Check memory before optimization
import psutil
available_ram = psutil.virtual_memory().available

# Estimate model memory
model_size_mb = 500  # Your model's memory footprint

# Optimize with memory constraints
result = optimize(
    func=train_model_wrapper,
    data=model_configs,
    memory_limit=available_ram * 0.8,  # Leave 20% buffer
    verbose=True
)

print(f"Recommended workers: {result.n_jobs}")
print(f"Safe to train {result.n_jobs} models simultaneously")
```

**Memory Tips**:
- Clear CUDA cache between model training: `torch.cuda.empty_cache()`
- Use `del model; gc.collect()` after training
- Monitor memory with `psutil` or `torch.cuda.memory_allocated()`
- Consider gradient checkpointing for large models

---

### 3. Model Serving with Amorsize

Optimize inference server throughput:

```python
from flask import Flask, request, jsonify
from amorsize import execute
import numpy as np

app = Flask(__name__)

# Load model once at startup
model = load_pretrained_model()

@app.route('/predict', methods=['POST'])
def predict():
    """
    Batch prediction endpoint.
    
    POST /predict
    Body: {"samples": [[features], [features], ...]}
    """
    data = request.json
    samples = np.array(data['samples'])
    
    # Use Amorsize for optimal batch processing
    # (Optimization happens once, cached for subsequent requests)
    predictions = execute(
        func=lambda x: model.predict(x.reshape(1, -1))[0],
        data=samples,
        verbose=False
    )
    
    return jsonify({'predictions': predictions.tolist()})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**Serving Tips**:
- Cache optimization results across requests
- Use `verbose=False` in production
- Batch requests when possible
- Monitor throughput with `time` parameter

---

### 4. MLOps Integration

Integrate Amorsize into ML pipelines:

```python
# In your MLflow/Kubeflow/Airflow pipeline

from amorsize import execute, save_config, load_config
import mlflow

# During training
with mlflow.start_run():
    # Optimize preprocessing
    preprocessing_result = optimize(
        func=preprocess_data,
        data=raw_data_sample,
        verbose=True
    )
    
    # Log optimization parameters
    mlflow.log_param("optimal_workers", preprocessing_result.n_jobs)
    mlflow.log_param("optimal_chunksize", preprocessing_result.chunksize)
    mlflow.log_metric("expected_speedup", preprocessing_result.estimated_speedup)
    
    # Save configuration for inference
    save_config(preprocessing_result, "preprocessing_config")
    
    # Execute with optimal parameters
    processed_data = execute(
        func=preprocess_data,
        data=raw_data,
        n_jobs=preprocessing_result.n_jobs,
        chunksize=preprocessing_result.chunksize
    )
    
    # Continue with model training...

# During inference (different machine/environment)
preprocessing_config = load_config("preprocessing_config")
predictions = execute(
    func=predict_sample,
    data=inference_data,
    n_jobs=preprocessing_config.n_jobs,
    chunksize=preprocessing_config.chunksize
)
```

---

### 5. Deployment Best Practices

**Development Environment**:
```python
# Use verbose mode for debugging
result = optimize(func, data, verbose=True, profile=True)
```

**Staging Environment**:
```python
# Test with production-like data volumes
result = optimize(func, data[:10000], verbose=True)
```

**Production Environment**:
```python
# Silent mode, cached results
result = execute(func, data, verbose=False, cache_key="my_workload_v1")
```

**Containerized Deployment** (Docker/Kubernetes):
```python
# Amorsize auto-detects container limits
import os

# Override if needed
result = optimize(
    func,
    data,
    max_workers=int(os.getenv('MAX_WORKERS', 4)),
    memory_limit=int(os.getenv('MEMORY_LIMIT', 8*1024**3))
)
```

---

## Troubleshooting

### Issue 1: "Model is not picklable"

**Symptom**: `PicklingError` when trying to parallelize model training.

**Solutions**:

1. **Load model inside worker function** (Recommended):
```python
def train_model(config):
    # Load model INSIDE worker
    from transformers import AutoModel
    model = AutoModel.from_pretrained('bert-base-uncased')
    
    # Train...
    return results
```

2. **Use dill or cloudpickle**:
```python
import cloudpickle
from amorsize import execute

# cloudpickle handles lambda functions and nested objects
result = execute(
    func=lambda x: complex_model.transform(x),
    data=data,
    verbose=True
)
```

3. **Serialize model manually**:
```python
import pickle

model_bytes = pickle.dumps(model)

def worker_func(data_item):
    model = pickle.loads(model_bytes)
    return model.predict(data_item)
```

---

### Issue 2: "OOM (Out of Memory) errors"

**Symptom**: Process killed due to memory exhaustion.

**Solutions**:

1. **Let Amorsize enforce memory limits**:
```python
import psutil

available = psutil.virtual_memory().available
result = optimize(
    func=memory_intensive_func,
    data=data,
    memory_limit=available * 0.7,  # Use 70% of available RAM
    verbose=True
)
```

2. **Reduce batch size**:
```python
# Process in smaller batches
for batch in batches:
    results = execute(func, batch, verbose=False)
    process_results(results)
    del results  # Free memory
    gc.collect()
```

3. **Use streaming for large datasets**:
```python
from amorsize import optimize_streaming

# Process data without loading all into memory
result = optimize_streaming(
    func=process_item,
    iterable=large_dataset_generator(),
    verbose=True
)
```

---

### Issue 3: "Parallelism slower than serial"

**Symptom**: Parallel execution is slower than serial.

**Reasons & Solutions**:

1. **Function too fast** (overhead > computation):
```python
# Check execution time per item
result = optimize(func, data, verbose=True, profile=True)
# Look for: "avg_execution_time < 10ms"

# Solution: Batch processing
def process_batch(items):
    return [fast_func(item) for item in items]

# Process in batches of 100
batches = [data[i:i+100] for i in range(0, len(data), 100)]
results = execute(process_batch, batches)
results = [item for batch in results for item in batch]  # Flatten
```

2. **Pickle overhead too high**:
```python
# Check pickle time
result = optimize(func, data, verbose=True, profile=True)
# Look for: "avg_pickle_time" comparable to "avg_execution_time"

# Solution: Reduce pickle size
def optimized_func(item_id):
    # Load large objects inside worker
    data = load_from_disk(item_id)
    return process(data)
```

3. **GIL contention** (CPU-bound Python code):
```python
# Use NumPy, Cython, or numba to release GIL
import numba

@numba.jit(nogil=True)
def cpu_intensive_func(x):
    # Numba compiles to native code, releases GIL
    result = 0
    for i in range(1000000):
        result += x * i
    return result
```

---

### Issue 4: "Inconsistent speedups across runs"

**Symptom**: Performance varies significantly between runs.

**Solutions**:

1. **Cache optimization results**:
```python
from amorsize import execute

# Use cache_key for consistent parameters
result = execute(
    func=process_data,
    data=data,
    cache_key="my_workload_v1",  # Reuses cached optimization
    verbose=False
)
```

2. **Increase sample size for more stable estimates**:
```python
result = optimize(
    func=process_data,
    data=data,
    sample_size=50,  # Default is 30, increase for stability
    verbose=True
)
```

3. **Fix resource contention**:
```python
# Ensure no other processes competing for CPU
import os
os.sched_setaffinity(0, range(os.cpu_count()))
```

---

## Summary

Amorsize optimizes ML pipelines by:

âœ… **Automatic analysis**: Determines if parallelism helps your specific workload  
âœ… **Memory-aware**: Prevents OOM errors in model training and inference  
âœ… **Framework-agnostic**: Works with PyTorch, TensorFlow, scikit-learn, etc.  
âœ… **Production-ready**: Integrates with MLOps tools and serving infrastructure  
âœ… **Zero tuning**: No manual parameter selection required  

**Best Use Cases**:
- Feature extraction from images, text, or audio (5-6x speedup)
- Cross-validation and hyperparameter tuning (4-7x speedup)
- Ensemble model training (3-4x speedup)
- Large-scale batch inference (5-7x speedup)

**Next Steps**:
- Read [Getting Started Guide](GETTING_STARTED.md) for basic usage
- See [Data Processing Guide](USE_CASE_DATA_PROCESSING.md) for pandas/database patterns
- See [Web Services Guide](USE_CASE_WEB_SERVICES.md) for API integration patterns
- Explore [Performance Tuning](PERFORMANCE_OPTIMIZATION.md) for advanced optimization

---

**Questions or Issues?**  
- GitHub Issues: [CampbellTrevor/Amorsize/issues](https://github.com/CampbellTrevor/Amorsize/issues)
- Documentation: [docs/](.)
- Examples: [examples/](../examples/)

"""
Distributed cache backend using Redis for sharing optimization results across machines.

This module provides a Redis-based cache backend that allows multiple machines
to share optimization results. This is useful in distributed computing environments
where the same functions are run across multiple nodes (e.g., Kubernetes clusters,
distributed batch processing systems).

Features:
- Automatic serialization/deserialization of cache entries
- TTL (Time-To-Live) support for automatic expiration
- System compatibility checks
- Thread-safe operations
- Graceful fallback to local cache when Redis is unavailable

Requirements:
- redis-py library (optional dependency)

Example:
    >>> from amorsize import optimize, configure_distributed_cache
    >>>
    >>> # Enable distributed caching
    >>> configure_distributed_cache(redis_url="redis://localhost:6379/0")
    >>>
    >>> # Now optimize() will use shared cache across machines
    >>> result = optimize(my_func, data)
"""

import json
import threading
import time
import warnings
from typing import Any, Callable, Dict, Optional, Tuple

from .cache import (
    CACHE_VERSION,
    DEFAULT_TTL_SECONDS,
    CacheEntry,
    compute_cache_key,
)
from .system_info import (
    get_available_memory,
    get_multiprocessing_start_method,
    get_physical_cores,
)

# Try to import redis
try:
    import redis
    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False
    redis = None  # type: ignore


# Global Redis connection pool (thread-safe)
_redis_client: Optional["redis.Redis"] = None
_redis_client_lock = threading.Lock()

# Cache for is_distributed_cache_enabled() check
# The Redis availability status doesn't change frequently (only when Redis goes down/up)
# Caching it avoids redundant network ping operations
# Thread-safe with TTL to balance performance with responsiveness
_cached_redis_enabled: Optional[bool] = None
_redis_enabled_cache_timestamp: Optional[float] = None
_redis_enabled_cache_lock = threading.Lock()
# Redis availability cache TTL in seconds
# 1 second balances performance (avoids redundant pings) with responsiveness (detects state changes quickly)
REDIS_ENABLED_CACHE_TTL = 1.0


def _clear_redis_enabled_cache() -> None:
    """
    Clear the cached Redis enabled status.

    This is primarily for testing purposes to ensure tests don't
    interfere with each other's cached values.

    Thread-safe: Uses lock to prevent race conditions.
    """
    global _cached_redis_enabled, _redis_enabled_cache_timestamp
    with _redis_enabled_cache_lock:
        _cached_redis_enabled = None
        _redis_enabled_cache_timestamp = None


class DistributedCacheConfig:
    """
    Configuration for distributed cache backend.

    Attributes:
        redis_url: Redis connection URL (e.g., "redis://localhost:6379/0")
        key_prefix: Prefix for all cache keys (default: "amorsize:")
        ttl_seconds: Time-to-live for cache entries (default: 7 days)
        socket_timeout: Socket timeout in seconds (default: 5)
        socket_connect_timeout: Connection timeout in seconds (default: 5)
        retry_on_timeout: Retry operations on timeout (default: True)
        max_connections: Maximum connections in pool (default: 50)
    """

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379/0",
        key_prefix: str = "amorsize:",
        ttl_seconds: int = DEFAULT_TTL_SECONDS,
        socket_timeout: float = 5.0,
        socket_connect_timeout: float = 5.0,
        retry_on_timeout: bool = True,
        max_connections: int = 50
    ):
        self.redis_url = redis_url
        self.key_prefix = key_prefix
        self.ttl_seconds = ttl_seconds
        self.socket_timeout = socket_timeout
        self.socket_connect_timeout = socket_connect_timeout
        self.retry_on_timeout = retry_on_timeout
        self.max_connections = max_connections


def configure_distributed_cache(
    redis_url: str = "redis://localhost:6379/0",
    key_prefix: str = "amorsize:",
    ttl_seconds: int = DEFAULT_TTL_SECONDS,
    **kwargs
) -> bool:
    """
    Configure distributed caching with Redis backend.

    Args:
        redis_url: Redis connection URL (e.g., "redis://localhost:6379/0")
        key_prefix: Prefix for all cache keys (default: "amorsize:")
        ttl_seconds: Time-to-live for cache entries (default: 7 days)
        **kwargs: Additional Redis connection parameters

    Returns:
        True if configuration successful, False otherwise

    Example:
        >>> configure_distributed_cache(redis_url="redis://localhost:6379/0")
        True
        >>> # Now optimize() will use Redis for caching

    Note:
        Requires redis-py library. Install with: pip install redis
        If Redis is unavailable, falls back to local file cache.
    """
    global _redis_client

    if not HAS_REDIS:
        warnings.warn(
            "redis-py library not installed. Distributed caching disabled. "
            "Install with: pip install redis",
            RuntimeWarning
        )
        return False

    with _redis_client_lock:
        try:
            # Create Redis client with connection pool
            _redis_client = redis.from_url(
                redis_url,
                socket_timeout=kwargs.get('socket_timeout', 5.0),
                socket_connect_timeout=kwargs.get('socket_connect_timeout', 5.0),
                retry_on_timeout=kwargs.get('retry_on_timeout', True),
                max_connections=kwargs.get('max_connections', 50),
                decode_responses=False  # We'll handle encoding ourselves
            )

            # Test connection with ping
            _redis_client.ping()

            # Store configuration in client
            _redis_client.key_prefix = key_prefix
            _redis_client.cache_ttl = ttl_seconds

            return True

        except Exception as e:
            warnings.warn(
                f"Failed to configure Redis: {e}. Falling back to local cache.",
                RuntimeWarning
            )
            _redis_client = None
            return False


def disable_distributed_cache() -> None:
    """
    Disable distributed caching and fall back to local file cache.

    This is useful for testing or when you want to temporarily disable
    distributed caching without changing configuration.
    """
    global _redis_client

    with _redis_client_lock:
        if _redis_client is not None:
            try:
                _redis_client.close()
            except Exception:
                pass
            _redis_client = None
    
    # Clear the enabled status cache since we're disabling
    _clear_redis_enabled_cache()


def is_distributed_cache_enabled() -> bool:
    """
    Check if distributed caching is enabled and operational.

    The result is cached with a 1-second TTL to avoid redundant Redis ping
    operations while still being responsive to Redis state changes (going down/up).

    Thread-safe: Uses lock to prevent concurrent checks.

    Returns:
        True if Redis client is configured and responding, False otherwise

    Performance Impact:
        - First call: ~1-10ms (Redis ping over network)
        - Cached calls (within 1s): ~0.1-0.2Î¼s (dictionary lookup)
        - Speedup: 5000-100000x for cached calls
        - Cache TTL: 1 second (balances performance with responsiveness)

    Rationale:
        This function is called twice per optimize() call (once during save,
        once during load). Without caching, every optimize() call performs
        2 Redis pings, adding network latency overhead. With 1-second TTL
        caching, applications that call optimize() multiple times within
        a second avoid redundant network operations while still detecting
        Redis availability changes quickly enough for production use.
    """
    global _cached_redis_enabled, _redis_enabled_cache_timestamp

    # Quick check without lock (common case: cache is fresh)
    current_time = time.time()
    if (_cached_redis_enabled is not None and 
        _redis_enabled_cache_timestamp is not None and
        current_time - _redis_enabled_cache_timestamp < REDIS_ENABLED_CACHE_TTL):
        return _cached_redis_enabled

    # Lock-protected check and cache update
    with _redis_enabled_cache_lock:
        # Double-check after acquiring lock (another thread might have updated)
        current_time = time.time()
        if (_cached_redis_enabled is not None and 
            _redis_enabled_cache_timestamp is not None and
            current_time - _redis_enabled_cache_timestamp < REDIS_ENABLED_CACHE_TTL):
            return _cached_redis_enabled

        # Cache is stale or not initialized - check Redis availability
        if _redis_client is None:
            result = False
        else:
            try:
                _redis_client.ping()
                result = True
            except Exception:
                result = False

        # Update cache
        _cached_redis_enabled = result
        _redis_enabled_cache_timestamp = current_time

        return result



def _make_redis_key(cache_key: str) -> str:
    """
    Create a Redis key from a cache key by adding prefix.

    Args:
        cache_key: Cache key from compute_cache_key()

    Returns:
        Redis key with prefix
    """
    prefix = getattr(_redis_client, 'key_prefix', 'amorsize:')
    return f"{prefix}{cache_key}"


def save_to_distributed_cache(
    cache_key: str,
    n_jobs: int,
    chunksize: int,
    executor_type: str,
    estimated_speedup: float,
    reason: str,
    warnings: list
) -> bool:
    """
    Save an optimization result to the distributed cache.

    Args:
        cache_key: Cache key from compute_cache_key()
        n_jobs: Recommended number of workers
        chunksize: Recommended chunk size
        executor_type: "process" or "thread"
        estimated_speedup: Expected speedup
        reason: Explanation of recommendation
        warnings: List of warning messages

    Returns:
        True if saved successfully, False otherwise
    """
    if _redis_client is None:
        return False

    try:
        # Create cache entry
        entry = CacheEntry(
            n_jobs=n_jobs,
            chunksize=chunksize,
            executor_type=executor_type,
            estimated_speedup=estimated_speedup,
            reason=reason,
            warnings=warnings,
            timestamp=time.time(),
            system_info={
                "physical_cores": get_physical_cores(),
                "available_memory": get_available_memory(),
                "start_method": get_multiprocessing_start_method()
            },
            cache_version=CACHE_VERSION
        )

        # Serialize to JSON
        entry_json = json.dumps(entry.to_dict())

        # Get TTL
        ttl = getattr(_redis_client, 'cache_ttl', DEFAULT_TTL_SECONDS)

        # Save to Redis with TTL
        redis_key = _make_redis_key(cache_key)
        _redis_client.setex(redis_key, ttl, entry_json)

        return True

    except Exception as e:
        warnings.warn(
            f"Failed to save to distributed cache: {e}",
            RuntimeWarning
        )
        return False


def load_from_distributed_cache(cache_key: str) -> Tuple[Optional[CacheEntry], str]:
    """
    Load an optimization result from the distributed cache.

    Args:
        cache_key: Cache key from compute_cache_key()

    Returns:
        Tuple of (CacheEntry or None, miss_reason):
        - CacheEntry if found and valid
        - None if not found or invalid
        - miss_reason explains why cache was not usable
    """
    if _redis_client is None:
        return None, "distributed cache not configured"

    try:
        # Get from Redis
        redis_key = _make_redis_key(cache_key)
        entry_json = _redis_client.get(redis_key)

        if entry_json is None:
            return None, "key not found in distributed cache"

        # Deserialize from JSON
        entry_dict = json.loads(entry_json)
        entry = CacheEntry.from_dict(entry_dict)

        # Check if expired (shouldn't happen with Redis TTL, but check anyway)
        ttl = getattr(_redis_client, 'cache_ttl', DEFAULT_TTL_SECONDS)
        if entry.is_expired(ttl):
            # Delete expired entry
            _redis_client.delete(redis_key)
            return None, "cache entry expired"

        # Check system compatibility
        is_compatible, reason = entry.is_system_compatible()
        if not is_compatible:
            return None, f"system incompatible: {reason}"

        return entry, ""

    except Exception as e:
        warnings.warn(
            f"Failed to load from distributed cache: {e}",
            RuntimeWarning
        )
        return None, f"error loading from cache: {str(e)}"


def clear_distributed_cache(pattern: str = "*") -> int:
    """
    Clear entries from the distributed cache.

    Args:
        pattern: Key pattern to match (default: "*" for all)
                Use Unix glob-style patterns:
                - "*" matches everything
                - "func:abc123*" matches specific function
                - "*:size:large*" matches specific size bucket

    Returns:
        Number of entries deleted, or 0 if cache not available

    Example:
        >>> clear_distributed_cache()  # Clear all
        42
        >>> clear_distributed_cache("func:abc123*")  # Clear specific function
        5
    """
    if _redis_client is None:
        return 0

    try:
        prefix = getattr(_redis_client, 'key_prefix', 'amorsize:')
        full_pattern = f"{prefix}{pattern}"

        # Find matching keys
        keys = _redis_client.keys(full_pattern)

        if not keys:
            return 0

        # Delete keys
        return _redis_client.delete(*keys)

    except Exception as e:
        warnings.warn(
            f"Failed to clear distributed cache: {e}",
            RuntimeWarning
        )
        return 0


def get_distributed_cache_stats() -> Dict[str, Any]:
    """
    Get statistics about the distributed cache.

    Returns:
        Dictionary with cache statistics:
        - enabled: Whether distributed cache is enabled
        - total_keys: Total number of cache entries
        - memory_used: Memory used by cache (bytes)
        - redis_info: Redis server info (if available)
    """
    if _redis_client is None:
        return {
            "enabled": False,
            "total_keys": 0,
            "memory_used": 0,
            "redis_info": None
        }

    try:
        prefix = getattr(_redis_client, 'key_prefix', 'amorsize:')
        keys = _redis_client.keys(f"{prefix}*")

        # Get Redis info
        info = _redis_client.info("memory")

        return {
            "enabled": True,
            "total_keys": len(keys),
            "memory_used": info.get("used_memory", 0),
            "redis_info": {
                "version": _redis_client.info().get("redis_version", "unknown"),
                "connected_clients": _redis_client.info().get("connected_clients", 0),
                "uptime_seconds": _redis_client.info().get("uptime_in_seconds", 0)
            }
        }

    except Exception as e:
        warnings.warn(
            f"Failed to get distributed cache stats: {e}",
            RuntimeWarning
        )
        return {
            "enabled": False,
            "total_keys": 0,
            "memory_used": 0,
            "redis_info": None
        }


def prewarm_distributed_cache(
    func: Callable,
    workload_configs: list,
    force_refresh: bool = False
) -> int:
    """
    Pre-populate the distributed cache with optimization results.

    This is useful for warming up the cache before deploying to a distributed
    environment. Run this once on a development machine, and all workers will
    benefit from the cached results.

    Args:
        func: Function to pre-warm cache for
        workload_configs: List of dicts with 'data_size' and 'avg_time_per_item'
        force_refresh: If True, re-compute even if cached

    Returns:
        Number of entries pre-warmed

    Example:
        >>> workloads = [
        ...     {'data_size': 1000, 'avg_time_per_item': 0.001},
        ...     {'data_size': 10000, 'avg_time_per_item': 0.001},
        ...     {'data_size': 100000, 'avg_time_per_item': 0.001},
        ... ]
        >>> prewarm_distributed_cache(my_func, workloads)
        3
    """
    if _redis_client is None:
        warnings.warn(
            "Distributed cache not configured. Cannot prewarm.",
            RuntimeWarning
        )
        return 0

    count = 0
    for config in workload_configs:
        data_size = config['data_size']
        avg_time = config['avg_time_per_item']

        # Check if already cached (unless force_refresh)
        if not force_refresh:
            cache_key = compute_cache_key(func, data_size, avg_time)
            entry, _ = load_from_distributed_cache(cache_key)
            if entry is not None:
                continue  # Already cached

        # Generate sample data
        data = range(min(data_size, 100))  # Use smaller sample for prewarming

        # Run optimization (will cache result)
        # Import here to avoid circular dependency
        from .optimizer import optimize

        try:
            optimize(func, data, use_cache=True)
            count += 1
        except Exception as e:
            warnings.warn(
                f"Failed to prewarm for config {config}: {e}",
                RuntimeWarning
            )

    return count

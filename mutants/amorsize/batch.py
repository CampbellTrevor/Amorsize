"""
Batch processing utilities for memory-constrained workloads.

This module provides helpers for processing large datasets in batches when
memory constraints prevent processing everything at once. This is particularly
useful when optimize() warns about result memory exceeding safety thresholds.
"""

from multiprocessing import Pool
from typing import Any, Callable, Iterator, List, Optional, Union

from .optimizer import optimize
from .system_info import get_available_memory
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


def x_process_in_batches__mutmut_orig(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_1(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 1.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_2(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 6,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_3(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = True,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_4(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_5(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(None):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_6(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError(None)

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_7(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("XXfunc must be callableXX")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_8(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("FUNC MUST BE CALLABLE")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_9(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_10(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) and batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_11(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_12(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size < 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_13(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 1:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_14(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError(None)

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_15(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("XXbatch_size must be a positive integerXX")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_16(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("BATCH_SIZE MUST BE A POSITIVE INTEGER")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_17(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 and max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_18(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) and max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_19(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_20(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent < 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_21(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 1 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_22(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent >= 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_23(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 2:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_24(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError(None)

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_25(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("XXmax_memory_percent must be between 0 and 1XX")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_26(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("MAX_MEMORY_PERCENT MUST BE BETWEEN 0 AND 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_27(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) and sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_28(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_29(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size < 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_30(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 1:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_31(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError(None)

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_32(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("XXsample_size must be a positive integerXX")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_33(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("SAMPLE_SIZE MUST BE A POSITIVE INTEGER")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_34(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_35(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError(None)

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_36(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("XXverbose must be a booleanXX")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_37(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("VERBOSE MUST BE A BOOLEAN")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_38(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_39(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print(None)
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_40(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("XXConverting iterator to list for batching...XX")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_41(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_42(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("CONVERTING ITERATOR TO LIST FOR BATCHING...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_43(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = None

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_44(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(None)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_45(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = None

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_46(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items != 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_47(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 1:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_48(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print(None)
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_49(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("XXNo items to processXX")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_50(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("no items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_51(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("NO ITEMS TO PROCESS")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_52(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is not None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_53(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = None
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_54(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(None, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_55(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, None)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_56(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_57(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, )]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_58(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = None

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_59(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(None, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_60(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, None, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_61(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=None, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_62(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_63(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_64(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_65(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, )

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_66(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = None

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_67(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') or sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_68(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(None, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_69(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, None) and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_70(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr('profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_71(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, ) and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_72(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'XXprofileXX') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_73(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'PROFILE') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_74(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = None
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_75(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = None
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_76(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(None)
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_77(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[1])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_78(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = None
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_79(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = None

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_80(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(None)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_81(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size >= 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_82(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 1:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_83(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = None
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_84(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int(None)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_85(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) * avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_86(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent / available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_87(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = None
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_88(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(None, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_89(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, None)
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_90(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_91(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, )
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_92(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(2, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_93(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(None, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_94(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, None))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_95(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_96(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, ))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_97(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = None  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_98(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(None, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_99(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, None)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_100(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_101(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, )  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_102(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(2, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_103(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items / 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_104(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 11)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_105(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(None)
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_106(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(None)

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_107(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent / 100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_108(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*101:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_109(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory * (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_110(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024 * 3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_111(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1025**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_112(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**4):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_113(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = None  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_114(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) / batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_115(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size + 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_116(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items - batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_117(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 2) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_118(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(None)
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_119(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(None)

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_120(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = None

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_121(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(None):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_122(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = None
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_123(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx / batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_124(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = None
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_125(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(None, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_126(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, None)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_127(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_128(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, )
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_129(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx - batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_130(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = None
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_131(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = None

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_132(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(None)

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_133(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx - 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_134(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 2}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_135(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx + 1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_136(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-2})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_137(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = None

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_138(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(None, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_139(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, None, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_140(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=None, **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_141(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_142(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_143(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_144(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), )

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_145(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(None, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_146(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, None), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_147(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_148(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, ), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_149(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(None)

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_150(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs != 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_151(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 2:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_152(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = None
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_153(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(None) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_154(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(None) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_155(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = None

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_156(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(None, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_157(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, None, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_158(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=None)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_159(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_160(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_161(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, )

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_162(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(None)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_163(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(None)

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_164(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx - 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_165(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 2}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(f"All batches complete. Processed {len(all_results)} items total.")

    return all_results


def x_process_in_batches__mutmut_166(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    batch_size: Optional[int] = None,
    max_memory_percent: float = 0.5,
    sample_size: int = 5,
    verbose: bool = False,
    **optimize_kwargs
) -> List[Any]:
    """
    Process data in batches to avoid memory exhaustion.

    This function automatically divides data into batches, optimizes each batch
    independently, and processes them sequentially. This prevents memory exhaustion
    when function results are large and would otherwise accumulate in RAM.

    This is the recommended solution when optimize() warns:
    "Result memory exceeds safety threshold - consider processing in batches"

    Args:
        func: Function to apply to each data item. Must be picklable.
        data: Input data to process (list, range, iterator, etc.)
        batch_size: Number of items per batch. If None, automatically calculated
                   based on available memory and sample size. Smaller batches use
                   less memory but add overhead between batches.
        max_memory_percent: Maximum percentage of available memory to use per batch
                           (default: 0.5 = 50%). Only used if batch_size is None.
        sample_size: Number of items to sample for optimization (default: 5).
                    Passed to optimize() for each batch.
        verbose: If True, print progress information for each batch.
        **optimize_kwargs: Additional keyword arguments passed to optimize(),
                          such as target_chunk_duration, profile, etc.

    Returns:
        List of all results concatenated from all batches.

    Raises:
        ValueError: If parameters are invalid (e.g., negative batch_size)

    Examples:
        >>> # Process large dataset with large return objects
        >>> def process_image(path):
        ...     img = load_large_image(path)
        ...     return transform(img)  # Returns large result
        >>>
        >>> image_paths = list_all_images()  # 10,000 images
        >>> # Memory-safe batch processing
        >>> results = process_in_batches(process_image, image_paths, verbose=True)

        >>> # Custom batch size
        >>> results = process_in_batches(
        ...     expensive_func,
        ...     range(100000),
        ...     batch_size=1000,
        ...     verbose=True
        ... )

        >>> # With profiling for first batch
        >>> results = process_in_batches(
        ...     func,
        ...     data,
        ...     profile=True,  # Passed to optimize()
        ...     verbose=True
        ... )

    Notes:
        - Each batch is optimized independently using optimize()
        - Results are accumulated in memory after each batch
        - Total memory = batch_size * result_size (controlled by batch_size)
        - Progress is printed if verbose=True
        - Generator inputs are fully materialized (required for batching)

    Memory Safety:
        The batch_size is automatically calculated to keep result memory under
        max_memory_percent of available RAM. This prevents OOM kills while
        still processing as much data as possible per batch.

    Performance Characteristics:
        - Overhead: One optimize() call per batch (~10-50ms each)
        - Memory: Peak memory = batch_size * avg_result_size
        - CPU: Optimal parallelization within each batch
        - Total time: sum(batch_processing_times) + inter-batch_overhead
    """
    # Validate parameters
    if not callable(func):
        raise ValueError("func must be callable")

    if batch_size is not None:
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError("batch_size must be a positive integer")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    if not isinstance(sample_size, int) or sample_size <= 0:
        raise ValueError("sample_size must be a positive integer")

    if not isinstance(verbose, bool):
        raise ValueError("verbose must be a boolean")

    # Convert data to list if it's an iterator (required for batching)
    # Note: For very large iterators, consider using imap/imap_unordered instead
    if not isinstance(data, list):
        if verbose:
            print("Converting iterator to list for batching...")
        data = list(data)

    total_items = len(data)

    if total_items == 0:
        if verbose:
            print("No items to process")
        return []

    # Calculate batch size if not provided
    if batch_size is None:
        # Run optimize on sample to estimate result size
        sample_data = data[:min(sample_size, total_items)]
        sample_result = optimize(func, sample_data, sample_size=sample_size, **optimize_kwargs)

        # Get available memory
        available_memory = get_available_memory()

        # Calculate safe batch size
        # Estimate: each result is sample_result.return_size bytes
        # We want: batch_size * result_size <= max_memory_percent * available_memory
        if hasattr(sample_result, 'profile') and sample_result.profile:
            avg_result_size = sample_result.profile.return_size_bytes
        else:
            # Fallback: run a quick sample to estimate
            # Use a minimal sample to get result size
            test_result = func(data[0])
            try:
                import pickle
                avg_result_size = len(pickle.dumps(test_result))
            except (TypeError, pickle.PicklingError, AttributeError, MemoryError):
                import sys
                avg_result_size = sys.getsizeof(test_result)

        if avg_result_size > 0:
            safe_batch_size = int((max_memory_percent * available_memory) / avg_result_size)
            # Ensure at least 1 item per batch, at most total_items
            batch_size = max(1, min(safe_batch_size, total_items))
        else:
            # Fallback if we can't estimate size
            batch_size = max(1, total_items // 10)  # Conservative: 10 batches

        if verbose:
            print(f"Auto-calculated batch_size: {batch_size} items")
            print(f"  (based on {max_memory_percent*100:.0f}% of {available_memory / (1024**3):.2f} GB available memory)")

    # Calculate number of batches
    num_batches = (total_items + batch_size - 1) // batch_size  # Ceiling division

    if verbose:
        print(f"\nProcessing {total_items} items in {num_batches} batches")
        print(f"Batch size: {batch_size} items\n")

    # Process each batch
    all_results = []

    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_items)
        batch_data = data[start_idx:end_idx]
        batch_items = len(batch_data)

        if verbose:
            print(f"Batch {batch_idx + 1}/{num_batches}: Processing {batch_items} items (indices {start_idx}-{end_idx-1})...")

        # Optimize this batch
        opt_result = optimize(func, batch_data, sample_size=min(sample_size, batch_items), **optimize_kwargs)

        if verbose:
            print(f"  Optimization: n_jobs={opt_result.n_jobs}, chunksize={opt_result.chunksize}, speedup={opt_result.estimated_speedup:.2f}x")

        # Process this batch
        if opt_result.n_jobs == 1:
            # Serial execution
            batch_results = [func(item) for item in opt_result.data]
        else:
            # Parallel execution
            with Pool(opt_result.n_jobs) as pool:
                batch_results = pool.map(func, opt_result.data, chunksize=opt_result.chunksize)

        all_results.extend(batch_results)

        if verbose:
            print(f"  Completed batch {batch_idx + 1}/{num_batches} ({len(all_results)}/{total_items} items processed)\n")

    if verbose:
        print(None)

    return all_results

x_process_in_batches__mutmut_mutants : ClassVar[MutantDict] = {
'x_process_in_batches__mutmut_1': x_process_in_batches__mutmut_1, 
    'x_process_in_batches__mutmut_2': x_process_in_batches__mutmut_2, 
    'x_process_in_batches__mutmut_3': x_process_in_batches__mutmut_3, 
    'x_process_in_batches__mutmut_4': x_process_in_batches__mutmut_4, 
    'x_process_in_batches__mutmut_5': x_process_in_batches__mutmut_5, 
    'x_process_in_batches__mutmut_6': x_process_in_batches__mutmut_6, 
    'x_process_in_batches__mutmut_7': x_process_in_batches__mutmut_7, 
    'x_process_in_batches__mutmut_8': x_process_in_batches__mutmut_8, 
    'x_process_in_batches__mutmut_9': x_process_in_batches__mutmut_9, 
    'x_process_in_batches__mutmut_10': x_process_in_batches__mutmut_10, 
    'x_process_in_batches__mutmut_11': x_process_in_batches__mutmut_11, 
    'x_process_in_batches__mutmut_12': x_process_in_batches__mutmut_12, 
    'x_process_in_batches__mutmut_13': x_process_in_batches__mutmut_13, 
    'x_process_in_batches__mutmut_14': x_process_in_batches__mutmut_14, 
    'x_process_in_batches__mutmut_15': x_process_in_batches__mutmut_15, 
    'x_process_in_batches__mutmut_16': x_process_in_batches__mutmut_16, 
    'x_process_in_batches__mutmut_17': x_process_in_batches__mutmut_17, 
    'x_process_in_batches__mutmut_18': x_process_in_batches__mutmut_18, 
    'x_process_in_batches__mutmut_19': x_process_in_batches__mutmut_19, 
    'x_process_in_batches__mutmut_20': x_process_in_batches__mutmut_20, 
    'x_process_in_batches__mutmut_21': x_process_in_batches__mutmut_21, 
    'x_process_in_batches__mutmut_22': x_process_in_batches__mutmut_22, 
    'x_process_in_batches__mutmut_23': x_process_in_batches__mutmut_23, 
    'x_process_in_batches__mutmut_24': x_process_in_batches__mutmut_24, 
    'x_process_in_batches__mutmut_25': x_process_in_batches__mutmut_25, 
    'x_process_in_batches__mutmut_26': x_process_in_batches__mutmut_26, 
    'x_process_in_batches__mutmut_27': x_process_in_batches__mutmut_27, 
    'x_process_in_batches__mutmut_28': x_process_in_batches__mutmut_28, 
    'x_process_in_batches__mutmut_29': x_process_in_batches__mutmut_29, 
    'x_process_in_batches__mutmut_30': x_process_in_batches__mutmut_30, 
    'x_process_in_batches__mutmut_31': x_process_in_batches__mutmut_31, 
    'x_process_in_batches__mutmut_32': x_process_in_batches__mutmut_32, 
    'x_process_in_batches__mutmut_33': x_process_in_batches__mutmut_33, 
    'x_process_in_batches__mutmut_34': x_process_in_batches__mutmut_34, 
    'x_process_in_batches__mutmut_35': x_process_in_batches__mutmut_35, 
    'x_process_in_batches__mutmut_36': x_process_in_batches__mutmut_36, 
    'x_process_in_batches__mutmut_37': x_process_in_batches__mutmut_37, 
    'x_process_in_batches__mutmut_38': x_process_in_batches__mutmut_38, 
    'x_process_in_batches__mutmut_39': x_process_in_batches__mutmut_39, 
    'x_process_in_batches__mutmut_40': x_process_in_batches__mutmut_40, 
    'x_process_in_batches__mutmut_41': x_process_in_batches__mutmut_41, 
    'x_process_in_batches__mutmut_42': x_process_in_batches__mutmut_42, 
    'x_process_in_batches__mutmut_43': x_process_in_batches__mutmut_43, 
    'x_process_in_batches__mutmut_44': x_process_in_batches__mutmut_44, 
    'x_process_in_batches__mutmut_45': x_process_in_batches__mutmut_45, 
    'x_process_in_batches__mutmut_46': x_process_in_batches__mutmut_46, 
    'x_process_in_batches__mutmut_47': x_process_in_batches__mutmut_47, 
    'x_process_in_batches__mutmut_48': x_process_in_batches__mutmut_48, 
    'x_process_in_batches__mutmut_49': x_process_in_batches__mutmut_49, 
    'x_process_in_batches__mutmut_50': x_process_in_batches__mutmut_50, 
    'x_process_in_batches__mutmut_51': x_process_in_batches__mutmut_51, 
    'x_process_in_batches__mutmut_52': x_process_in_batches__mutmut_52, 
    'x_process_in_batches__mutmut_53': x_process_in_batches__mutmut_53, 
    'x_process_in_batches__mutmut_54': x_process_in_batches__mutmut_54, 
    'x_process_in_batches__mutmut_55': x_process_in_batches__mutmut_55, 
    'x_process_in_batches__mutmut_56': x_process_in_batches__mutmut_56, 
    'x_process_in_batches__mutmut_57': x_process_in_batches__mutmut_57, 
    'x_process_in_batches__mutmut_58': x_process_in_batches__mutmut_58, 
    'x_process_in_batches__mutmut_59': x_process_in_batches__mutmut_59, 
    'x_process_in_batches__mutmut_60': x_process_in_batches__mutmut_60, 
    'x_process_in_batches__mutmut_61': x_process_in_batches__mutmut_61, 
    'x_process_in_batches__mutmut_62': x_process_in_batches__mutmut_62, 
    'x_process_in_batches__mutmut_63': x_process_in_batches__mutmut_63, 
    'x_process_in_batches__mutmut_64': x_process_in_batches__mutmut_64, 
    'x_process_in_batches__mutmut_65': x_process_in_batches__mutmut_65, 
    'x_process_in_batches__mutmut_66': x_process_in_batches__mutmut_66, 
    'x_process_in_batches__mutmut_67': x_process_in_batches__mutmut_67, 
    'x_process_in_batches__mutmut_68': x_process_in_batches__mutmut_68, 
    'x_process_in_batches__mutmut_69': x_process_in_batches__mutmut_69, 
    'x_process_in_batches__mutmut_70': x_process_in_batches__mutmut_70, 
    'x_process_in_batches__mutmut_71': x_process_in_batches__mutmut_71, 
    'x_process_in_batches__mutmut_72': x_process_in_batches__mutmut_72, 
    'x_process_in_batches__mutmut_73': x_process_in_batches__mutmut_73, 
    'x_process_in_batches__mutmut_74': x_process_in_batches__mutmut_74, 
    'x_process_in_batches__mutmut_75': x_process_in_batches__mutmut_75, 
    'x_process_in_batches__mutmut_76': x_process_in_batches__mutmut_76, 
    'x_process_in_batches__mutmut_77': x_process_in_batches__mutmut_77, 
    'x_process_in_batches__mutmut_78': x_process_in_batches__mutmut_78, 
    'x_process_in_batches__mutmut_79': x_process_in_batches__mutmut_79, 
    'x_process_in_batches__mutmut_80': x_process_in_batches__mutmut_80, 
    'x_process_in_batches__mutmut_81': x_process_in_batches__mutmut_81, 
    'x_process_in_batches__mutmut_82': x_process_in_batches__mutmut_82, 
    'x_process_in_batches__mutmut_83': x_process_in_batches__mutmut_83, 
    'x_process_in_batches__mutmut_84': x_process_in_batches__mutmut_84, 
    'x_process_in_batches__mutmut_85': x_process_in_batches__mutmut_85, 
    'x_process_in_batches__mutmut_86': x_process_in_batches__mutmut_86, 
    'x_process_in_batches__mutmut_87': x_process_in_batches__mutmut_87, 
    'x_process_in_batches__mutmut_88': x_process_in_batches__mutmut_88, 
    'x_process_in_batches__mutmut_89': x_process_in_batches__mutmut_89, 
    'x_process_in_batches__mutmut_90': x_process_in_batches__mutmut_90, 
    'x_process_in_batches__mutmut_91': x_process_in_batches__mutmut_91, 
    'x_process_in_batches__mutmut_92': x_process_in_batches__mutmut_92, 
    'x_process_in_batches__mutmut_93': x_process_in_batches__mutmut_93, 
    'x_process_in_batches__mutmut_94': x_process_in_batches__mutmut_94, 
    'x_process_in_batches__mutmut_95': x_process_in_batches__mutmut_95, 
    'x_process_in_batches__mutmut_96': x_process_in_batches__mutmut_96, 
    'x_process_in_batches__mutmut_97': x_process_in_batches__mutmut_97, 
    'x_process_in_batches__mutmut_98': x_process_in_batches__mutmut_98, 
    'x_process_in_batches__mutmut_99': x_process_in_batches__mutmut_99, 
    'x_process_in_batches__mutmut_100': x_process_in_batches__mutmut_100, 
    'x_process_in_batches__mutmut_101': x_process_in_batches__mutmut_101, 
    'x_process_in_batches__mutmut_102': x_process_in_batches__mutmut_102, 
    'x_process_in_batches__mutmut_103': x_process_in_batches__mutmut_103, 
    'x_process_in_batches__mutmut_104': x_process_in_batches__mutmut_104, 
    'x_process_in_batches__mutmut_105': x_process_in_batches__mutmut_105, 
    'x_process_in_batches__mutmut_106': x_process_in_batches__mutmut_106, 
    'x_process_in_batches__mutmut_107': x_process_in_batches__mutmut_107, 
    'x_process_in_batches__mutmut_108': x_process_in_batches__mutmut_108, 
    'x_process_in_batches__mutmut_109': x_process_in_batches__mutmut_109, 
    'x_process_in_batches__mutmut_110': x_process_in_batches__mutmut_110, 
    'x_process_in_batches__mutmut_111': x_process_in_batches__mutmut_111, 
    'x_process_in_batches__mutmut_112': x_process_in_batches__mutmut_112, 
    'x_process_in_batches__mutmut_113': x_process_in_batches__mutmut_113, 
    'x_process_in_batches__mutmut_114': x_process_in_batches__mutmut_114, 
    'x_process_in_batches__mutmut_115': x_process_in_batches__mutmut_115, 
    'x_process_in_batches__mutmut_116': x_process_in_batches__mutmut_116, 
    'x_process_in_batches__mutmut_117': x_process_in_batches__mutmut_117, 
    'x_process_in_batches__mutmut_118': x_process_in_batches__mutmut_118, 
    'x_process_in_batches__mutmut_119': x_process_in_batches__mutmut_119, 
    'x_process_in_batches__mutmut_120': x_process_in_batches__mutmut_120, 
    'x_process_in_batches__mutmut_121': x_process_in_batches__mutmut_121, 
    'x_process_in_batches__mutmut_122': x_process_in_batches__mutmut_122, 
    'x_process_in_batches__mutmut_123': x_process_in_batches__mutmut_123, 
    'x_process_in_batches__mutmut_124': x_process_in_batches__mutmut_124, 
    'x_process_in_batches__mutmut_125': x_process_in_batches__mutmut_125, 
    'x_process_in_batches__mutmut_126': x_process_in_batches__mutmut_126, 
    'x_process_in_batches__mutmut_127': x_process_in_batches__mutmut_127, 
    'x_process_in_batches__mutmut_128': x_process_in_batches__mutmut_128, 
    'x_process_in_batches__mutmut_129': x_process_in_batches__mutmut_129, 
    'x_process_in_batches__mutmut_130': x_process_in_batches__mutmut_130, 
    'x_process_in_batches__mutmut_131': x_process_in_batches__mutmut_131, 
    'x_process_in_batches__mutmut_132': x_process_in_batches__mutmut_132, 
    'x_process_in_batches__mutmut_133': x_process_in_batches__mutmut_133, 
    'x_process_in_batches__mutmut_134': x_process_in_batches__mutmut_134, 
    'x_process_in_batches__mutmut_135': x_process_in_batches__mutmut_135, 
    'x_process_in_batches__mutmut_136': x_process_in_batches__mutmut_136, 
    'x_process_in_batches__mutmut_137': x_process_in_batches__mutmut_137, 
    'x_process_in_batches__mutmut_138': x_process_in_batches__mutmut_138, 
    'x_process_in_batches__mutmut_139': x_process_in_batches__mutmut_139, 
    'x_process_in_batches__mutmut_140': x_process_in_batches__mutmut_140, 
    'x_process_in_batches__mutmut_141': x_process_in_batches__mutmut_141, 
    'x_process_in_batches__mutmut_142': x_process_in_batches__mutmut_142, 
    'x_process_in_batches__mutmut_143': x_process_in_batches__mutmut_143, 
    'x_process_in_batches__mutmut_144': x_process_in_batches__mutmut_144, 
    'x_process_in_batches__mutmut_145': x_process_in_batches__mutmut_145, 
    'x_process_in_batches__mutmut_146': x_process_in_batches__mutmut_146, 
    'x_process_in_batches__mutmut_147': x_process_in_batches__mutmut_147, 
    'x_process_in_batches__mutmut_148': x_process_in_batches__mutmut_148, 
    'x_process_in_batches__mutmut_149': x_process_in_batches__mutmut_149, 
    'x_process_in_batches__mutmut_150': x_process_in_batches__mutmut_150, 
    'x_process_in_batches__mutmut_151': x_process_in_batches__mutmut_151, 
    'x_process_in_batches__mutmut_152': x_process_in_batches__mutmut_152, 
    'x_process_in_batches__mutmut_153': x_process_in_batches__mutmut_153, 
    'x_process_in_batches__mutmut_154': x_process_in_batches__mutmut_154, 
    'x_process_in_batches__mutmut_155': x_process_in_batches__mutmut_155, 
    'x_process_in_batches__mutmut_156': x_process_in_batches__mutmut_156, 
    'x_process_in_batches__mutmut_157': x_process_in_batches__mutmut_157, 
    'x_process_in_batches__mutmut_158': x_process_in_batches__mutmut_158, 
    'x_process_in_batches__mutmut_159': x_process_in_batches__mutmut_159, 
    'x_process_in_batches__mutmut_160': x_process_in_batches__mutmut_160, 
    'x_process_in_batches__mutmut_161': x_process_in_batches__mutmut_161, 
    'x_process_in_batches__mutmut_162': x_process_in_batches__mutmut_162, 
    'x_process_in_batches__mutmut_163': x_process_in_batches__mutmut_163, 
    'x_process_in_batches__mutmut_164': x_process_in_batches__mutmut_164, 
    'x_process_in_batches__mutmut_165': x_process_in_batches__mutmut_165, 
    'x_process_in_batches__mutmut_166': x_process_in_batches__mutmut_166
}

def process_in_batches(*args, **kwargs):
    result = _mutmut_trampoline(x_process_in_batches__mutmut_orig, x_process_in_batches__mutmut_mutants, args, kwargs)
    return result 

process_in_batches.__signature__ = _mutmut_signature(x_process_in_batches__mutmut_orig)
x_process_in_batches__mutmut_orig.__name__ = 'x_process_in_batches'


def x_estimate_safe_batch_size__mutmut_orig(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_1(
    result_size_bytes: int,
    max_memory_percent: float = 1.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_2(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes < 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_3(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 1:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_4(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError(None)

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_5(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("XXresult_size_bytes must be positiveXX")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_6(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("RESULT_SIZE_BYTES MUST BE POSITIVE")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_7(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 and max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_8(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) and max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_9(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_10(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent < 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_11(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 1 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_12(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent >= 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_13(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 2:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_14(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError(None)

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_15(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("XXmax_memory_percent must be between 0 and 1XX")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_16(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("MAX_MEMORY_PERCENT MUST BE BETWEEN 0 AND 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_17(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = None
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_18(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = None

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_19(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int(None)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_20(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) * result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_21(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent / available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_22(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(None, safe_batch_size)


def x_estimate_safe_batch_size__mutmut_23(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, None)


def x_estimate_safe_batch_size__mutmut_24(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(safe_batch_size)


def x_estimate_safe_batch_size__mutmut_25(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(1, )


def x_estimate_safe_batch_size__mutmut_26(
    result_size_bytes: int,
    max_memory_percent: float = 0.5
) -> int:
    """
    Estimate safe batch size based on result size and available memory.

    This is a helper function for users who want to manually calculate batch sizes
    before calling process_in_batches().

    Args:
        result_size_bytes: Size of a single result in bytes
        max_memory_percent: Maximum percentage of available memory to use (default: 0.5)

    Returns:
        Safe batch size (number of items)

    Examples:
        >>> # Estimate batch size for 10MB results
        >>> batch_size = estimate_safe_batch_size(10 * 1024 * 1024)
        >>> print(f"Safe batch size: {batch_size} items")

        >>> # More conservative (30% of memory)
        >>> batch_size = estimate_safe_batch_size(
        ...     result_size_bytes=50 * 1024 * 1024,
        ...     max_memory_percent=0.3
        ... )
    """
    if result_size_bytes <= 0:
        raise ValueError("result_size_bytes must be positive")

    if not isinstance(max_memory_percent, (int, float)) or max_memory_percent <= 0 or max_memory_percent > 1:
        raise ValueError("max_memory_percent must be between 0 and 1")

    available_memory = get_available_memory()
    safe_batch_size = int((max_memory_percent * available_memory) / result_size_bytes)

    # Ensure at least 1 item
    return max(2, safe_batch_size)

x_estimate_safe_batch_size__mutmut_mutants : ClassVar[MutantDict] = {
'x_estimate_safe_batch_size__mutmut_1': x_estimate_safe_batch_size__mutmut_1, 
    'x_estimate_safe_batch_size__mutmut_2': x_estimate_safe_batch_size__mutmut_2, 
    'x_estimate_safe_batch_size__mutmut_3': x_estimate_safe_batch_size__mutmut_3, 
    'x_estimate_safe_batch_size__mutmut_4': x_estimate_safe_batch_size__mutmut_4, 
    'x_estimate_safe_batch_size__mutmut_5': x_estimate_safe_batch_size__mutmut_5, 
    'x_estimate_safe_batch_size__mutmut_6': x_estimate_safe_batch_size__mutmut_6, 
    'x_estimate_safe_batch_size__mutmut_7': x_estimate_safe_batch_size__mutmut_7, 
    'x_estimate_safe_batch_size__mutmut_8': x_estimate_safe_batch_size__mutmut_8, 
    'x_estimate_safe_batch_size__mutmut_9': x_estimate_safe_batch_size__mutmut_9, 
    'x_estimate_safe_batch_size__mutmut_10': x_estimate_safe_batch_size__mutmut_10, 
    'x_estimate_safe_batch_size__mutmut_11': x_estimate_safe_batch_size__mutmut_11, 
    'x_estimate_safe_batch_size__mutmut_12': x_estimate_safe_batch_size__mutmut_12, 
    'x_estimate_safe_batch_size__mutmut_13': x_estimate_safe_batch_size__mutmut_13, 
    'x_estimate_safe_batch_size__mutmut_14': x_estimate_safe_batch_size__mutmut_14, 
    'x_estimate_safe_batch_size__mutmut_15': x_estimate_safe_batch_size__mutmut_15, 
    'x_estimate_safe_batch_size__mutmut_16': x_estimate_safe_batch_size__mutmut_16, 
    'x_estimate_safe_batch_size__mutmut_17': x_estimate_safe_batch_size__mutmut_17, 
    'x_estimate_safe_batch_size__mutmut_18': x_estimate_safe_batch_size__mutmut_18, 
    'x_estimate_safe_batch_size__mutmut_19': x_estimate_safe_batch_size__mutmut_19, 
    'x_estimate_safe_batch_size__mutmut_20': x_estimate_safe_batch_size__mutmut_20, 
    'x_estimate_safe_batch_size__mutmut_21': x_estimate_safe_batch_size__mutmut_21, 
    'x_estimate_safe_batch_size__mutmut_22': x_estimate_safe_batch_size__mutmut_22, 
    'x_estimate_safe_batch_size__mutmut_23': x_estimate_safe_batch_size__mutmut_23, 
    'x_estimate_safe_batch_size__mutmut_24': x_estimate_safe_batch_size__mutmut_24, 
    'x_estimate_safe_batch_size__mutmut_25': x_estimate_safe_batch_size__mutmut_25, 
    'x_estimate_safe_batch_size__mutmut_26': x_estimate_safe_batch_size__mutmut_26
}

def estimate_safe_batch_size(*args, **kwargs):
    result = _mutmut_trampoline(x_estimate_safe_batch_size__mutmut_orig, x_estimate_safe_batch_size__mutmut_mutants, args, kwargs)
    return result 

estimate_safe_batch_size.__signature__ = _mutmut_signature(x_estimate_safe_batch_size__mutmut_orig)
x_estimate_safe_batch_size__mutmut_orig.__name__ = 'x_estimate_safe_batch_size'

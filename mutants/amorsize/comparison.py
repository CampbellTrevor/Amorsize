"""
Comparison mode for analyzing different parallelization strategies.

This module provides tools to compare multiple optimization strategies
(different n_jobs, chunksizes, or execution methods) side-by-side to help
users make informed decisions about parallelization parameters.
"""

import math
import time
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Pool
from typing import Any, Callable, Iterator, List, Optional, Tuple, Union

from .optimizer import OptimizationResult, optimize
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


class ComparisonConfig:
    """
    Configuration for a single strategy to compare.

    Attributes:
        name: Human-readable name for this configuration
        n_jobs: Number of workers (1 for serial)
        chunksize: Chunk size for batching
        executor_type: "process", "thread", or "serial"
    """

    def xǁComparisonConfigǁ__init____mutmut_orig(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_1(
        self,
        name: str,
        n_jobs: int = 2,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_2(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 2,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_3(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "XXprocessXX"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_4(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "PROCESS"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_5(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs <= 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_6(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 2:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_7(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(None)
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_8(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize <= 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_9(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 2:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_10(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(None)
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_11(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_12(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["XXprocessXX", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_13(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["PROCESS", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_14(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "XXthreadXX", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_15(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "THREAD", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_16(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "XXserialXX"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_17(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "SERIAL"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_18(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(None)

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_19(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = None
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_20(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = None
        self.chunksize = chunksize
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_21(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = None
        self.executor_type = executor_type

    def xǁComparisonConfigǁ__init____mutmut_22(
        self,
        name: str,
        n_jobs: int = 1,
        chunksize: int = 1,
        executor_type: str = "process"
    ):
        if n_jobs < 1:
            raise ValueError(f"n_jobs must be >= 1, got {n_jobs}")
        if chunksize < 1:
            raise ValueError(f"chunksize must be >= 1, got {chunksize}")
        if executor_type not in ["process", "thread", "serial"]:
            raise ValueError(f"executor_type must be 'process', 'thread', or 'serial', got '{executor_type}'")

        self.name = name
        self.n_jobs = n_jobs
        self.chunksize = chunksize
        self.executor_type = None
    
    xǁComparisonConfigǁ__init____mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonConfigǁ__init____mutmut_1': xǁComparisonConfigǁ__init____mutmut_1, 
        'xǁComparisonConfigǁ__init____mutmut_2': xǁComparisonConfigǁ__init____mutmut_2, 
        'xǁComparisonConfigǁ__init____mutmut_3': xǁComparisonConfigǁ__init____mutmut_3, 
        'xǁComparisonConfigǁ__init____mutmut_4': xǁComparisonConfigǁ__init____mutmut_4, 
        'xǁComparisonConfigǁ__init____mutmut_5': xǁComparisonConfigǁ__init____mutmut_5, 
        'xǁComparisonConfigǁ__init____mutmut_6': xǁComparisonConfigǁ__init____mutmut_6, 
        'xǁComparisonConfigǁ__init____mutmut_7': xǁComparisonConfigǁ__init____mutmut_7, 
        'xǁComparisonConfigǁ__init____mutmut_8': xǁComparisonConfigǁ__init____mutmut_8, 
        'xǁComparisonConfigǁ__init____mutmut_9': xǁComparisonConfigǁ__init____mutmut_9, 
        'xǁComparisonConfigǁ__init____mutmut_10': xǁComparisonConfigǁ__init____mutmut_10, 
        'xǁComparisonConfigǁ__init____mutmut_11': xǁComparisonConfigǁ__init____mutmut_11, 
        'xǁComparisonConfigǁ__init____mutmut_12': xǁComparisonConfigǁ__init____mutmut_12, 
        'xǁComparisonConfigǁ__init____mutmut_13': xǁComparisonConfigǁ__init____mutmut_13, 
        'xǁComparisonConfigǁ__init____mutmut_14': xǁComparisonConfigǁ__init____mutmut_14, 
        'xǁComparisonConfigǁ__init____mutmut_15': xǁComparisonConfigǁ__init____mutmut_15, 
        'xǁComparisonConfigǁ__init____mutmut_16': xǁComparisonConfigǁ__init____mutmut_16, 
        'xǁComparisonConfigǁ__init____mutmut_17': xǁComparisonConfigǁ__init____mutmut_17, 
        'xǁComparisonConfigǁ__init____mutmut_18': xǁComparisonConfigǁ__init____mutmut_18, 
        'xǁComparisonConfigǁ__init____mutmut_19': xǁComparisonConfigǁ__init____mutmut_19, 
        'xǁComparisonConfigǁ__init____mutmut_20': xǁComparisonConfigǁ__init____mutmut_20, 
        'xǁComparisonConfigǁ__init____mutmut_21': xǁComparisonConfigǁ__init____mutmut_21, 
        'xǁComparisonConfigǁ__init____mutmut_22': xǁComparisonConfigǁ__init____mutmut_22
    }
    
    def __init__(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonConfigǁ__init____mutmut_orig"), object.__getattribute__(self, "xǁComparisonConfigǁ__init____mutmut_mutants"), args, kwargs, self)
        return result 
    
    __init__.__signature__ = _mutmut_signature(xǁComparisonConfigǁ__init____mutmut_orig)
    xǁComparisonConfigǁ__init____mutmut_orig.__name__ = 'xǁComparisonConfigǁ__init__'

    def xǁComparisonConfigǁ__repr____mutmut_orig(self):
        if self.n_jobs == 1 or self.executor_type == "serial":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_1(self):
        if self.n_jobs == 1 and self.executor_type == "serial":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_2(self):
        if self.n_jobs != 1 or self.executor_type == "serial":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_3(self):
        if self.n_jobs == 2 or self.executor_type == "serial":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_4(self):
        if self.n_jobs == 1 or self.executor_type != "serial":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_5(self):
        if self.n_jobs == 1 or self.executor_type == "XXserialXX":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"

    def xǁComparisonConfigǁ__repr____mutmut_6(self):
        if self.n_jobs == 1 or self.executor_type == "SERIAL":
            return f"ComparisonConfig('{self.name}', serial)"
        return f"ComparisonConfig('{self.name}', n_jobs={self.n_jobs}, chunksize={self.chunksize}, {self.executor_type})"
    
    xǁComparisonConfigǁ__repr____mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonConfigǁ__repr____mutmut_1': xǁComparisonConfigǁ__repr____mutmut_1, 
        'xǁComparisonConfigǁ__repr____mutmut_2': xǁComparisonConfigǁ__repr____mutmut_2, 
        'xǁComparisonConfigǁ__repr____mutmut_3': xǁComparisonConfigǁ__repr____mutmut_3, 
        'xǁComparisonConfigǁ__repr____mutmut_4': xǁComparisonConfigǁ__repr____mutmut_4, 
        'xǁComparisonConfigǁ__repr____mutmut_5': xǁComparisonConfigǁ__repr____mutmut_5, 
        'xǁComparisonConfigǁ__repr____mutmut_6': xǁComparisonConfigǁ__repr____mutmut_6
    }
    
    def __repr__(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonConfigǁ__repr____mutmut_orig"), object.__getattribute__(self, "xǁComparisonConfigǁ__repr____mutmut_mutants"), args, kwargs, self)
        return result 
    
    __repr__.__signature__ = _mutmut_signature(xǁComparisonConfigǁ__repr____mutmut_orig)
    xǁComparisonConfigǁ__repr____mutmut_orig.__name__ = 'xǁComparisonConfigǁ__repr__'

    def xǁComparisonConfigǁ__str____mutmut_orig(self):
        if self.n_jobs == 1 or self.executor_type == "serial":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_1(self):
        if self.n_jobs == 1 and self.executor_type == "serial":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_2(self):
        if self.n_jobs != 1 or self.executor_type == "serial":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_3(self):
        if self.n_jobs == 2 or self.executor_type == "serial":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_4(self):
        if self.n_jobs == 1 or self.executor_type != "serial":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_5(self):
        if self.n_jobs == 1 or self.executor_type == "XXserialXX":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"

    def xǁComparisonConfigǁ__str____mutmut_6(self):
        if self.n_jobs == 1 or self.executor_type == "SERIAL":
            return f"{self.name}: Serial execution"
        return f"{self.name}: {self.n_jobs} {self.executor_type}s, chunksize={self.chunksize}"
    
    xǁComparisonConfigǁ__str____mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonConfigǁ__str____mutmut_1': xǁComparisonConfigǁ__str____mutmut_1, 
        'xǁComparisonConfigǁ__str____mutmut_2': xǁComparisonConfigǁ__str____mutmut_2, 
        'xǁComparisonConfigǁ__str____mutmut_3': xǁComparisonConfigǁ__str____mutmut_3, 
        'xǁComparisonConfigǁ__str____mutmut_4': xǁComparisonConfigǁ__str____mutmut_4, 
        'xǁComparisonConfigǁ__str____mutmut_5': xǁComparisonConfigǁ__str____mutmut_5, 
        'xǁComparisonConfigǁ__str____mutmut_6': xǁComparisonConfigǁ__str____mutmut_6
    }
    
    def __str__(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonConfigǁ__str____mutmut_orig"), object.__getattribute__(self, "xǁComparisonConfigǁ__str____mutmut_mutants"), args, kwargs, self)
        return result 
    
    __str__.__signature__ = _mutmut_signature(xǁComparisonConfigǁ__str____mutmut_orig)
    xǁComparisonConfigǁ__str____mutmut_orig.__name__ = 'xǁComparisonConfigǁ__str__'


class ComparisonResult:
    """
    Result of comparing multiple parallelization strategies.

    Attributes:
        configs: List of configurations that were compared
        execution_times: Execution times for each config (seconds)
        speedups: Speedup relative to serial (first config) for each config
        best_config_index: Index of fastest configuration
        best_config: Fastest configuration
        best_time: Execution time of best configuration
        recommendations: List of insights from comparison
    """

    def xǁComparisonResultǁ__init____mutmut_orig(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_1(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = None
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_2(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = None
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_3(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = None
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_4(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = None
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_5(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = None
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_6(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = None
        self.recommendations = recommendations or []

    def xǁComparisonResultǁ__init____mutmut_7(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = None

    def xǁComparisonResultǁ__init____mutmut_8(
        self,
        configs: List[ComparisonConfig],
        execution_times: List[float],
        speedups: List[float],
        best_config_index: int,
        recommendations: Optional[List[str]] = None
    ):
        self.configs = configs
        self.execution_times = execution_times
        self.speedups = speedups
        self.best_config_index = best_config_index
        self.best_config = configs[best_config_index]
        self.best_time = execution_times[best_config_index]
        self.recommendations = recommendations and []
    
    xǁComparisonResultǁ__init____mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonResultǁ__init____mutmut_1': xǁComparisonResultǁ__init____mutmut_1, 
        'xǁComparisonResultǁ__init____mutmut_2': xǁComparisonResultǁ__init____mutmut_2, 
        'xǁComparisonResultǁ__init____mutmut_3': xǁComparisonResultǁ__init____mutmut_3, 
        'xǁComparisonResultǁ__init____mutmut_4': xǁComparisonResultǁ__init____mutmut_4, 
        'xǁComparisonResultǁ__init____mutmut_5': xǁComparisonResultǁ__init____mutmut_5, 
        'xǁComparisonResultǁ__init____mutmut_6': xǁComparisonResultǁ__init____mutmut_6, 
        'xǁComparisonResultǁ__init____mutmut_7': xǁComparisonResultǁ__init____mutmut_7, 
        'xǁComparisonResultǁ__init____mutmut_8': xǁComparisonResultǁ__init____mutmut_8
    }
    
    def __init__(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonResultǁ__init____mutmut_orig"), object.__getattribute__(self, "xǁComparisonResultǁ__init____mutmut_mutants"), args, kwargs, self)
        return result 
    
    __init__.__signature__ = _mutmut_signature(xǁComparisonResultǁ__init____mutmut_orig)
    xǁComparisonResultǁ__init____mutmut_orig.__name__ = 'xǁComparisonResultǁ__init__'

    def __repr__(self):
        return (f"ComparisonResult(best='{self.best_config.name}', "
                f"time={self.best_time:.4f}s, speedup={self.speedups[self.best_config_index]:.2f}x)")

    def xǁComparisonResultǁ__str____mutmut_orig(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_1(self):
        result = None

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_2(self):
        result = "XX=== Strategy Comparison Results ===\n\nXX"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_3(self):
        result = "=== strategy comparison results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_4(self):
        result = "=== STRATEGY COMPARISON RESULTS ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_5(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result = f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_6(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result -= f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_7(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'XXStrategyXX':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_8(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_9(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'STRATEGY':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_10(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'XXTime (s)XX':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_11(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_12(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'TIME (S)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_13(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'XXSpeedupXX':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_14(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_15(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'SPEEDUP':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_16(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'XXStatusXX':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_17(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_18(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'STATUS':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_19(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result = "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_20(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result -= "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_21(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 - "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_22(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" / 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_23(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "XX-XX" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_24(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 71 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_25(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "XX\nXX"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_26(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(None):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_27(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(None, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_28(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, None, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_29(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, None)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_30(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_31(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_32(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, )):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_33(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i != self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_34(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = None
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_35(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "XX⭐ FASTESTXX"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_36(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ fastest"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_37(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup <= 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_38(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 2.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_39(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = None
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_40(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "XX⚠️  SlowerXX"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_41(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_42(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  SLOWER"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_43(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup <= 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_44(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 2.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_45(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = None
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_46(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "XX~ SimilarXX"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_47(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_48(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ SIMILAR"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_49(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = None

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_50(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "XX✓ FasterXX"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_51(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_52(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ FASTER"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_53(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result = f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_54(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result -= f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_55(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result = "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_56(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result -= "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_57(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "XX\nXX"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_58(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result = f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_59(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result -= f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_60(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result = f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_61(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result -= f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_62(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result = f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_63(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result -= f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_64(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result = "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_65(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result -= "\nRecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_66(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "XX\nRecommendations:\nXX"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_67(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nrecommendations:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_68(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRECOMMENDATIONS:\n"
            for rec in self.recommendations:
                result += f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_69(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result = f"  • {rec}\n"

        return result

    def xǁComparisonResultǁ__str____mutmut_70(self):
        result = "=== Strategy Comparison Results ===\n\n"

        # Table header
        result += f"{'Strategy':<30} {'Time (s)':<12} {'Speedup':<10} {'Status':<15}\n"
        result += "-" * 70 + "\n"

        # Add each configuration
        for i, (config, exec_time, speedup) in enumerate(zip(self.configs, self.execution_times, self.speedups)):
            # Status indicator
            if i == self.best_config_index:
                status = "⭐ FASTEST"
            elif speedup < 1.0:
                status = "⚠️  Slower"
            elif speedup < 1.1:
                status = "~ Similar"
            else:
                status = "✓ Faster"

            result += f"{config.name:<30} {exec_time:<12.4f} {speedup:<10.2f}x {status:<15}\n"

        result += "\n"
        result += f"Best Strategy: {self.best_config.name}\n"
        result += f"Best Time: {self.best_time:.4f}s\n"
        result += f"Best Speedup: {self.speedups[self.best_config_index]:.2f}x\n"

        if self.recommendations:
            result += "\nRecommendations:\n"
            for rec in self.recommendations:
                result -= f"  • {rec}\n"

        return result
    
    xǁComparisonResultǁ__str____mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonResultǁ__str____mutmut_1': xǁComparisonResultǁ__str____mutmut_1, 
        'xǁComparisonResultǁ__str____mutmut_2': xǁComparisonResultǁ__str____mutmut_2, 
        'xǁComparisonResultǁ__str____mutmut_3': xǁComparisonResultǁ__str____mutmut_3, 
        'xǁComparisonResultǁ__str____mutmut_4': xǁComparisonResultǁ__str____mutmut_4, 
        'xǁComparisonResultǁ__str____mutmut_5': xǁComparisonResultǁ__str____mutmut_5, 
        'xǁComparisonResultǁ__str____mutmut_6': xǁComparisonResultǁ__str____mutmut_6, 
        'xǁComparisonResultǁ__str____mutmut_7': xǁComparisonResultǁ__str____mutmut_7, 
        'xǁComparisonResultǁ__str____mutmut_8': xǁComparisonResultǁ__str____mutmut_8, 
        'xǁComparisonResultǁ__str____mutmut_9': xǁComparisonResultǁ__str____mutmut_9, 
        'xǁComparisonResultǁ__str____mutmut_10': xǁComparisonResultǁ__str____mutmut_10, 
        'xǁComparisonResultǁ__str____mutmut_11': xǁComparisonResultǁ__str____mutmut_11, 
        'xǁComparisonResultǁ__str____mutmut_12': xǁComparisonResultǁ__str____mutmut_12, 
        'xǁComparisonResultǁ__str____mutmut_13': xǁComparisonResultǁ__str____mutmut_13, 
        'xǁComparisonResultǁ__str____mutmut_14': xǁComparisonResultǁ__str____mutmut_14, 
        'xǁComparisonResultǁ__str____mutmut_15': xǁComparisonResultǁ__str____mutmut_15, 
        'xǁComparisonResultǁ__str____mutmut_16': xǁComparisonResultǁ__str____mutmut_16, 
        'xǁComparisonResultǁ__str____mutmut_17': xǁComparisonResultǁ__str____mutmut_17, 
        'xǁComparisonResultǁ__str____mutmut_18': xǁComparisonResultǁ__str____mutmut_18, 
        'xǁComparisonResultǁ__str____mutmut_19': xǁComparisonResultǁ__str____mutmut_19, 
        'xǁComparisonResultǁ__str____mutmut_20': xǁComparisonResultǁ__str____mutmut_20, 
        'xǁComparisonResultǁ__str____mutmut_21': xǁComparisonResultǁ__str____mutmut_21, 
        'xǁComparisonResultǁ__str____mutmut_22': xǁComparisonResultǁ__str____mutmut_22, 
        'xǁComparisonResultǁ__str____mutmut_23': xǁComparisonResultǁ__str____mutmut_23, 
        'xǁComparisonResultǁ__str____mutmut_24': xǁComparisonResultǁ__str____mutmut_24, 
        'xǁComparisonResultǁ__str____mutmut_25': xǁComparisonResultǁ__str____mutmut_25, 
        'xǁComparisonResultǁ__str____mutmut_26': xǁComparisonResultǁ__str____mutmut_26, 
        'xǁComparisonResultǁ__str____mutmut_27': xǁComparisonResultǁ__str____mutmut_27, 
        'xǁComparisonResultǁ__str____mutmut_28': xǁComparisonResultǁ__str____mutmut_28, 
        'xǁComparisonResultǁ__str____mutmut_29': xǁComparisonResultǁ__str____mutmut_29, 
        'xǁComparisonResultǁ__str____mutmut_30': xǁComparisonResultǁ__str____mutmut_30, 
        'xǁComparisonResultǁ__str____mutmut_31': xǁComparisonResultǁ__str____mutmut_31, 
        'xǁComparisonResultǁ__str____mutmut_32': xǁComparisonResultǁ__str____mutmut_32, 
        'xǁComparisonResultǁ__str____mutmut_33': xǁComparisonResultǁ__str____mutmut_33, 
        'xǁComparisonResultǁ__str____mutmut_34': xǁComparisonResultǁ__str____mutmut_34, 
        'xǁComparisonResultǁ__str____mutmut_35': xǁComparisonResultǁ__str____mutmut_35, 
        'xǁComparisonResultǁ__str____mutmut_36': xǁComparisonResultǁ__str____mutmut_36, 
        'xǁComparisonResultǁ__str____mutmut_37': xǁComparisonResultǁ__str____mutmut_37, 
        'xǁComparisonResultǁ__str____mutmut_38': xǁComparisonResultǁ__str____mutmut_38, 
        'xǁComparisonResultǁ__str____mutmut_39': xǁComparisonResultǁ__str____mutmut_39, 
        'xǁComparisonResultǁ__str____mutmut_40': xǁComparisonResultǁ__str____mutmut_40, 
        'xǁComparisonResultǁ__str____mutmut_41': xǁComparisonResultǁ__str____mutmut_41, 
        'xǁComparisonResultǁ__str____mutmut_42': xǁComparisonResultǁ__str____mutmut_42, 
        'xǁComparisonResultǁ__str____mutmut_43': xǁComparisonResultǁ__str____mutmut_43, 
        'xǁComparisonResultǁ__str____mutmut_44': xǁComparisonResultǁ__str____mutmut_44, 
        'xǁComparisonResultǁ__str____mutmut_45': xǁComparisonResultǁ__str____mutmut_45, 
        'xǁComparisonResultǁ__str____mutmut_46': xǁComparisonResultǁ__str____mutmut_46, 
        'xǁComparisonResultǁ__str____mutmut_47': xǁComparisonResultǁ__str____mutmut_47, 
        'xǁComparisonResultǁ__str____mutmut_48': xǁComparisonResultǁ__str____mutmut_48, 
        'xǁComparisonResultǁ__str____mutmut_49': xǁComparisonResultǁ__str____mutmut_49, 
        'xǁComparisonResultǁ__str____mutmut_50': xǁComparisonResultǁ__str____mutmut_50, 
        'xǁComparisonResultǁ__str____mutmut_51': xǁComparisonResultǁ__str____mutmut_51, 
        'xǁComparisonResultǁ__str____mutmut_52': xǁComparisonResultǁ__str____mutmut_52, 
        'xǁComparisonResultǁ__str____mutmut_53': xǁComparisonResultǁ__str____mutmut_53, 
        'xǁComparisonResultǁ__str____mutmut_54': xǁComparisonResultǁ__str____mutmut_54, 
        'xǁComparisonResultǁ__str____mutmut_55': xǁComparisonResultǁ__str____mutmut_55, 
        'xǁComparisonResultǁ__str____mutmut_56': xǁComparisonResultǁ__str____mutmut_56, 
        'xǁComparisonResultǁ__str____mutmut_57': xǁComparisonResultǁ__str____mutmut_57, 
        'xǁComparisonResultǁ__str____mutmut_58': xǁComparisonResultǁ__str____mutmut_58, 
        'xǁComparisonResultǁ__str____mutmut_59': xǁComparisonResultǁ__str____mutmut_59, 
        'xǁComparisonResultǁ__str____mutmut_60': xǁComparisonResultǁ__str____mutmut_60, 
        'xǁComparisonResultǁ__str____mutmut_61': xǁComparisonResultǁ__str____mutmut_61, 
        'xǁComparisonResultǁ__str____mutmut_62': xǁComparisonResultǁ__str____mutmut_62, 
        'xǁComparisonResultǁ__str____mutmut_63': xǁComparisonResultǁ__str____mutmut_63, 
        'xǁComparisonResultǁ__str____mutmut_64': xǁComparisonResultǁ__str____mutmut_64, 
        'xǁComparisonResultǁ__str____mutmut_65': xǁComparisonResultǁ__str____mutmut_65, 
        'xǁComparisonResultǁ__str____mutmut_66': xǁComparisonResultǁ__str____mutmut_66, 
        'xǁComparisonResultǁ__str____mutmut_67': xǁComparisonResultǁ__str____mutmut_67, 
        'xǁComparisonResultǁ__str____mutmut_68': xǁComparisonResultǁ__str____mutmut_68, 
        'xǁComparisonResultǁ__str____mutmut_69': xǁComparisonResultǁ__str____mutmut_69, 
        'xǁComparisonResultǁ__str____mutmut_70': xǁComparisonResultǁ__str____mutmut_70
    }
    
    def __str__(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonResultǁ__str____mutmut_orig"), object.__getattribute__(self, "xǁComparisonResultǁ__str____mutmut_mutants"), args, kwargs, self)
        return result 
    
    __str__.__signature__ = _mutmut_signature(xǁComparisonResultǁ__str____mutmut_orig)
    xǁComparisonResultǁ__str____mutmut_orig.__name__ = 'xǁComparisonResultǁ__str__'

    def xǁComparisonResultǁget_sorted_configs__mutmut_orig(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_1(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = None
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_2(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(None)
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_3(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(None, self.execution_times, self.speedups))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_4(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, None, self.speedups))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_5(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, None))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_6(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.execution_times, self.speedups))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_7(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.speedups))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_8(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, ))
        return sorted(combined, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_9(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(None, key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_10(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(combined, key=None)

    def xǁComparisonResultǁget_sorted_configs__mutmut_11(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(key=lambda x: x[1])

    def xǁComparisonResultǁget_sorted_configs__mutmut_12(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(combined, )

    def xǁComparisonResultǁget_sorted_configs__mutmut_13(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(combined, key=lambda x: None)

    def xǁComparisonResultǁget_sorted_configs__mutmut_14(self) -> List[Tuple[ComparisonConfig, float, float]]:
        """
        Get configurations sorted by execution time (fastest first).

        Returns:
            List of tuples: (config, time, speedup) sorted by time
        """
        combined = list(zip(self.configs, self.execution_times, self.speedups))
        return sorted(combined, key=lambda x: x[2])
    
    xǁComparisonResultǁget_sorted_configs__mutmut_mutants : ClassVar[MutantDict] = {
    'xǁComparisonResultǁget_sorted_configs__mutmut_1': xǁComparisonResultǁget_sorted_configs__mutmut_1, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_2': xǁComparisonResultǁget_sorted_configs__mutmut_2, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_3': xǁComparisonResultǁget_sorted_configs__mutmut_3, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_4': xǁComparisonResultǁget_sorted_configs__mutmut_4, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_5': xǁComparisonResultǁget_sorted_configs__mutmut_5, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_6': xǁComparisonResultǁget_sorted_configs__mutmut_6, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_7': xǁComparisonResultǁget_sorted_configs__mutmut_7, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_8': xǁComparisonResultǁget_sorted_configs__mutmut_8, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_9': xǁComparisonResultǁget_sorted_configs__mutmut_9, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_10': xǁComparisonResultǁget_sorted_configs__mutmut_10, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_11': xǁComparisonResultǁget_sorted_configs__mutmut_11, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_12': xǁComparisonResultǁget_sorted_configs__mutmut_12, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_13': xǁComparisonResultǁget_sorted_configs__mutmut_13, 
        'xǁComparisonResultǁget_sorted_configs__mutmut_14': xǁComparisonResultǁget_sorted_configs__mutmut_14
    }
    
    def get_sorted_configs(self, *args, **kwargs):
        result = _mutmut_trampoline(object.__getattribute__(self, "xǁComparisonResultǁget_sorted_configs__mutmut_orig"), object.__getattribute__(self, "xǁComparisonResultǁget_sorted_configs__mutmut_mutants"), args, kwargs, self)
        return result 
    
    get_sorted_configs.__signature__ = _mutmut_signature(xǁComparisonResultǁget_sorted_configs__mutmut_orig)
    xǁComparisonResultǁget_sorted_configs__mutmut_orig.__name__ = 'xǁComparisonResultǁget_sorted_configs'


def x_compare_strategies__mutmut_orig(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_1(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 121.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_2(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = True
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_3(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_4(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(None):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_5(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError(None)
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_6(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("XXfunc must be callableXX")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_7(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("FUNC MUST BE CALLABLE")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_8(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is not None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_9(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError(None)
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_10(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("XXdata cannot be NoneXX")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_11(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be none")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_12(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("DATA CANNOT BE NONE")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_13(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs and len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_14(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_15(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) != 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_16(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 1:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_17(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError(None)
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_18(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("XXconfigs cannot be emptyXX")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_19(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("CONFIGS CANNOT BE EMPTY")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_20(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout < 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_21(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 1:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_22(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(None)

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_23(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_24(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = None

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_25(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(None)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_26(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None or len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_27(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_28(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) >= max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_29(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(None)
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_30(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = None

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_31(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) != 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_32(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 1:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_33(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError(None)

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_34(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("XXdata cannot be empty for benchmarkingXX")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_35(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("DATA CANNOT BE EMPTY FOR BENCHMARKING")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_36(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(None)

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_37(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = None
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_38(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = None

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_39(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(None):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_40(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(None)

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_41(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i - 1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_42(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+2}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_43(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = None

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_44(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 and config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_45(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs != 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_46(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 2 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_47(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type != "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_48(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "XXserialXX":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_49(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "SERIAL":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_50(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = None

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_51(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(None)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_52(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type != "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_53(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "XXprocessXX":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_54(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "PROCESS":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_55(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=None) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_56(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = None

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_57(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(None, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_58(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, None, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_59(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=None)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_60(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_61(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_62(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, )

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_63(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type != "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_64(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "XXthreadXX":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_65(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "THREAD":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_66(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=None) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_67(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = None

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_68(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(None)

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_69(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(None, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_70(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, None))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_71(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_72(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, ))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_73(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(None)

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_74(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(None)

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_75(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = None
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_76(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = None
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_77(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end + start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_78(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(None)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_79(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(None)

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_80(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time >= timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_81(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(None)

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_82(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = None
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_83(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[1]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_84(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = None

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_85(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time * t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_86(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t >= 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_87(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 1 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_88(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 2.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_89(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = None

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_90(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(None)

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_91(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.rindex(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_92(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(None))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_93(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = None

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_94(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx != 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_95(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 1:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_96(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append(None)
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_97(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("XXSerial execution is fastest - parallelization adds overhead without benefitXX")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_98(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_99(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("SERIAL EXECUTION IS FASTEST - PARALLELIZATION ADDS OVERHEAD WITHOUT BENEFIT")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_100(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append(None)
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_101(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("XXConsider increasing workload size or function complexityXX")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_102(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_103(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("CONSIDER INCREASING WORKLOAD SIZE OR FUNCTION COMPLEXITY")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_104(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(None)

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_105(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs >= 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_106(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 2:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_107(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = None
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_108(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] * best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_109(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency >= 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_110(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 1.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_111(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(None)
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_112(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency / 100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_113(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*101:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_114(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency <= 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_115(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 1.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_116(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(None)

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_117(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency / 100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_118(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*101:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_119(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(None):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_120(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(None, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_121(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, None)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_122(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_123(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, )):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_124(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx or speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_125(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i == best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_126(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup <= 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_127(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 1.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_128(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(None)

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_129(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = None
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_130(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(None)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_131(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type != "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_132(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "XXthreadXX" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_133(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "THREAD" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_134(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = None

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_135(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(None)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_136(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type != "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_137(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "XXprocessXX" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_138(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "PROCESS" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_139(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread or has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_140(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = None
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_141(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(None, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_142(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, None) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_143(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_144(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, ) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_145(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type != "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_146(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "XXthreadXX"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_147(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "THREAD"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_148(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = None

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_149(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(None, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_150(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, None) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_151(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_152(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, ) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_153(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type != "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_154(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "XXprocessXX"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_155(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "PROCESS"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_156(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times or process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_157(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = None
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_158(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) * len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_159(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(None) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_160(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = None

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_161(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) * len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_162(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(None) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_163(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread <= avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_164(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process / 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_165(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 1.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_166(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append(None)
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_167(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("XXThreading is significantly faster - workload may be I/O-boundXX")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_168(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("threading is significantly faster - workload may be i/o-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_169(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("THREADING IS SIGNIFICANTLY FASTER - WORKLOAD MAY BE I/O-BOUND")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_170(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process <= avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_171(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread / 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_172(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 1.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_173(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append(None)

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_174(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("XXMultiprocessing is significantly faster - workload is CPU-boundXX")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_175(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("multiprocessing is significantly faster - workload is cpu-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_176(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("MULTIPROCESSING IS SIGNIFICANTLY FASTER - WORKLOAD IS CPU-BOUND")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_177(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=None,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_178(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=None,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_179(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=None,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_180(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=None,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_181(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=None
    )


def x_compare_strategies__mutmut_182(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_183(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        speedups=speedups,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_184(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        best_config_index=best_idx,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_185(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        recommendations=recommendations
    )


def x_compare_strategies__mutmut_186(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    configs: List[ComparisonConfig],
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> ComparisonResult:
    """
    Compare multiple parallelization strategies by benchmarking each.

    This function runs actual benchmarks for each configuration and compares
    their performance. This helps users understand trade-offs between different
    parallelization approaches and choose the best one for their workload.

    Args:
        func: Function to benchmark (must accept single argument)
        data: Input data (list, generator, or iterator)
        configs: List of configurations to compare
        max_items: Maximum items to benchmark (limits runtime for large datasets)
        timeout: Maximum time for each benchmark in seconds (default: 120s)
        verbose: Print progress information (default: False)

    Returns:
        ComparisonResult with execution times and speedups for each config

    Raises:
        ValueError: If parameters are invalid
        TimeoutError: If any benchmark exceeds timeout

    Example:
        >>> from amorsize import compare_strategies, ComparisonConfig
        >>>
        >>> def expensive_func(x):
        ...     return sum(i**2 for i in range(x))
        >>>
        >>> data = range(100, 1000)
        >>>
        >>> configs = [
        ...     ComparisonConfig("Serial", n_jobs=1),
        ...     ComparisonConfig("2 Workers", n_jobs=2, chunksize=50),
        ...     ComparisonConfig("4 Workers", n_jobs=4, chunksize=25),
        ...     ComparisonConfig("8 Workers", n_jobs=8, chunksize=13),
        ... ]
        >>>
        >>> result = compare_strategies(expensive_func, data, configs, verbose=True)
        >>> print(result)

    Notes:
        - First config is used as baseline for speedup calculation
        - Configs are run in order provided
        - Use max_items to limit runtime for large datasets
        - Timeout applies to each individual benchmark
    """
    # Validate inputs
    if not callable(func):
        raise ValueError("func must be callable")
    if data is None:
        raise ValueError("data cannot be None")
    if not configs or len(configs) == 0:
        raise ValueError("configs cannot be empty")
    if timeout <= 0:
        raise ValueError(f"timeout must be positive, got {timeout}")

    # Convert data to list for benchmarking (need to iterate multiple times)
    if not isinstance(data, list):
        data = list(data)

    # Limit dataset size if requested
    if max_items is not None and len(data) > max_items:
        if verbose:
            print(f"Limiting benchmark to first {max_items} of {len(data)} items\n")
        data = data[:max_items]

    if len(data) == 0:
        raise ValueError("data cannot be empty for benchmarking")

    if verbose:
        print(f"Comparing {len(configs)} strategies on {len(data)} items\n")

    execution_times = []
    recommendations = []

    # Benchmark each configuration
    for i, config in enumerate(configs):
        if verbose:
            print(f"[{i+1}/{len(configs)}] Testing: {config}")

        start = time.perf_counter()

        try:
            if config.n_jobs == 1 or config.executor_type == "serial":
                # Serial execution
                for item in data:
                    _ = func(item)

            elif config.executor_type == "process":
                # Multiprocessing execution
                with Pool(processes=config.n_jobs) as pool:
                    _ = pool.map(func, data, chunksize=config.chunksize)

            elif config.executor_type == "thread":
                # Threading execution
                with ThreadPoolExecutor(max_workers=config.n_jobs) as executor:
                    # ThreadPoolExecutor.map doesn't have chunksize, but we can simulate
                    _ = list(executor.map(func, data))

            else:
                raise ValueError(f"Unknown executor_type: {config.executor_type}")

        except Exception as e:
            raise RuntimeError(f"Execution failed for config '{config.name}': {e}")

        end = time.perf_counter()
        exec_time = end - start
        execution_times.append(exec_time)

        if verbose:
            print(f"    Execution time: {exec_time:.4f}s")

        # Check timeout
        if exec_time > timeout:
            raise TimeoutError(f"Config '{config.name}' exceeded timeout ({timeout}s)")

    if verbose:
        print()

    # Calculate speedups relative to first (baseline) config
    baseline_time = execution_times[0]
    speedups = [baseline_time / t if t > 0 else 1.0 for t in execution_times]

    # Find best configuration
    best_idx = execution_times.index(min(execution_times))

    # Generate recommendations
    best_config = configs[best_idx]

    if best_idx == 0:
        recommendations.append("Serial execution is fastest - parallelization adds overhead without benefit")
        recommendations.append("Consider increasing workload size or function complexity")
    else:
        recommendations.append(f"Best strategy uses {best_config.n_jobs} workers with chunksize {best_config.chunksize}")

        # Check if speedup is close to linear
        if best_config.n_jobs > 1:
            efficiency = speedups[best_idx] / best_config.n_jobs
            if efficiency > 0.85:
                recommendations.append(f"Excellent parallel efficiency ({efficiency*100:.1f}%) - near-linear scaling")
            elif efficiency < 0.5:
                recommendations.append(f"Low parallel efficiency ({efficiency*100:.1f}%) - overhead is significant")

        # Check if any config is much worse
        for i, (config, speedup) in enumerate(zip(configs, speedups)):
            if i != best_idx and speedup < 0.8:
                recommendations.append(f"Config '{config.name}' is significantly slower - avoid this configuration")

    # Check for threading vs multiprocessing comparison
    has_thread = any(c.executor_type == "thread" for c in configs)
    has_process = any(c.executor_type == "process" for c in configs)

    if has_thread and has_process:
        thread_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "thread"]
        process_times = [t for c, t in zip(configs, execution_times) if c.executor_type == "process"]

        if thread_times and process_times:
            # Use math.fsum() for better numerical precision in timing averages
            avg_thread = math.fsum(thread_times) / len(thread_times)
            avg_process = math.fsum(process_times) / len(process_times)

            if avg_thread < avg_process * 0.9:
                recommendations.append("Threading is significantly faster - workload may be I/O-bound")
            elif avg_process < avg_thread * 0.9:
                recommendations.append("Multiprocessing is significantly faster - workload is CPU-bound")

    return ComparisonResult(
        configs=configs,
        execution_times=execution_times,
        speedups=speedups,
        best_config_index=best_idx,
        )

x_compare_strategies__mutmut_mutants : ClassVar[MutantDict] = {
'x_compare_strategies__mutmut_1': x_compare_strategies__mutmut_1, 
    'x_compare_strategies__mutmut_2': x_compare_strategies__mutmut_2, 
    'x_compare_strategies__mutmut_3': x_compare_strategies__mutmut_3, 
    'x_compare_strategies__mutmut_4': x_compare_strategies__mutmut_4, 
    'x_compare_strategies__mutmut_5': x_compare_strategies__mutmut_5, 
    'x_compare_strategies__mutmut_6': x_compare_strategies__mutmut_6, 
    'x_compare_strategies__mutmut_7': x_compare_strategies__mutmut_7, 
    'x_compare_strategies__mutmut_8': x_compare_strategies__mutmut_8, 
    'x_compare_strategies__mutmut_9': x_compare_strategies__mutmut_9, 
    'x_compare_strategies__mutmut_10': x_compare_strategies__mutmut_10, 
    'x_compare_strategies__mutmut_11': x_compare_strategies__mutmut_11, 
    'x_compare_strategies__mutmut_12': x_compare_strategies__mutmut_12, 
    'x_compare_strategies__mutmut_13': x_compare_strategies__mutmut_13, 
    'x_compare_strategies__mutmut_14': x_compare_strategies__mutmut_14, 
    'x_compare_strategies__mutmut_15': x_compare_strategies__mutmut_15, 
    'x_compare_strategies__mutmut_16': x_compare_strategies__mutmut_16, 
    'x_compare_strategies__mutmut_17': x_compare_strategies__mutmut_17, 
    'x_compare_strategies__mutmut_18': x_compare_strategies__mutmut_18, 
    'x_compare_strategies__mutmut_19': x_compare_strategies__mutmut_19, 
    'x_compare_strategies__mutmut_20': x_compare_strategies__mutmut_20, 
    'x_compare_strategies__mutmut_21': x_compare_strategies__mutmut_21, 
    'x_compare_strategies__mutmut_22': x_compare_strategies__mutmut_22, 
    'x_compare_strategies__mutmut_23': x_compare_strategies__mutmut_23, 
    'x_compare_strategies__mutmut_24': x_compare_strategies__mutmut_24, 
    'x_compare_strategies__mutmut_25': x_compare_strategies__mutmut_25, 
    'x_compare_strategies__mutmut_26': x_compare_strategies__mutmut_26, 
    'x_compare_strategies__mutmut_27': x_compare_strategies__mutmut_27, 
    'x_compare_strategies__mutmut_28': x_compare_strategies__mutmut_28, 
    'x_compare_strategies__mutmut_29': x_compare_strategies__mutmut_29, 
    'x_compare_strategies__mutmut_30': x_compare_strategies__mutmut_30, 
    'x_compare_strategies__mutmut_31': x_compare_strategies__mutmut_31, 
    'x_compare_strategies__mutmut_32': x_compare_strategies__mutmut_32, 
    'x_compare_strategies__mutmut_33': x_compare_strategies__mutmut_33, 
    'x_compare_strategies__mutmut_34': x_compare_strategies__mutmut_34, 
    'x_compare_strategies__mutmut_35': x_compare_strategies__mutmut_35, 
    'x_compare_strategies__mutmut_36': x_compare_strategies__mutmut_36, 
    'x_compare_strategies__mutmut_37': x_compare_strategies__mutmut_37, 
    'x_compare_strategies__mutmut_38': x_compare_strategies__mutmut_38, 
    'x_compare_strategies__mutmut_39': x_compare_strategies__mutmut_39, 
    'x_compare_strategies__mutmut_40': x_compare_strategies__mutmut_40, 
    'x_compare_strategies__mutmut_41': x_compare_strategies__mutmut_41, 
    'x_compare_strategies__mutmut_42': x_compare_strategies__mutmut_42, 
    'x_compare_strategies__mutmut_43': x_compare_strategies__mutmut_43, 
    'x_compare_strategies__mutmut_44': x_compare_strategies__mutmut_44, 
    'x_compare_strategies__mutmut_45': x_compare_strategies__mutmut_45, 
    'x_compare_strategies__mutmut_46': x_compare_strategies__mutmut_46, 
    'x_compare_strategies__mutmut_47': x_compare_strategies__mutmut_47, 
    'x_compare_strategies__mutmut_48': x_compare_strategies__mutmut_48, 
    'x_compare_strategies__mutmut_49': x_compare_strategies__mutmut_49, 
    'x_compare_strategies__mutmut_50': x_compare_strategies__mutmut_50, 
    'x_compare_strategies__mutmut_51': x_compare_strategies__mutmut_51, 
    'x_compare_strategies__mutmut_52': x_compare_strategies__mutmut_52, 
    'x_compare_strategies__mutmut_53': x_compare_strategies__mutmut_53, 
    'x_compare_strategies__mutmut_54': x_compare_strategies__mutmut_54, 
    'x_compare_strategies__mutmut_55': x_compare_strategies__mutmut_55, 
    'x_compare_strategies__mutmut_56': x_compare_strategies__mutmut_56, 
    'x_compare_strategies__mutmut_57': x_compare_strategies__mutmut_57, 
    'x_compare_strategies__mutmut_58': x_compare_strategies__mutmut_58, 
    'x_compare_strategies__mutmut_59': x_compare_strategies__mutmut_59, 
    'x_compare_strategies__mutmut_60': x_compare_strategies__mutmut_60, 
    'x_compare_strategies__mutmut_61': x_compare_strategies__mutmut_61, 
    'x_compare_strategies__mutmut_62': x_compare_strategies__mutmut_62, 
    'x_compare_strategies__mutmut_63': x_compare_strategies__mutmut_63, 
    'x_compare_strategies__mutmut_64': x_compare_strategies__mutmut_64, 
    'x_compare_strategies__mutmut_65': x_compare_strategies__mutmut_65, 
    'x_compare_strategies__mutmut_66': x_compare_strategies__mutmut_66, 
    'x_compare_strategies__mutmut_67': x_compare_strategies__mutmut_67, 
    'x_compare_strategies__mutmut_68': x_compare_strategies__mutmut_68, 
    'x_compare_strategies__mutmut_69': x_compare_strategies__mutmut_69, 
    'x_compare_strategies__mutmut_70': x_compare_strategies__mutmut_70, 
    'x_compare_strategies__mutmut_71': x_compare_strategies__mutmut_71, 
    'x_compare_strategies__mutmut_72': x_compare_strategies__mutmut_72, 
    'x_compare_strategies__mutmut_73': x_compare_strategies__mutmut_73, 
    'x_compare_strategies__mutmut_74': x_compare_strategies__mutmut_74, 
    'x_compare_strategies__mutmut_75': x_compare_strategies__mutmut_75, 
    'x_compare_strategies__mutmut_76': x_compare_strategies__mutmut_76, 
    'x_compare_strategies__mutmut_77': x_compare_strategies__mutmut_77, 
    'x_compare_strategies__mutmut_78': x_compare_strategies__mutmut_78, 
    'x_compare_strategies__mutmut_79': x_compare_strategies__mutmut_79, 
    'x_compare_strategies__mutmut_80': x_compare_strategies__mutmut_80, 
    'x_compare_strategies__mutmut_81': x_compare_strategies__mutmut_81, 
    'x_compare_strategies__mutmut_82': x_compare_strategies__mutmut_82, 
    'x_compare_strategies__mutmut_83': x_compare_strategies__mutmut_83, 
    'x_compare_strategies__mutmut_84': x_compare_strategies__mutmut_84, 
    'x_compare_strategies__mutmut_85': x_compare_strategies__mutmut_85, 
    'x_compare_strategies__mutmut_86': x_compare_strategies__mutmut_86, 
    'x_compare_strategies__mutmut_87': x_compare_strategies__mutmut_87, 
    'x_compare_strategies__mutmut_88': x_compare_strategies__mutmut_88, 
    'x_compare_strategies__mutmut_89': x_compare_strategies__mutmut_89, 
    'x_compare_strategies__mutmut_90': x_compare_strategies__mutmut_90, 
    'x_compare_strategies__mutmut_91': x_compare_strategies__mutmut_91, 
    'x_compare_strategies__mutmut_92': x_compare_strategies__mutmut_92, 
    'x_compare_strategies__mutmut_93': x_compare_strategies__mutmut_93, 
    'x_compare_strategies__mutmut_94': x_compare_strategies__mutmut_94, 
    'x_compare_strategies__mutmut_95': x_compare_strategies__mutmut_95, 
    'x_compare_strategies__mutmut_96': x_compare_strategies__mutmut_96, 
    'x_compare_strategies__mutmut_97': x_compare_strategies__mutmut_97, 
    'x_compare_strategies__mutmut_98': x_compare_strategies__mutmut_98, 
    'x_compare_strategies__mutmut_99': x_compare_strategies__mutmut_99, 
    'x_compare_strategies__mutmut_100': x_compare_strategies__mutmut_100, 
    'x_compare_strategies__mutmut_101': x_compare_strategies__mutmut_101, 
    'x_compare_strategies__mutmut_102': x_compare_strategies__mutmut_102, 
    'x_compare_strategies__mutmut_103': x_compare_strategies__mutmut_103, 
    'x_compare_strategies__mutmut_104': x_compare_strategies__mutmut_104, 
    'x_compare_strategies__mutmut_105': x_compare_strategies__mutmut_105, 
    'x_compare_strategies__mutmut_106': x_compare_strategies__mutmut_106, 
    'x_compare_strategies__mutmut_107': x_compare_strategies__mutmut_107, 
    'x_compare_strategies__mutmut_108': x_compare_strategies__mutmut_108, 
    'x_compare_strategies__mutmut_109': x_compare_strategies__mutmut_109, 
    'x_compare_strategies__mutmut_110': x_compare_strategies__mutmut_110, 
    'x_compare_strategies__mutmut_111': x_compare_strategies__mutmut_111, 
    'x_compare_strategies__mutmut_112': x_compare_strategies__mutmut_112, 
    'x_compare_strategies__mutmut_113': x_compare_strategies__mutmut_113, 
    'x_compare_strategies__mutmut_114': x_compare_strategies__mutmut_114, 
    'x_compare_strategies__mutmut_115': x_compare_strategies__mutmut_115, 
    'x_compare_strategies__mutmut_116': x_compare_strategies__mutmut_116, 
    'x_compare_strategies__mutmut_117': x_compare_strategies__mutmut_117, 
    'x_compare_strategies__mutmut_118': x_compare_strategies__mutmut_118, 
    'x_compare_strategies__mutmut_119': x_compare_strategies__mutmut_119, 
    'x_compare_strategies__mutmut_120': x_compare_strategies__mutmut_120, 
    'x_compare_strategies__mutmut_121': x_compare_strategies__mutmut_121, 
    'x_compare_strategies__mutmut_122': x_compare_strategies__mutmut_122, 
    'x_compare_strategies__mutmut_123': x_compare_strategies__mutmut_123, 
    'x_compare_strategies__mutmut_124': x_compare_strategies__mutmut_124, 
    'x_compare_strategies__mutmut_125': x_compare_strategies__mutmut_125, 
    'x_compare_strategies__mutmut_126': x_compare_strategies__mutmut_126, 
    'x_compare_strategies__mutmut_127': x_compare_strategies__mutmut_127, 
    'x_compare_strategies__mutmut_128': x_compare_strategies__mutmut_128, 
    'x_compare_strategies__mutmut_129': x_compare_strategies__mutmut_129, 
    'x_compare_strategies__mutmut_130': x_compare_strategies__mutmut_130, 
    'x_compare_strategies__mutmut_131': x_compare_strategies__mutmut_131, 
    'x_compare_strategies__mutmut_132': x_compare_strategies__mutmut_132, 
    'x_compare_strategies__mutmut_133': x_compare_strategies__mutmut_133, 
    'x_compare_strategies__mutmut_134': x_compare_strategies__mutmut_134, 
    'x_compare_strategies__mutmut_135': x_compare_strategies__mutmut_135, 
    'x_compare_strategies__mutmut_136': x_compare_strategies__mutmut_136, 
    'x_compare_strategies__mutmut_137': x_compare_strategies__mutmut_137, 
    'x_compare_strategies__mutmut_138': x_compare_strategies__mutmut_138, 
    'x_compare_strategies__mutmut_139': x_compare_strategies__mutmut_139, 
    'x_compare_strategies__mutmut_140': x_compare_strategies__mutmut_140, 
    'x_compare_strategies__mutmut_141': x_compare_strategies__mutmut_141, 
    'x_compare_strategies__mutmut_142': x_compare_strategies__mutmut_142, 
    'x_compare_strategies__mutmut_143': x_compare_strategies__mutmut_143, 
    'x_compare_strategies__mutmut_144': x_compare_strategies__mutmut_144, 
    'x_compare_strategies__mutmut_145': x_compare_strategies__mutmut_145, 
    'x_compare_strategies__mutmut_146': x_compare_strategies__mutmut_146, 
    'x_compare_strategies__mutmut_147': x_compare_strategies__mutmut_147, 
    'x_compare_strategies__mutmut_148': x_compare_strategies__mutmut_148, 
    'x_compare_strategies__mutmut_149': x_compare_strategies__mutmut_149, 
    'x_compare_strategies__mutmut_150': x_compare_strategies__mutmut_150, 
    'x_compare_strategies__mutmut_151': x_compare_strategies__mutmut_151, 
    'x_compare_strategies__mutmut_152': x_compare_strategies__mutmut_152, 
    'x_compare_strategies__mutmut_153': x_compare_strategies__mutmut_153, 
    'x_compare_strategies__mutmut_154': x_compare_strategies__mutmut_154, 
    'x_compare_strategies__mutmut_155': x_compare_strategies__mutmut_155, 
    'x_compare_strategies__mutmut_156': x_compare_strategies__mutmut_156, 
    'x_compare_strategies__mutmut_157': x_compare_strategies__mutmut_157, 
    'x_compare_strategies__mutmut_158': x_compare_strategies__mutmut_158, 
    'x_compare_strategies__mutmut_159': x_compare_strategies__mutmut_159, 
    'x_compare_strategies__mutmut_160': x_compare_strategies__mutmut_160, 
    'x_compare_strategies__mutmut_161': x_compare_strategies__mutmut_161, 
    'x_compare_strategies__mutmut_162': x_compare_strategies__mutmut_162, 
    'x_compare_strategies__mutmut_163': x_compare_strategies__mutmut_163, 
    'x_compare_strategies__mutmut_164': x_compare_strategies__mutmut_164, 
    'x_compare_strategies__mutmut_165': x_compare_strategies__mutmut_165, 
    'x_compare_strategies__mutmut_166': x_compare_strategies__mutmut_166, 
    'x_compare_strategies__mutmut_167': x_compare_strategies__mutmut_167, 
    'x_compare_strategies__mutmut_168': x_compare_strategies__mutmut_168, 
    'x_compare_strategies__mutmut_169': x_compare_strategies__mutmut_169, 
    'x_compare_strategies__mutmut_170': x_compare_strategies__mutmut_170, 
    'x_compare_strategies__mutmut_171': x_compare_strategies__mutmut_171, 
    'x_compare_strategies__mutmut_172': x_compare_strategies__mutmut_172, 
    'x_compare_strategies__mutmut_173': x_compare_strategies__mutmut_173, 
    'x_compare_strategies__mutmut_174': x_compare_strategies__mutmut_174, 
    'x_compare_strategies__mutmut_175': x_compare_strategies__mutmut_175, 
    'x_compare_strategies__mutmut_176': x_compare_strategies__mutmut_176, 
    'x_compare_strategies__mutmut_177': x_compare_strategies__mutmut_177, 
    'x_compare_strategies__mutmut_178': x_compare_strategies__mutmut_178, 
    'x_compare_strategies__mutmut_179': x_compare_strategies__mutmut_179, 
    'x_compare_strategies__mutmut_180': x_compare_strategies__mutmut_180, 
    'x_compare_strategies__mutmut_181': x_compare_strategies__mutmut_181, 
    'x_compare_strategies__mutmut_182': x_compare_strategies__mutmut_182, 
    'x_compare_strategies__mutmut_183': x_compare_strategies__mutmut_183, 
    'x_compare_strategies__mutmut_184': x_compare_strategies__mutmut_184, 
    'x_compare_strategies__mutmut_185': x_compare_strategies__mutmut_185, 
    'x_compare_strategies__mutmut_186': x_compare_strategies__mutmut_186
}

def compare_strategies(*args, **kwargs):
    result = _mutmut_trampoline(x_compare_strategies__mutmut_orig, x_compare_strategies__mutmut_mutants, args, kwargs)
    return result 

compare_strategies.__signature__ = _mutmut_signature(x_compare_strategies__mutmut_orig)
x_compare_strategies__mutmut_orig.__name__ = 'x_compare_strategies'


def x_compare_with_optimizer__mutmut_orig(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_1(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 121.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_2(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = True
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_3(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print(None)

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_4(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("XXComputing optimizer recommendation...\nXX")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_5(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_6(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("COMPUTING OPTIMIZER RECOMMENDATION...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_7(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = None

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_8(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(None, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_9(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, None, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_10(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=None)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_11(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_12(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_13(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, )

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_14(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(None)
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_15(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(None)

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_16(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = None

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_17(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(None)

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_18(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig(None, n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_19(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=None))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_20(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig(n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_21(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", ))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_22(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("XXSerialXX", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_23(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_24(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("SERIAL", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_25(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=2))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_26(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        None
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_27(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            None,
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_28(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=None,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_29(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=None,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_30(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=None
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_31(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_32(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_33(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_34(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_35(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "XXOptimizerXX",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_36(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_37(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "OPTIMIZER",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_38(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(None)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_39(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = None

    return comparison, optimization


def x_compare_with_optimizer__mutmut_40(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        None, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_41(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, None, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_42(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, None,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_43(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=None,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_44(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=None,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_45(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=None
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_46(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        data, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_47(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, configs,
        max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_48(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, max_items=max_items,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_49(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        timeout=timeout,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_50(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        verbose=verbose
    )

    return comparison, optimization


def x_compare_with_optimizer__mutmut_51(
    func: Callable[[Any], Any],
    data: Union[List, Iterator],
    additional_configs: Optional[List[ComparisonConfig]] = None,
    max_items: Optional[int] = None,
    timeout: float = 120.0,
    verbose: bool = False
) -> Tuple[ComparisonResult, OptimizationResult]:
    """
    Compare optimizer recommendation against alternative strategies.

    This convenience function:
    1. Gets optimizer recommendation
    2. Benchmarks optimizer recommendation + additional configs
    3. Returns comparison results and original optimization

    Args:
        func: Function to benchmark
        data: Input data
        additional_configs: Additional configurations to compare (optional)
        max_items: Maximum items to benchmark
        timeout: Maximum time per benchmark
        verbose: Print progress information

    Returns:
        Tuple of (ComparisonResult, OptimizationResult)

    Example:
        >>> result, opt = compare_with_optimizer(func, data, verbose=True)
        >>> print(f"Optimizer recommended: {opt.n_jobs} workers")
        >>> print(f"Best strategy: {result.best_config.name}")
    """
    if verbose:
        print("Computing optimizer recommendation...\n")

    # Get optimizer recommendation
    optimization = optimize(func, data, verbose=verbose)

    if verbose:
        print(f"\nOptimizer recommends: n_jobs={optimization.n_jobs}, "
              f"chunksize={optimization.chunksize}, executor={optimization.executor_type}")
        print(f"Predicted speedup: {optimization.estimated_speedup:.2f}x\n")

    # Build list of configs to compare
    configs = []

    # Add serial baseline
    configs.append(ComparisonConfig("Serial", n_jobs=1))

    # Add optimizer recommendation
    configs.append(
        ComparisonConfig(
            "Optimizer",
            n_jobs=optimization.n_jobs,
            chunksize=optimization.chunksize,
            executor_type=optimization.executor_type
        )
    )

    # Add any additional configs
    if additional_configs:
        configs.extend(additional_configs)

    # Run comparison
    comparison = compare_strategies(
        func, data, configs,
        max_items=max_items,
        timeout=timeout,
        )

    return comparison, optimization

x_compare_with_optimizer__mutmut_mutants : ClassVar[MutantDict] = {
'x_compare_with_optimizer__mutmut_1': x_compare_with_optimizer__mutmut_1, 
    'x_compare_with_optimizer__mutmut_2': x_compare_with_optimizer__mutmut_2, 
    'x_compare_with_optimizer__mutmut_3': x_compare_with_optimizer__mutmut_3, 
    'x_compare_with_optimizer__mutmut_4': x_compare_with_optimizer__mutmut_4, 
    'x_compare_with_optimizer__mutmut_5': x_compare_with_optimizer__mutmut_5, 
    'x_compare_with_optimizer__mutmut_6': x_compare_with_optimizer__mutmut_6, 
    'x_compare_with_optimizer__mutmut_7': x_compare_with_optimizer__mutmut_7, 
    'x_compare_with_optimizer__mutmut_8': x_compare_with_optimizer__mutmut_8, 
    'x_compare_with_optimizer__mutmut_9': x_compare_with_optimizer__mutmut_9, 
    'x_compare_with_optimizer__mutmut_10': x_compare_with_optimizer__mutmut_10, 
    'x_compare_with_optimizer__mutmut_11': x_compare_with_optimizer__mutmut_11, 
    'x_compare_with_optimizer__mutmut_12': x_compare_with_optimizer__mutmut_12, 
    'x_compare_with_optimizer__mutmut_13': x_compare_with_optimizer__mutmut_13, 
    'x_compare_with_optimizer__mutmut_14': x_compare_with_optimizer__mutmut_14, 
    'x_compare_with_optimizer__mutmut_15': x_compare_with_optimizer__mutmut_15, 
    'x_compare_with_optimizer__mutmut_16': x_compare_with_optimizer__mutmut_16, 
    'x_compare_with_optimizer__mutmut_17': x_compare_with_optimizer__mutmut_17, 
    'x_compare_with_optimizer__mutmut_18': x_compare_with_optimizer__mutmut_18, 
    'x_compare_with_optimizer__mutmut_19': x_compare_with_optimizer__mutmut_19, 
    'x_compare_with_optimizer__mutmut_20': x_compare_with_optimizer__mutmut_20, 
    'x_compare_with_optimizer__mutmut_21': x_compare_with_optimizer__mutmut_21, 
    'x_compare_with_optimizer__mutmut_22': x_compare_with_optimizer__mutmut_22, 
    'x_compare_with_optimizer__mutmut_23': x_compare_with_optimizer__mutmut_23, 
    'x_compare_with_optimizer__mutmut_24': x_compare_with_optimizer__mutmut_24, 
    'x_compare_with_optimizer__mutmut_25': x_compare_with_optimizer__mutmut_25, 
    'x_compare_with_optimizer__mutmut_26': x_compare_with_optimizer__mutmut_26, 
    'x_compare_with_optimizer__mutmut_27': x_compare_with_optimizer__mutmut_27, 
    'x_compare_with_optimizer__mutmut_28': x_compare_with_optimizer__mutmut_28, 
    'x_compare_with_optimizer__mutmut_29': x_compare_with_optimizer__mutmut_29, 
    'x_compare_with_optimizer__mutmut_30': x_compare_with_optimizer__mutmut_30, 
    'x_compare_with_optimizer__mutmut_31': x_compare_with_optimizer__mutmut_31, 
    'x_compare_with_optimizer__mutmut_32': x_compare_with_optimizer__mutmut_32, 
    'x_compare_with_optimizer__mutmut_33': x_compare_with_optimizer__mutmut_33, 
    'x_compare_with_optimizer__mutmut_34': x_compare_with_optimizer__mutmut_34, 
    'x_compare_with_optimizer__mutmut_35': x_compare_with_optimizer__mutmut_35, 
    'x_compare_with_optimizer__mutmut_36': x_compare_with_optimizer__mutmut_36, 
    'x_compare_with_optimizer__mutmut_37': x_compare_with_optimizer__mutmut_37, 
    'x_compare_with_optimizer__mutmut_38': x_compare_with_optimizer__mutmut_38, 
    'x_compare_with_optimizer__mutmut_39': x_compare_with_optimizer__mutmut_39, 
    'x_compare_with_optimizer__mutmut_40': x_compare_with_optimizer__mutmut_40, 
    'x_compare_with_optimizer__mutmut_41': x_compare_with_optimizer__mutmut_41, 
    'x_compare_with_optimizer__mutmut_42': x_compare_with_optimizer__mutmut_42, 
    'x_compare_with_optimizer__mutmut_43': x_compare_with_optimizer__mutmut_43, 
    'x_compare_with_optimizer__mutmut_44': x_compare_with_optimizer__mutmut_44, 
    'x_compare_with_optimizer__mutmut_45': x_compare_with_optimizer__mutmut_45, 
    'x_compare_with_optimizer__mutmut_46': x_compare_with_optimizer__mutmut_46, 
    'x_compare_with_optimizer__mutmut_47': x_compare_with_optimizer__mutmut_47, 
    'x_compare_with_optimizer__mutmut_48': x_compare_with_optimizer__mutmut_48, 
    'x_compare_with_optimizer__mutmut_49': x_compare_with_optimizer__mutmut_49, 
    'x_compare_with_optimizer__mutmut_50': x_compare_with_optimizer__mutmut_50, 
    'x_compare_with_optimizer__mutmut_51': x_compare_with_optimizer__mutmut_51
}

def compare_with_optimizer(*args, **kwargs):
    result = _mutmut_trampoline(x_compare_with_optimizer__mutmut_orig, x_compare_with_optimizer__mutmut_mutants, args, kwargs)
    return result 

compare_with_optimizer.__signature__ = _mutmut_signature(x_compare_with_optimizer__mutmut_orig)
x_compare_with_optimizer__mutmut_orig.__name__ = 'x_compare_with_optimizer'

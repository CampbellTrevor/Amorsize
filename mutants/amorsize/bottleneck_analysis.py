"""
Bottleneck analysis module for identifying performance limiters.

This module analyzes optimization results to identify what's preventing
better parallelization performance and provides actionable recommendations.
"""

from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

# Bottleneck detection thresholds
SPAWN_OVERHEAD_THRESHOLD = 0.2  # 20% of parallel time
IPC_OVERHEAD_THRESHOLD = 0.15  # 15% of parallel time
CHUNKING_OVERHEAD_THRESHOLD = 0.1  # 10% of parallel time
MEMORY_USAGE_THRESHOLD = 0.7  # 70% of available memory
MIN_COMPUTATION_TIME_PER_ITEM = 0.001  # 1ms per item minimum
MIN_TOTAL_WORKLOAD_TIME = 1.0  # 1 second minimum total work
HETEROGENEOUS_CV_THRESHOLD = 0.5  # Coefficient of variation threshold
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


class BottleneckType(Enum):
    """Types of performance bottlenecks."""
    SPAWN_OVERHEAD = "spawn_overhead"
    IPC_OVERHEAD = "ipc_overhead"
    CHUNKING_OVERHEAD = "chunking_overhead"
    MEMORY_CONSTRAINT = "memory_constraint"
    WORKLOAD_TOO_SMALL = "workload_too_small"
    INSUFFICIENT_COMPUTATION = "insufficient_computation"
    DATA_SIZE = "data_size"
    HETEROGENEOUS_WORKLOAD = "heterogeneous_workload"
    NONE = "none"


@dataclass
class BottleneckAnalysis:
    """
    Results of bottleneck analysis.
    
    Attributes:
        primary_bottleneck: The most significant performance limiter
        bottleneck_severity: Score from 0-1 indicating how much this bottleneck hurts performance
        contributing_factors: List of secondary bottlenecks
        recommendations: List of actionable recommendations to address bottlenecks
        overhead_breakdown: Percentage breakdown of overhead sources
        efficiency_score: Overall parallelization efficiency (0-1)
    """
    primary_bottleneck: BottleneckType
    bottleneck_severity: float
    contributing_factors: List[Tuple[BottleneckType, float]]
    recommendations: List[str]
    overhead_breakdown: Dict[str, float]
    efficiency_score: float


def x_analyze_bottlenecks__mutmut_orig(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_1(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 1.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_2(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = None
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_3(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = None
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_4(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = None
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_5(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time / total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_6(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = None
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_7(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time * n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_8(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs >= 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_9(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 1 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_10(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = None
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_11(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead - chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_12(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost - ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_13(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = None
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_14(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(None, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_15(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, None)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_16(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_17(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, )
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_18(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = None
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_19(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup * theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_20(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup >= 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_21(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 1 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_22(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 1.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_23(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = None
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_24(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(None, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_25(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, None)
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_26(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_27(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, )
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_28(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(2.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_29(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(None, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_30(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, None))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_31(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_32(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, ))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_33(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(1.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_34(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = None
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_35(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time - total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_36(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = None
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_37(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time >= 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_38(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 1:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_39(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = None
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_40(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['XXcomputationXX'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_41(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['COMPUTATION'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_42(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) / 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_43(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time * total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_44(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 101
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_45(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = None
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_46(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['XXspawnXX'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_47(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['SPAWN'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_48(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) / 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_49(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost * total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_50(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 101
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_51(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = None
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_52(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['XXipcXX'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_53(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['IPC'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_54(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) / 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_55(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead * total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_56(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 101
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_57(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = None
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_58(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['XXchunkingXX'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_59(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['CHUNKING'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_60(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) / 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_61(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead * total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_62(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 101
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_63(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 or spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_64(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time >= 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_65(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 1 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_66(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost * total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_67(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time >= SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_68(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = None
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_69(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost * total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_70(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append(None)
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_71(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            None
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_72(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "XXSpawn overhead is significant. Consider:\nXX"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_73(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "spawn overhead is significant. consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_74(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "SPAWN OVERHEAD IS SIGNIFICANT. CONSIDER:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_75(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "XX  • Increasing workload per item to amortize spawn costs\nXX"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_76(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_77(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • INCREASING WORKLOAD PER ITEM TO AMORTIZE SPAWN COSTS\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_78(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "XX  • Using a persistent pool with PoolManager for repeated operationsXX"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_79(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • using a persistent pool with poolmanager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_80(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • USING A PERSISTENT POOL WITH POOLMANAGER FOR REPEATED OPERATIONS"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_81(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 or ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_82(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time >= 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_83(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 1 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_84(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead * total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_85(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time >= IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_86(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = None
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_87(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead * total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_88(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append(None)
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_89(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            None
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_90(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "XXIPC/serialization overhead is significant. Consider:\nXX"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_91(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "ipc/serialization overhead is significant. consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_92(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/SERIALIZATION OVERHEAD IS SIGNIFICANT. CONSIDER:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_93(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "XX  • Reducing data size passed to workers\nXX"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_94(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_95(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • REDUCING DATA SIZE PASSED TO WORKERS\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_96(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "XX  • Using more efficient data structures (arrays instead of objects)\nXX"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_97(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_98(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • USING MORE EFFICIENT DATA STRUCTURES (ARRAYS INSTEAD OF OBJECTS)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_99(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "XX  • Processing data in larger chunks to amortize pickling costsXX"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_100(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_101(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • PROCESSING DATA IN LARGER CHUNKS TO AMORTIZE PICKLING COSTS"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_102(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = None
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_103(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) / chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_104(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize + 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_105(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items - chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_106(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 2) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_107(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize >= 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_108(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 1 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_109(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 or chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_110(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time >= 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_111(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 1 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_112(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead * total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_113(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time >= CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_114(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = None
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_115(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead * total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_116(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append(None)
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_117(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            None
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_118(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize / 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_119(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 3} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_120(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "XX  • Using batch processing for very large datasetsXX"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_121(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_122(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • USING BATCH PROCESSING FOR VERY LARGE DATASETS"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_123(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = None
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_124(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) * available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_125(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs / estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_126(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory >= 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_127(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 1 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_128(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 1
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_129(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores or memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_130(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs <= physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_131(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio >= MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_132(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = None
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_133(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(None, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_134(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, None)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_135(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_136(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, )
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_137(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(2.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_138(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append(None)
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_139(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            None
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_140(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "XX  • Processing data in smaller batches\nXX"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_141(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_142(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • PROCESSING DATA IN SMALLER BATCHES\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_143(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "XX  • Reducing memory footprint of your function\nXX"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_144(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_145(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • REDUCING MEMORY FOOTPRINT OF YOUR FUNCTION\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_146(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory * (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_147(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024 * 3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_148(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1025**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_149(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**4):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_150(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 or avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_151(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time >= 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_152(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 1 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_153(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time <= MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_154(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = None
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_155(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 + min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_156(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 2.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_157(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(None, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_158(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, None)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_159(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_160(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, )
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_161(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(2.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_162(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time * MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_163(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append(None)
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_164(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            None
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_165(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time / 1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_166(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1001:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_167(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "XX  • Increasing computation per item\nXX"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_168(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_169(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • INCREASING COMPUTATION PER ITEM\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_170(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "XX  • Batching multiple items together before processing\nXX"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_171(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_172(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • BATCHING MULTIPLE ITEMS TOGETHER BEFORE PROCESSING\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_173(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "XX  • Using serial execution for such lightweight tasksXX"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_174(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_175(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • USING SERIAL EXECUTION FOR SUCH LIGHTWEIGHT TASKS"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_176(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 or total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_177(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time >= 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_178(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 1 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_179(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time <= MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_180(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = None
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_181(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 + min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_182(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 2.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_183(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(None, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_184(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, None)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_185(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_186(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, )
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_187(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(2.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_188(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append(None)
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_189(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            None
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_190(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "XX  • Accumulating more data before processing\nXX"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_191(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_192(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • ACCUMULATING MORE DATA BEFORE PROCESSING\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_193(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "XX  • Using serial execution for such small workloadsXX"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_194(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_195(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • USING SERIAL EXECUTION FOR SUCH SMALL WORKLOADS"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_196(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation >= HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_197(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = None
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_198(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(None, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_199(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, None)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_200(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_201(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, )
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_202(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(2.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_203(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append(None)
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_204(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            None
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_205(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "XX  • Using dynamic chunking with smaller chunks\nXX"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_206(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_207(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • USING DYNAMIC CHUNKING WITH SMALLER CHUNKS\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_208(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "XX  • Sorting items by expected execution time\nXX"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_209(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_210(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • SORTING ITEMS BY EXPECTED EXECUTION TIME\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_211(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "XX  • Using imap_unordered for better load balancingXX"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_212(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_213(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • USING IMAP_UNORDERED FOR BETTER LOAD BALANCING"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_214(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=None, reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_215(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=None)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_216(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_217(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], )
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_218(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: None, reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_219(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[2], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_220(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=False)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_221(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = None
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_222(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[1][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_223(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][1]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_224(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = None
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_225(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[1][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_226(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][2]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_227(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = None
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_228(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[2:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_229(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) >= 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_230(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 2 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_231(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = None
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_232(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = None
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_233(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 1.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_234(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = None
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_235(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score >= 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_236(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 1.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_237(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append(None)
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_238(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("XX✅ Excellent parallelization efficiency! No significant bottlenecks detected.XX")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_239(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ excellent parallelization efficiency! no significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_240(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ EXCELLENT PARALLELIZATION EFFICIENCY! NO SIGNIFICANT BOTTLENECKS DETECTED.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_241(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score >= 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_242(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 1.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_243(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append(None)
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_244(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("XXGood parallelization efficiency. Minor optimizations possible.XX")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_245(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("good parallelization efficiency. minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_246(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("GOOD PARALLELIZATION EFFICIENCY. MINOR OPTIMIZATIONS POSSIBLE.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_247(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=None,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_248(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=None,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_249(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=None,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_250(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=None,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_251(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=None,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_252(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=None
    )


def x_analyze_bottlenecks__mutmut_253(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_254(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_255(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_256(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        overhead_breakdown=overhead_breakdown,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_257(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        efficiency_score=efficiency_score
    )


def x_analyze_bottlenecks__mutmut_258(
    n_jobs: int,
    chunksize: int,
    total_items: int,
    avg_execution_time: float,
    spawn_cost: float,
    ipc_overhead: float,
    chunking_overhead: float,
    estimated_speedup: float,
    physical_cores: int,
    available_memory: int,
    estimated_memory_per_job: int,
    coefficient_of_variation: float = 0.0
) -> BottleneckAnalysis:
    """
    Analyze optimization results to identify performance bottlenecks.
    
    Args:
        n_jobs: Number of parallel workers
        chunksize: Chunk size for task distribution
        total_items: Total number of items to process
        avg_execution_time: Average time per item (seconds)
        spawn_cost: Process spawn overhead (seconds)
        ipc_overhead: Inter-process communication overhead (seconds)
        chunking_overhead: Task distribution overhead (seconds)
        estimated_speedup: Expected speedup factor
        physical_cores: Number of physical CPU cores
        available_memory: Available system memory (bytes)
        estimated_memory_per_job: Memory needed per worker (bytes)
        coefficient_of_variation: Workload variability (0 = uniform, >0.5 = heterogeneous)
    
    Returns:
        BottleneckAnalysis with identified bottlenecks and recommendations
    """
    recommendations = []
    bottlenecks = []
    
    # Calculate total execution time components
    total_serial_time = avg_execution_time * total_items
    parallel_compute_time = total_serial_time / n_jobs if n_jobs > 0 else total_serial_time
    total_overhead = spawn_cost + ipc_overhead + chunking_overhead
    
    # Calculate efficiency score (actual speedup / theoretical maximum)
    theoretical_max_speedup = min(n_jobs, physical_cores)
    efficiency_score = estimated_speedup / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
    efficiency_score = min(1.0, max(0.0, efficiency_score))
    
    # Overhead breakdown percentages
    total_parallel_time = parallel_compute_time + total_overhead
    overhead_breakdown = {}
    if total_parallel_time > 0:
        overhead_breakdown['computation'] = (parallel_compute_time / total_parallel_time) * 100
        overhead_breakdown['spawn'] = (spawn_cost / total_parallel_time) * 100
        overhead_breakdown['ipc'] = (ipc_overhead / total_parallel_time) * 100
        overhead_breakdown['chunking'] = (chunking_overhead / total_parallel_time) * 100
    
    # 1. Check for spawn overhead bottleneck
    if total_parallel_time > 0 and spawn_cost / total_parallel_time > SPAWN_OVERHEAD_THRESHOLD:
        severity = spawn_cost / total_parallel_time
        bottlenecks.append((BottleneckType.SPAWN_OVERHEAD, severity))
        recommendations.append(
            "Spawn overhead is significant. Consider:\n"
            f"  • Using 'fork' or 'forkserver' start method (current spawn cost: {spawn_cost:.3f}s)\n"
            "  • Increasing workload per item to amortize spawn costs\n"
            "  • Using a persistent pool with PoolManager for repeated operations"
        )
    
    # 2. Check for IPC overhead bottleneck
    if total_parallel_time > 0 and ipc_overhead / total_parallel_time > IPC_OVERHEAD_THRESHOLD:
        severity = ipc_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.IPC_OVERHEAD, severity))
        recommendations.append(
            "IPC/serialization overhead is significant. Consider:\n"
            "  • Reducing data size passed to workers\n"
            "  • Using more efficient data structures (arrays instead of objects)\n"
            "  • Processing data in larger chunks to amortize pickling costs"
        )
    
    # 3. Check for chunking overhead bottleneck
    num_chunks = (total_items + chunksize - 1) // chunksize if chunksize > 0 else total_items
    if total_parallel_time > 0 and chunking_overhead / total_parallel_time > CHUNKING_OVERHEAD_THRESHOLD:
        severity = chunking_overhead / total_parallel_time
        bottlenecks.append((BottleneckType.CHUNKING_OVERHEAD, severity))
        recommendations.append(
            f"Task distribution overhead is significant ({num_chunks} chunks). Consider:\n"
            f"  • Increasing chunksize from {chunksize} to {chunksize * 2} or more\n"
            "  • Using batch processing for very large datasets"
        )
    
    # 4. Check for memory constraint bottleneck
    memory_usage_ratio = (n_jobs * estimated_memory_per_job) / available_memory if available_memory > 0 else 0
    if n_jobs < physical_cores and memory_usage_ratio > MEMORY_USAGE_THRESHOLD:
        severity = min(1.0, memory_usage_ratio)
        bottlenecks.append((BottleneckType.MEMORY_CONSTRAINT, severity))
        recommendations.append(
            f"Memory constraints limiting workers (using {n_jobs}/{physical_cores} cores). Consider:\n"
            "  • Processing data in smaller batches\n"
            "  • Reducing memory footprint of your function\n"
            f"  • Adding more RAM (current: {available_memory / (1024**3):.1f} GB)"
        )
    
    # 5. Check for insufficient computation per item
    if avg_execution_time > 0 and avg_execution_time < MIN_COMPUTATION_TIME_PER_ITEM:
        severity = 1.0 - min(1.0, avg_execution_time / MIN_COMPUTATION_TIME_PER_ITEM)
        bottlenecks.append((BottleneckType.INSUFFICIENT_COMPUTATION, severity))
        recommendations.append(
            f"Each item takes only {avg_execution_time*1000:.3f}ms. Overhead dominates. Consider:\n"
            "  • Increasing computation per item\n"
            "  • Batching multiple items together before processing\n"
            "  • Using serial execution for such lightweight tasks"
        )
    
    # 6. Check for small workload bottleneck
    if total_serial_time > 0 and total_serial_time < MIN_TOTAL_WORKLOAD_TIME:
        severity = 1.0 - min(1.0, total_serial_time)
        bottlenecks.append((BottleneckType.WORKLOAD_TOO_SMALL, severity))
        recommendations.append(
            f"Total workload is small ({total_serial_time:.3f}s). Consider:\n"
            "  • Accumulating more data before processing\n"
            f"  • Increasing dataset size from {total_items} items\n"
            "  • Using serial execution for such small workloads"
        )
    
    # 7. Check for heterogeneous workload
    if coefficient_of_variation > HETEROGENEOUS_CV_THRESHOLD:
        severity = min(1.0, coefficient_of_variation)
        bottlenecks.append((BottleneckType.HETEROGENEOUS_WORKLOAD, severity))
        recommendations.append(
            f"Workload is heterogeneous (CV={coefficient_of_variation:.2f}). Consider:\n"
            "  • Using dynamic chunking with smaller chunks\n"
            "  • Sorting items by expected execution time\n"
            "  • Using imap_unordered for better load balancing"
        )
    
    # Determine primary bottleneck (highest severity)
    if bottlenecks:
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        primary_bottleneck = bottlenecks[0][0]
        bottleneck_severity = bottlenecks[0][1]
        contributing_factors = bottlenecks[1:] if len(bottlenecks) > 1 else []
    else:
        primary_bottleneck = BottleneckType.NONE
        bottleneck_severity = 0.0
        contributing_factors = []
        if efficiency_score > 0.8:
            recommendations.append("✅ Excellent parallelization efficiency! No significant bottlenecks detected.")
        elif efficiency_score > 0.5:
            recommendations.append("Good parallelization efficiency. Minor optimizations possible.")
    
    return BottleneckAnalysis(
        primary_bottleneck=primary_bottleneck,
        bottleneck_severity=bottleneck_severity,
        contributing_factors=contributing_factors,
        recommendations=recommendations,
        overhead_breakdown=overhead_breakdown,
        )

x_analyze_bottlenecks__mutmut_mutants : ClassVar[MutantDict] = {
'x_analyze_bottlenecks__mutmut_1': x_analyze_bottlenecks__mutmut_1, 
    'x_analyze_bottlenecks__mutmut_2': x_analyze_bottlenecks__mutmut_2, 
    'x_analyze_bottlenecks__mutmut_3': x_analyze_bottlenecks__mutmut_3, 
    'x_analyze_bottlenecks__mutmut_4': x_analyze_bottlenecks__mutmut_4, 
    'x_analyze_bottlenecks__mutmut_5': x_analyze_bottlenecks__mutmut_5, 
    'x_analyze_bottlenecks__mutmut_6': x_analyze_bottlenecks__mutmut_6, 
    'x_analyze_bottlenecks__mutmut_7': x_analyze_bottlenecks__mutmut_7, 
    'x_analyze_bottlenecks__mutmut_8': x_analyze_bottlenecks__mutmut_8, 
    'x_analyze_bottlenecks__mutmut_9': x_analyze_bottlenecks__mutmut_9, 
    'x_analyze_bottlenecks__mutmut_10': x_analyze_bottlenecks__mutmut_10, 
    'x_analyze_bottlenecks__mutmut_11': x_analyze_bottlenecks__mutmut_11, 
    'x_analyze_bottlenecks__mutmut_12': x_analyze_bottlenecks__mutmut_12, 
    'x_analyze_bottlenecks__mutmut_13': x_analyze_bottlenecks__mutmut_13, 
    'x_analyze_bottlenecks__mutmut_14': x_analyze_bottlenecks__mutmut_14, 
    'x_analyze_bottlenecks__mutmut_15': x_analyze_bottlenecks__mutmut_15, 
    'x_analyze_bottlenecks__mutmut_16': x_analyze_bottlenecks__mutmut_16, 
    'x_analyze_bottlenecks__mutmut_17': x_analyze_bottlenecks__mutmut_17, 
    'x_analyze_bottlenecks__mutmut_18': x_analyze_bottlenecks__mutmut_18, 
    'x_analyze_bottlenecks__mutmut_19': x_analyze_bottlenecks__mutmut_19, 
    'x_analyze_bottlenecks__mutmut_20': x_analyze_bottlenecks__mutmut_20, 
    'x_analyze_bottlenecks__mutmut_21': x_analyze_bottlenecks__mutmut_21, 
    'x_analyze_bottlenecks__mutmut_22': x_analyze_bottlenecks__mutmut_22, 
    'x_analyze_bottlenecks__mutmut_23': x_analyze_bottlenecks__mutmut_23, 
    'x_analyze_bottlenecks__mutmut_24': x_analyze_bottlenecks__mutmut_24, 
    'x_analyze_bottlenecks__mutmut_25': x_analyze_bottlenecks__mutmut_25, 
    'x_analyze_bottlenecks__mutmut_26': x_analyze_bottlenecks__mutmut_26, 
    'x_analyze_bottlenecks__mutmut_27': x_analyze_bottlenecks__mutmut_27, 
    'x_analyze_bottlenecks__mutmut_28': x_analyze_bottlenecks__mutmut_28, 
    'x_analyze_bottlenecks__mutmut_29': x_analyze_bottlenecks__mutmut_29, 
    'x_analyze_bottlenecks__mutmut_30': x_analyze_bottlenecks__mutmut_30, 
    'x_analyze_bottlenecks__mutmut_31': x_analyze_bottlenecks__mutmut_31, 
    'x_analyze_bottlenecks__mutmut_32': x_analyze_bottlenecks__mutmut_32, 
    'x_analyze_bottlenecks__mutmut_33': x_analyze_bottlenecks__mutmut_33, 
    'x_analyze_bottlenecks__mutmut_34': x_analyze_bottlenecks__mutmut_34, 
    'x_analyze_bottlenecks__mutmut_35': x_analyze_bottlenecks__mutmut_35, 
    'x_analyze_bottlenecks__mutmut_36': x_analyze_bottlenecks__mutmut_36, 
    'x_analyze_bottlenecks__mutmut_37': x_analyze_bottlenecks__mutmut_37, 
    'x_analyze_bottlenecks__mutmut_38': x_analyze_bottlenecks__mutmut_38, 
    'x_analyze_bottlenecks__mutmut_39': x_analyze_bottlenecks__mutmut_39, 
    'x_analyze_bottlenecks__mutmut_40': x_analyze_bottlenecks__mutmut_40, 
    'x_analyze_bottlenecks__mutmut_41': x_analyze_bottlenecks__mutmut_41, 
    'x_analyze_bottlenecks__mutmut_42': x_analyze_bottlenecks__mutmut_42, 
    'x_analyze_bottlenecks__mutmut_43': x_analyze_bottlenecks__mutmut_43, 
    'x_analyze_bottlenecks__mutmut_44': x_analyze_bottlenecks__mutmut_44, 
    'x_analyze_bottlenecks__mutmut_45': x_analyze_bottlenecks__mutmut_45, 
    'x_analyze_bottlenecks__mutmut_46': x_analyze_bottlenecks__mutmut_46, 
    'x_analyze_bottlenecks__mutmut_47': x_analyze_bottlenecks__mutmut_47, 
    'x_analyze_bottlenecks__mutmut_48': x_analyze_bottlenecks__mutmut_48, 
    'x_analyze_bottlenecks__mutmut_49': x_analyze_bottlenecks__mutmut_49, 
    'x_analyze_bottlenecks__mutmut_50': x_analyze_bottlenecks__mutmut_50, 
    'x_analyze_bottlenecks__mutmut_51': x_analyze_bottlenecks__mutmut_51, 
    'x_analyze_bottlenecks__mutmut_52': x_analyze_bottlenecks__mutmut_52, 
    'x_analyze_bottlenecks__mutmut_53': x_analyze_bottlenecks__mutmut_53, 
    'x_analyze_bottlenecks__mutmut_54': x_analyze_bottlenecks__mutmut_54, 
    'x_analyze_bottlenecks__mutmut_55': x_analyze_bottlenecks__mutmut_55, 
    'x_analyze_bottlenecks__mutmut_56': x_analyze_bottlenecks__mutmut_56, 
    'x_analyze_bottlenecks__mutmut_57': x_analyze_bottlenecks__mutmut_57, 
    'x_analyze_bottlenecks__mutmut_58': x_analyze_bottlenecks__mutmut_58, 
    'x_analyze_bottlenecks__mutmut_59': x_analyze_bottlenecks__mutmut_59, 
    'x_analyze_bottlenecks__mutmut_60': x_analyze_bottlenecks__mutmut_60, 
    'x_analyze_bottlenecks__mutmut_61': x_analyze_bottlenecks__mutmut_61, 
    'x_analyze_bottlenecks__mutmut_62': x_analyze_bottlenecks__mutmut_62, 
    'x_analyze_bottlenecks__mutmut_63': x_analyze_bottlenecks__mutmut_63, 
    'x_analyze_bottlenecks__mutmut_64': x_analyze_bottlenecks__mutmut_64, 
    'x_analyze_bottlenecks__mutmut_65': x_analyze_bottlenecks__mutmut_65, 
    'x_analyze_bottlenecks__mutmut_66': x_analyze_bottlenecks__mutmut_66, 
    'x_analyze_bottlenecks__mutmut_67': x_analyze_bottlenecks__mutmut_67, 
    'x_analyze_bottlenecks__mutmut_68': x_analyze_bottlenecks__mutmut_68, 
    'x_analyze_bottlenecks__mutmut_69': x_analyze_bottlenecks__mutmut_69, 
    'x_analyze_bottlenecks__mutmut_70': x_analyze_bottlenecks__mutmut_70, 
    'x_analyze_bottlenecks__mutmut_71': x_analyze_bottlenecks__mutmut_71, 
    'x_analyze_bottlenecks__mutmut_72': x_analyze_bottlenecks__mutmut_72, 
    'x_analyze_bottlenecks__mutmut_73': x_analyze_bottlenecks__mutmut_73, 
    'x_analyze_bottlenecks__mutmut_74': x_analyze_bottlenecks__mutmut_74, 
    'x_analyze_bottlenecks__mutmut_75': x_analyze_bottlenecks__mutmut_75, 
    'x_analyze_bottlenecks__mutmut_76': x_analyze_bottlenecks__mutmut_76, 
    'x_analyze_bottlenecks__mutmut_77': x_analyze_bottlenecks__mutmut_77, 
    'x_analyze_bottlenecks__mutmut_78': x_analyze_bottlenecks__mutmut_78, 
    'x_analyze_bottlenecks__mutmut_79': x_analyze_bottlenecks__mutmut_79, 
    'x_analyze_bottlenecks__mutmut_80': x_analyze_bottlenecks__mutmut_80, 
    'x_analyze_bottlenecks__mutmut_81': x_analyze_bottlenecks__mutmut_81, 
    'x_analyze_bottlenecks__mutmut_82': x_analyze_bottlenecks__mutmut_82, 
    'x_analyze_bottlenecks__mutmut_83': x_analyze_bottlenecks__mutmut_83, 
    'x_analyze_bottlenecks__mutmut_84': x_analyze_bottlenecks__mutmut_84, 
    'x_analyze_bottlenecks__mutmut_85': x_analyze_bottlenecks__mutmut_85, 
    'x_analyze_bottlenecks__mutmut_86': x_analyze_bottlenecks__mutmut_86, 
    'x_analyze_bottlenecks__mutmut_87': x_analyze_bottlenecks__mutmut_87, 
    'x_analyze_bottlenecks__mutmut_88': x_analyze_bottlenecks__mutmut_88, 
    'x_analyze_bottlenecks__mutmut_89': x_analyze_bottlenecks__mutmut_89, 
    'x_analyze_bottlenecks__mutmut_90': x_analyze_bottlenecks__mutmut_90, 
    'x_analyze_bottlenecks__mutmut_91': x_analyze_bottlenecks__mutmut_91, 
    'x_analyze_bottlenecks__mutmut_92': x_analyze_bottlenecks__mutmut_92, 
    'x_analyze_bottlenecks__mutmut_93': x_analyze_bottlenecks__mutmut_93, 
    'x_analyze_bottlenecks__mutmut_94': x_analyze_bottlenecks__mutmut_94, 
    'x_analyze_bottlenecks__mutmut_95': x_analyze_bottlenecks__mutmut_95, 
    'x_analyze_bottlenecks__mutmut_96': x_analyze_bottlenecks__mutmut_96, 
    'x_analyze_bottlenecks__mutmut_97': x_analyze_bottlenecks__mutmut_97, 
    'x_analyze_bottlenecks__mutmut_98': x_analyze_bottlenecks__mutmut_98, 
    'x_analyze_bottlenecks__mutmut_99': x_analyze_bottlenecks__mutmut_99, 
    'x_analyze_bottlenecks__mutmut_100': x_analyze_bottlenecks__mutmut_100, 
    'x_analyze_bottlenecks__mutmut_101': x_analyze_bottlenecks__mutmut_101, 
    'x_analyze_bottlenecks__mutmut_102': x_analyze_bottlenecks__mutmut_102, 
    'x_analyze_bottlenecks__mutmut_103': x_analyze_bottlenecks__mutmut_103, 
    'x_analyze_bottlenecks__mutmut_104': x_analyze_bottlenecks__mutmut_104, 
    'x_analyze_bottlenecks__mutmut_105': x_analyze_bottlenecks__mutmut_105, 
    'x_analyze_bottlenecks__mutmut_106': x_analyze_bottlenecks__mutmut_106, 
    'x_analyze_bottlenecks__mutmut_107': x_analyze_bottlenecks__mutmut_107, 
    'x_analyze_bottlenecks__mutmut_108': x_analyze_bottlenecks__mutmut_108, 
    'x_analyze_bottlenecks__mutmut_109': x_analyze_bottlenecks__mutmut_109, 
    'x_analyze_bottlenecks__mutmut_110': x_analyze_bottlenecks__mutmut_110, 
    'x_analyze_bottlenecks__mutmut_111': x_analyze_bottlenecks__mutmut_111, 
    'x_analyze_bottlenecks__mutmut_112': x_analyze_bottlenecks__mutmut_112, 
    'x_analyze_bottlenecks__mutmut_113': x_analyze_bottlenecks__mutmut_113, 
    'x_analyze_bottlenecks__mutmut_114': x_analyze_bottlenecks__mutmut_114, 
    'x_analyze_bottlenecks__mutmut_115': x_analyze_bottlenecks__mutmut_115, 
    'x_analyze_bottlenecks__mutmut_116': x_analyze_bottlenecks__mutmut_116, 
    'x_analyze_bottlenecks__mutmut_117': x_analyze_bottlenecks__mutmut_117, 
    'x_analyze_bottlenecks__mutmut_118': x_analyze_bottlenecks__mutmut_118, 
    'x_analyze_bottlenecks__mutmut_119': x_analyze_bottlenecks__mutmut_119, 
    'x_analyze_bottlenecks__mutmut_120': x_analyze_bottlenecks__mutmut_120, 
    'x_analyze_bottlenecks__mutmut_121': x_analyze_bottlenecks__mutmut_121, 
    'x_analyze_bottlenecks__mutmut_122': x_analyze_bottlenecks__mutmut_122, 
    'x_analyze_bottlenecks__mutmut_123': x_analyze_bottlenecks__mutmut_123, 
    'x_analyze_bottlenecks__mutmut_124': x_analyze_bottlenecks__mutmut_124, 
    'x_analyze_bottlenecks__mutmut_125': x_analyze_bottlenecks__mutmut_125, 
    'x_analyze_bottlenecks__mutmut_126': x_analyze_bottlenecks__mutmut_126, 
    'x_analyze_bottlenecks__mutmut_127': x_analyze_bottlenecks__mutmut_127, 
    'x_analyze_bottlenecks__mutmut_128': x_analyze_bottlenecks__mutmut_128, 
    'x_analyze_bottlenecks__mutmut_129': x_analyze_bottlenecks__mutmut_129, 
    'x_analyze_bottlenecks__mutmut_130': x_analyze_bottlenecks__mutmut_130, 
    'x_analyze_bottlenecks__mutmut_131': x_analyze_bottlenecks__mutmut_131, 
    'x_analyze_bottlenecks__mutmut_132': x_analyze_bottlenecks__mutmut_132, 
    'x_analyze_bottlenecks__mutmut_133': x_analyze_bottlenecks__mutmut_133, 
    'x_analyze_bottlenecks__mutmut_134': x_analyze_bottlenecks__mutmut_134, 
    'x_analyze_bottlenecks__mutmut_135': x_analyze_bottlenecks__mutmut_135, 
    'x_analyze_bottlenecks__mutmut_136': x_analyze_bottlenecks__mutmut_136, 
    'x_analyze_bottlenecks__mutmut_137': x_analyze_bottlenecks__mutmut_137, 
    'x_analyze_bottlenecks__mutmut_138': x_analyze_bottlenecks__mutmut_138, 
    'x_analyze_bottlenecks__mutmut_139': x_analyze_bottlenecks__mutmut_139, 
    'x_analyze_bottlenecks__mutmut_140': x_analyze_bottlenecks__mutmut_140, 
    'x_analyze_bottlenecks__mutmut_141': x_analyze_bottlenecks__mutmut_141, 
    'x_analyze_bottlenecks__mutmut_142': x_analyze_bottlenecks__mutmut_142, 
    'x_analyze_bottlenecks__mutmut_143': x_analyze_bottlenecks__mutmut_143, 
    'x_analyze_bottlenecks__mutmut_144': x_analyze_bottlenecks__mutmut_144, 
    'x_analyze_bottlenecks__mutmut_145': x_analyze_bottlenecks__mutmut_145, 
    'x_analyze_bottlenecks__mutmut_146': x_analyze_bottlenecks__mutmut_146, 
    'x_analyze_bottlenecks__mutmut_147': x_analyze_bottlenecks__mutmut_147, 
    'x_analyze_bottlenecks__mutmut_148': x_analyze_bottlenecks__mutmut_148, 
    'x_analyze_bottlenecks__mutmut_149': x_analyze_bottlenecks__mutmut_149, 
    'x_analyze_bottlenecks__mutmut_150': x_analyze_bottlenecks__mutmut_150, 
    'x_analyze_bottlenecks__mutmut_151': x_analyze_bottlenecks__mutmut_151, 
    'x_analyze_bottlenecks__mutmut_152': x_analyze_bottlenecks__mutmut_152, 
    'x_analyze_bottlenecks__mutmut_153': x_analyze_bottlenecks__mutmut_153, 
    'x_analyze_bottlenecks__mutmut_154': x_analyze_bottlenecks__mutmut_154, 
    'x_analyze_bottlenecks__mutmut_155': x_analyze_bottlenecks__mutmut_155, 
    'x_analyze_bottlenecks__mutmut_156': x_analyze_bottlenecks__mutmut_156, 
    'x_analyze_bottlenecks__mutmut_157': x_analyze_bottlenecks__mutmut_157, 
    'x_analyze_bottlenecks__mutmut_158': x_analyze_bottlenecks__mutmut_158, 
    'x_analyze_bottlenecks__mutmut_159': x_analyze_bottlenecks__mutmut_159, 
    'x_analyze_bottlenecks__mutmut_160': x_analyze_bottlenecks__mutmut_160, 
    'x_analyze_bottlenecks__mutmut_161': x_analyze_bottlenecks__mutmut_161, 
    'x_analyze_bottlenecks__mutmut_162': x_analyze_bottlenecks__mutmut_162, 
    'x_analyze_bottlenecks__mutmut_163': x_analyze_bottlenecks__mutmut_163, 
    'x_analyze_bottlenecks__mutmut_164': x_analyze_bottlenecks__mutmut_164, 
    'x_analyze_bottlenecks__mutmut_165': x_analyze_bottlenecks__mutmut_165, 
    'x_analyze_bottlenecks__mutmut_166': x_analyze_bottlenecks__mutmut_166, 
    'x_analyze_bottlenecks__mutmut_167': x_analyze_bottlenecks__mutmut_167, 
    'x_analyze_bottlenecks__mutmut_168': x_analyze_bottlenecks__mutmut_168, 
    'x_analyze_bottlenecks__mutmut_169': x_analyze_bottlenecks__mutmut_169, 
    'x_analyze_bottlenecks__mutmut_170': x_analyze_bottlenecks__mutmut_170, 
    'x_analyze_bottlenecks__mutmut_171': x_analyze_bottlenecks__mutmut_171, 
    'x_analyze_bottlenecks__mutmut_172': x_analyze_bottlenecks__mutmut_172, 
    'x_analyze_bottlenecks__mutmut_173': x_analyze_bottlenecks__mutmut_173, 
    'x_analyze_bottlenecks__mutmut_174': x_analyze_bottlenecks__mutmut_174, 
    'x_analyze_bottlenecks__mutmut_175': x_analyze_bottlenecks__mutmut_175, 
    'x_analyze_bottlenecks__mutmut_176': x_analyze_bottlenecks__mutmut_176, 
    'x_analyze_bottlenecks__mutmut_177': x_analyze_bottlenecks__mutmut_177, 
    'x_analyze_bottlenecks__mutmut_178': x_analyze_bottlenecks__mutmut_178, 
    'x_analyze_bottlenecks__mutmut_179': x_analyze_bottlenecks__mutmut_179, 
    'x_analyze_bottlenecks__mutmut_180': x_analyze_bottlenecks__mutmut_180, 
    'x_analyze_bottlenecks__mutmut_181': x_analyze_bottlenecks__mutmut_181, 
    'x_analyze_bottlenecks__mutmut_182': x_analyze_bottlenecks__mutmut_182, 
    'x_analyze_bottlenecks__mutmut_183': x_analyze_bottlenecks__mutmut_183, 
    'x_analyze_bottlenecks__mutmut_184': x_analyze_bottlenecks__mutmut_184, 
    'x_analyze_bottlenecks__mutmut_185': x_analyze_bottlenecks__mutmut_185, 
    'x_analyze_bottlenecks__mutmut_186': x_analyze_bottlenecks__mutmut_186, 
    'x_analyze_bottlenecks__mutmut_187': x_analyze_bottlenecks__mutmut_187, 
    'x_analyze_bottlenecks__mutmut_188': x_analyze_bottlenecks__mutmut_188, 
    'x_analyze_bottlenecks__mutmut_189': x_analyze_bottlenecks__mutmut_189, 
    'x_analyze_bottlenecks__mutmut_190': x_analyze_bottlenecks__mutmut_190, 
    'x_analyze_bottlenecks__mutmut_191': x_analyze_bottlenecks__mutmut_191, 
    'x_analyze_bottlenecks__mutmut_192': x_analyze_bottlenecks__mutmut_192, 
    'x_analyze_bottlenecks__mutmut_193': x_analyze_bottlenecks__mutmut_193, 
    'x_analyze_bottlenecks__mutmut_194': x_analyze_bottlenecks__mutmut_194, 
    'x_analyze_bottlenecks__mutmut_195': x_analyze_bottlenecks__mutmut_195, 
    'x_analyze_bottlenecks__mutmut_196': x_analyze_bottlenecks__mutmut_196, 
    'x_analyze_bottlenecks__mutmut_197': x_analyze_bottlenecks__mutmut_197, 
    'x_analyze_bottlenecks__mutmut_198': x_analyze_bottlenecks__mutmut_198, 
    'x_analyze_bottlenecks__mutmut_199': x_analyze_bottlenecks__mutmut_199, 
    'x_analyze_bottlenecks__mutmut_200': x_analyze_bottlenecks__mutmut_200, 
    'x_analyze_bottlenecks__mutmut_201': x_analyze_bottlenecks__mutmut_201, 
    'x_analyze_bottlenecks__mutmut_202': x_analyze_bottlenecks__mutmut_202, 
    'x_analyze_bottlenecks__mutmut_203': x_analyze_bottlenecks__mutmut_203, 
    'x_analyze_bottlenecks__mutmut_204': x_analyze_bottlenecks__mutmut_204, 
    'x_analyze_bottlenecks__mutmut_205': x_analyze_bottlenecks__mutmut_205, 
    'x_analyze_bottlenecks__mutmut_206': x_analyze_bottlenecks__mutmut_206, 
    'x_analyze_bottlenecks__mutmut_207': x_analyze_bottlenecks__mutmut_207, 
    'x_analyze_bottlenecks__mutmut_208': x_analyze_bottlenecks__mutmut_208, 
    'x_analyze_bottlenecks__mutmut_209': x_analyze_bottlenecks__mutmut_209, 
    'x_analyze_bottlenecks__mutmut_210': x_analyze_bottlenecks__mutmut_210, 
    'x_analyze_bottlenecks__mutmut_211': x_analyze_bottlenecks__mutmut_211, 
    'x_analyze_bottlenecks__mutmut_212': x_analyze_bottlenecks__mutmut_212, 
    'x_analyze_bottlenecks__mutmut_213': x_analyze_bottlenecks__mutmut_213, 
    'x_analyze_bottlenecks__mutmut_214': x_analyze_bottlenecks__mutmut_214, 
    'x_analyze_bottlenecks__mutmut_215': x_analyze_bottlenecks__mutmut_215, 
    'x_analyze_bottlenecks__mutmut_216': x_analyze_bottlenecks__mutmut_216, 
    'x_analyze_bottlenecks__mutmut_217': x_analyze_bottlenecks__mutmut_217, 
    'x_analyze_bottlenecks__mutmut_218': x_analyze_bottlenecks__mutmut_218, 
    'x_analyze_bottlenecks__mutmut_219': x_analyze_bottlenecks__mutmut_219, 
    'x_analyze_bottlenecks__mutmut_220': x_analyze_bottlenecks__mutmut_220, 
    'x_analyze_bottlenecks__mutmut_221': x_analyze_bottlenecks__mutmut_221, 
    'x_analyze_bottlenecks__mutmut_222': x_analyze_bottlenecks__mutmut_222, 
    'x_analyze_bottlenecks__mutmut_223': x_analyze_bottlenecks__mutmut_223, 
    'x_analyze_bottlenecks__mutmut_224': x_analyze_bottlenecks__mutmut_224, 
    'x_analyze_bottlenecks__mutmut_225': x_analyze_bottlenecks__mutmut_225, 
    'x_analyze_bottlenecks__mutmut_226': x_analyze_bottlenecks__mutmut_226, 
    'x_analyze_bottlenecks__mutmut_227': x_analyze_bottlenecks__mutmut_227, 
    'x_analyze_bottlenecks__mutmut_228': x_analyze_bottlenecks__mutmut_228, 
    'x_analyze_bottlenecks__mutmut_229': x_analyze_bottlenecks__mutmut_229, 
    'x_analyze_bottlenecks__mutmut_230': x_analyze_bottlenecks__mutmut_230, 
    'x_analyze_bottlenecks__mutmut_231': x_analyze_bottlenecks__mutmut_231, 
    'x_analyze_bottlenecks__mutmut_232': x_analyze_bottlenecks__mutmut_232, 
    'x_analyze_bottlenecks__mutmut_233': x_analyze_bottlenecks__mutmut_233, 
    'x_analyze_bottlenecks__mutmut_234': x_analyze_bottlenecks__mutmut_234, 
    'x_analyze_bottlenecks__mutmut_235': x_analyze_bottlenecks__mutmut_235, 
    'x_analyze_bottlenecks__mutmut_236': x_analyze_bottlenecks__mutmut_236, 
    'x_analyze_bottlenecks__mutmut_237': x_analyze_bottlenecks__mutmut_237, 
    'x_analyze_bottlenecks__mutmut_238': x_analyze_bottlenecks__mutmut_238, 
    'x_analyze_bottlenecks__mutmut_239': x_analyze_bottlenecks__mutmut_239, 
    'x_analyze_bottlenecks__mutmut_240': x_analyze_bottlenecks__mutmut_240, 
    'x_analyze_bottlenecks__mutmut_241': x_analyze_bottlenecks__mutmut_241, 
    'x_analyze_bottlenecks__mutmut_242': x_analyze_bottlenecks__mutmut_242, 
    'x_analyze_bottlenecks__mutmut_243': x_analyze_bottlenecks__mutmut_243, 
    'x_analyze_bottlenecks__mutmut_244': x_analyze_bottlenecks__mutmut_244, 
    'x_analyze_bottlenecks__mutmut_245': x_analyze_bottlenecks__mutmut_245, 
    'x_analyze_bottlenecks__mutmut_246': x_analyze_bottlenecks__mutmut_246, 
    'x_analyze_bottlenecks__mutmut_247': x_analyze_bottlenecks__mutmut_247, 
    'x_analyze_bottlenecks__mutmut_248': x_analyze_bottlenecks__mutmut_248, 
    'x_analyze_bottlenecks__mutmut_249': x_analyze_bottlenecks__mutmut_249, 
    'x_analyze_bottlenecks__mutmut_250': x_analyze_bottlenecks__mutmut_250, 
    'x_analyze_bottlenecks__mutmut_251': x_analyze_bottlenecks__mutmut_251, 
    'x_analyze_bottlenecks__mutmut_252': x_analyze_bottlenecks__mutmut_252, 
    'x_analyze_bottlenecks__mutmut_253': x_analyze_bottlenecks__mutmut_253, 
    'x_analyze_bottlenecks__mutmut_254': x_analyze_bottlenecks__mutmut_254, 
    'x_analyze_bottlenecks__mutmut_255': x_analyze_bottlenecks__mutmut_255, 
    'x_analyze_bottlenecks__mutmut_256': x_analyze_bottlenecks__mutmut_256, 
    'x_analyze_bottlenecks__mutmut_257': x_analyze_bottlenecks__mutmut_257, 
    'x_analyze_bottlenecks__mutmut_258': x_analyze_bottlenecks__mutmut_258
}

def analyze_bottlenecks(*args, **kwargs):
    result = _mutmut_trampoline(x_analyze_bottlenecks__mutmut_orig, x_analyze_bottlenecks__mutmut_mutants, args, kwargs)
    return result 

analyze_bottlenecks.__signature__ = _mutmut_signature(x_analyze_bottlenecks__mutmut_orig)
x_analyze_bottlenecks__mutmut_orig.__name__ = 'x_analyze_bottlenecks'


def x_format_bottleneck_report__mutmut_orig(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_1(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = None
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_2(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append(None)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_3(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" / 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_4(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("XX=XX" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_5(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 71)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_6(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append(None)
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_7(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("XXPERFORMANCE BOTTLENECK ANALYSISXX")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_8(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("performance bottleneck analysis")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_9(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append(None)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_10(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" / 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_11(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("XX=XX" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_12(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 71)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_13(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(None)
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_14(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score / 100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_15(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*101:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_16(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score >= 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_17(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 1.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_18(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append(None)
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_19(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("XXStatus: ✅ ExcellentXX")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_20(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("status: ✅ excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_21(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("STATUS: ✅ EXCELLENT")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_22(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score >= 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_23(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 1.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_24(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append(None)
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_25(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("XXStatus: ✓ GoodXX")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_26(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("status: ✓ good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_27(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("STATUS: ✓ GOOD")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_28(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score >= 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_29(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 1.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_30(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append(None)
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_31(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("XXStatus: ⚠ FairXX")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_32(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("status: ⚠ fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_33(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("STATUS: ⚠ FAIR")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_34(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append(None)
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_35(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("XXStatus: ⚠ PoorXX")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_36(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("status: ⚠ poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_37(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("STATUS: ⚠ POOR")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_38(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck == BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_39(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(None)
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_40(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace(None, ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_41(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', None).title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_42(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace(' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_43(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ).title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_44(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('XX_XX', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_45(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', 'XX XX').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_46(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(None)
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_47(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity / 100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_48(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*101:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_49(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append(None)
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_50(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("XX\nContributing Factors:XX")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_51(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\ncontributing factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_52(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nCONTRIBUTING FACTORS:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_53(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:4]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_54(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(None)
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_55(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace(None, ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_56(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', None).title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_57(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace(' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_58(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ).title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_59(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('XX_XX', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_60(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', 'XX XX').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_61(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity / 100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_62(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*101:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_63(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append(None)
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_64(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("XX\nTime Distribution:XX")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_65(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\ntime distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_66(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTIME DISTRIBUTION:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_67(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(None, key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_68(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=None, reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_69(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=None):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_70(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_71(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_72(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], ):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_73(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: None, reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_74(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[2], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_75(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=False):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_76(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = None  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_77(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(None)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_78(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage * 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_79(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 3)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_80(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = None
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_81(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" / bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_82(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "XX█XX" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_83(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(None)
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_84(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append(None)
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_85(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("XX\nRECOMMENDATIONS:XX")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_86(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nrecommendations:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_87(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append(None)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_88(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" / 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_89(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("XX-XX" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_90(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 71)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_91(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(None, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_92(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, None):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_93(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_94(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, ):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_95(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 2):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_96(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(None)
    
    lines.append("\n" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_97(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append(None)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_98(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" - "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_99(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("XX\nXX" + "=" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_100(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" / 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_101(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "XX=XX" * 70)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_102(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 71)
    return "\n".join(lines)


def x_format_bottleneck_report__mutmut_103(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "\n".join(None)


def x_format_bottleneck_report__mutmut_104(analysis: BottleneckAnalysis) -> str:
    """
    Format bottleneck analysis into a human-readable report.
    
    Args:
        analysis: BottleneckAnalysis result
    
    Returns:
        Formatted report string
    """
    lines = []
    lines.append("=" * 70)
    lines.append("PERFORMANCE BOTTLENECK ANALYSIS")
    lines.append("=" * 70)
    
    # Efficiency score
    lines.append(f"\nOverall Efficiency: {analysis.efficiency_score*100:.1f}%")
    if analysis.efficiency_score > 0.8:
        lines.append("Status: ✅ Excellent")
    elif analysis.efficiency_score > 0.6:
        lines.append("Status: ✓ Good")
    elif analysis.efficiency_score > 0.4:
        lines.append("Status: ⚠ Fair")
    else:
        lines.append("Status: ⚠ Poor")
    
    # Primary bottleneck
    if analysis.primary_bottleneck != BottleneckType.NONE:
        lines.append(f"\nPrimary Bottleneck: {analysis.primary_bottleneck.value.replace('_', ' ').title()}")
        lines.append(f"Severity: {analysis.bottleneck_severity*100:.1f}%")
        
        # Contributing factors
        if analysis.contributing_factors:
            lines.append("\nContributing Factors:")
            for bottleneck, severity in analysis.contributing_factors[:3]:  # Top 3
                lines.append(f"  • {bottleneck.value.replace('_', ' ').title()}: {severity*100:.1f}%")
    
    # Overhead breakdown
    if analysis.overhead_breakdown:
        lines.append("\nTime Distribution:")
        for component, percentage in sorted(analysis.overhead_breakdown.items(), key=lambda x: x[1], reverse=True):
            bar_length = int(percentage / 2)  # Scale to 50 chars max
            bar = "█" * bar_length
            lines.append(f"  {component.capitalize():12s} [{bar:<50s}] {percentage:5.1f}%")
    
    # Recommendations
    if analysis.recommendations:
        lines.append("\nRECOMMENDATIONS:")
        lines.append("-" * 70)
        for i, rec in enumerate(analysis.recommendations, 1):
            lines.append(f"\n{i}. {rec}")
    
    lines.append("\n" + "=" * 70)
    return "XX\nXX".join(lines)

x_format_bottleneck_report__mutmut_mutants : ClassVar[MutantDict] = {
'x_format_bottleneck_report__mutmut_1': x_format_bottleneck_report__mutmut_1, 
    'x_format_bottleneck_report__mutmut_2': x_format_bottleneck_report__mutmut_2, 
    'x_format_bottleneck_report__mutmut_3': x_format_bottleneck_report__mutmut_3, 
    'x_format_bottleneck_report__mutmut_4': x_format_bottleneck_report__mutmut_4, 
    'x_format_bottleneck_report__mutmut_5': x_format_bottleneck_report__mutmut_5, 
    'x_format_bottleneck_report__mutmut_6': x_format_bottleneck_report__mutmut_6, 
    'x_format_bottleneck_report__mutmut_7': x_format_bottleneck_report__mutmut_7, 
    'x_format_bottleneck_report__mutmut_8': x_format_bottleneck_report__mutmut_8, 
    'x_format_bottleneck_report__mutmut_9': x_format_bottleneck_report__mutmut_9, 
    'x_format_bottleneck_report__mutmut_10': x_format_bottleneck_report__mutmut_10, 
    'x_format_bottleneck_report__mutmut_11': x_format_bottleneck_report__mutmut_11, 
    'x_format_bottleneck_report__mutmut_12': x_format_bottleneck_report__mutmut_12, 
    'x_format_bottleneck_report__mutmut_13': x_format_bottleneck_report__mutmut_13, 
    'x_format_bottleneck_report__mutmut_14': x_format_bottleneck_report__mutmut_14, 
    'x_format_bottleneck_report__mutmut_15': x_format_bottleneck_report__mutmut_15, 
    'x_format_bottleneck_report__mutmut_16': x_format_bottleneck_report__mutmut_16, 
    'x_format_bottleneck_report__mutmut_17': x_format_bottleneck_report__mutmut_17, 
    'x_format_bottleneck_report__mutmut_18': x_format_bottleneck_report__mutmut_18, 
    'x_format_bottleneck_report__mutmut_19': x_format_bottleneck_report__mutmut_19, 
    'x_format_bottleneck_report__mutmut_20': x_format_bottleneck_report__mutmut_20, 
    'x_format_bottleneck_report__mutmut_21': x_format_bottleneck_report__mutmut_21, 
    'x_format_bottleneck_report__mutmut_22': x_format_bottleneck_report__mutmut_22, 
    'x_format_bottleneck_report__mutmut_23': x_format_bottleneck_report__mutmut_23, 
    'x_format_bottleneck_report__mutmut_24': x_format_bottleneck_report__mutmut_24, 
    'x_format_bottleneck_report__mutmut_25': x_format_bottleneck_report__mutmut_25, 
    'x_format_bottleneck_report__mutmut_26': x_format_bottleneck_report__mutmut_26, 
    'x_format_bottleneck_report__mutmut_27': x_format_bottleneck_report__mutmut_27, 
    'x_format_bottleneck_report__mutmut_28': x_format_bottleneck_report__mutmut_28, 
    'x_format_bottleneck_report__mutmut_29': x_format_bottleneck_report__mutmut_29, 
    'x_format_bottleneck_report__mutmut_30': x_format_bottleneck_report__mutmut_30, 
    'x_format_bottleneck_report__mutmut_31': x_format_bottleneck_report__mutmut_31, 
    'x_format_bottleneck_report__mutmut_32': x_format_bottleneck_report__mutmut_32, 
    'x_format_bottleneck_report__mutmut_33': x_format_bottleneck_report__mutmut_33, 
    'x_format_bottleneck_report__mutmut_34': x_format_bottleneck_report__mutmut_34, 
    'x_format_bottleneck_report__mutmut_35': x_format_bottleneck_report__mutmut_35, 
    'x_format_bottleneck_report__mutmut_36': x_format_bottleneck_report__mutmut_36, 
    'x_format_bottleneck_report__mutmut_37': x_format_bottleneck_report__mutmut_37, 
    'x_format_bottleneck_report__mutmut_38': x_format_bottleneck_report__mutmut_38, 
    'x_format_bottleneck_report__mutmut_39': x_format_bottleneck_report__mutmut_39, 
    'x_format_bottleneck_report__mutmut_40': x_format_bottleneck_report__mutmut_40, 
    'x_format_bottleneck_report__mutmut_41': x_format_bottleneck_report__mutmut_41, 
    'x_format_bottleneck_report__mutmut_42': x_format_bottleneck_report__mutmut_42, 
    'x_format_bottleneck_report__mutmut_43': x_format_bottleneck_report__mutmut_43, 
    'x_format_bottleneck_report__mutmut_44': x_format_bottleneck_report__mutmut_44, 
    'x_format_bottleneck_report__mutmut_45': x_format_bottleneck_report__mutmut_45, 
    'x_format_bottleneck_report__mutmut_46': x_format_bottleneck_report__mutmut_46, 
    'x_format_bottleneck_report__mutmut_47': x_format_bottleneck_report__mutmut_47, 
    'x_format_bottleneck_report__mutmut_48': x_format_bottleneck_report__mutmut_48, 
    'x_format_bottleneck_report__mutmut_49': x_format_bottleneck_report__mutmut_49, 
    'x_format_bottleneck_report__mutmut_50': x_format_bottleneck_report__mutmut_50, 
    'x_format_bottleneck_report__mutmut_51': x_format_bottleneck_report__mutmut_51, 
    'x_format_bottleneck_report__mutmut_52': x_format_bottleneck_report__mutmut_52, 
    'x_format_bottleneck_report__mutmut_53': x_format_bottleneck_report__mutmut_53, 
    'x_format_bottleneck_report__mutmut_54': x_format_bottleneck_report__mutmut_54, 
    'x_format_bottleneck_report__mutmut_55': x_format_bottleneck_report__mutmut_55, 
    'x_format_bottleneck_report__mutmut_56': x_format_bottleneck_report__mutmut_56, 
    'x_format_bottleneck_report__mutmut_57': x_format_bottleneck_report__mutmut_57, 
    'x_format_bottleneck_report__mutmut_58': x_format_bottleneck_report__mutmut_58, 
    'x_format_bottleneck_report__mutmut_59': x_format_bottleneck_report__mutmut_59, 
    'x_format_bottleneck_report__mutmut_60': x_format_bottleneck_report__mutmut_60, 
    'x_format_bottleneck_report__mutmut_61': x_format_bottleneck_report__mutmut_61, 
    'x_format_bottleneck_report__mutmut_62': x_format_bottleneck_report__mutmut_62, 
    'x_format_bottleneck_report__mutmut_63': x_format_bottleneck_report__mutmut_63, 
    'x_format_bottleneck_report__mutmut_64': x_format_bottleneck_report__mutmut_64, 
    'x_format_bottleneck_report__mutmut_65': x_format_bottleneck_report__mutmut_65, 
    'x_format_bottleneck_report__mutmut_66': x_format_bottleneck_report__mutmut_66, 
    'x_format_bottleneck_report__mutmut_67': x_format_bottleneck_report__mutmut_67, 
    'x_format_bottleneck_report__mutmut_68': x_format_bottleneck_report__mutmut_68, 
    'x_format_bottleneck_report__mutmut_69': x_format_bottleneck_report__mutmut_69, 
    'x_format_bottleneck_report__mutmut_70': x_format_bottleneck_report__mutmut_70, 
    'x_format_bottleneck_report__mutmut_71': x_format_bottleneck_report__mutmut_71, 
    'x_format_bottleneck_report__mutmut_72': x_format_bottleneck_report__mutmut_72, 
    'x_format_bottleneck_report__mutmut_73': x_format_bottleneck_report__mutmut_73, 
    'x_format_bottleneck_report__mutmut_74': x_format_bottleneck_report__mutmut_74, 
    'x_format_bottleneck_report__mutmut_75': x_format_bottleneck_report__mutmut_75, 
    'x_format_bottleneck_report__mutmut_76': x_format_bottleneck_report__mutmut_76, 
    'x_format_bottleneck_report__mutmut_77': x_format_bottleneck_report__mutmut_77, 
    'x_format_bottleneck_report__mutmut_78': x_format_bottleneck_report__mutmut_78, 
    'x_format_bottleneck_report__mutmut_79': x_format_bottleneck_report__mutmut_79, 
    'x_format_bottleneck_report__mutmut_80': x_format_bottleneck_report__mutmut_80, 
    'x_format_bottleneck_report__mutmut_81': x_format_bottleneck_report__mutmut_81, 
    'x_format_bottleneck_report__mutmut_82': x_format_bottleneck_report__mutmut_82, 
    'x_format_bottleneck_report__mutmut_83': x_format_bottleneck_report__mutmut_83, 
    'x_format_bottleneck_report__mutmut_84': x_format_bottleneck_report__mutmut_84, 
    'x_format_bottleneck_report__mutmut_85': x_format_bottleneck_report__mutmut_85, 
    'x_format_bottleneck_report__mutmut_86': x_format_bottleneck_report__mutmut_86, 
    'x_format_bottleneck_report__mutmut_87': x_format_bottleneck_report__mutmut_87, 
    'x_format_bottleneck_report__mutmut_88': x_format_bottleneck_report__mutmut_88, 
    'x_format_bottleneck_report__mutmut_89': x_format_bottleneck_report__mutmut_89, 
    'x_format_bottleneck_report__mutmut_90': x_format_bottleneck_report__mutmut_90, 
    'x_format_bottleneck_report__mutmut_91': x_format_bottleneck_report__mutmut_91, 
    'x_format_bottleneck_report__mutmut_92': x_format_bottleneck_report__mutmut_92, 
    'x_format_bottleneck_report__mutmut_93': x_format_bottleneck_report__mutmut_93, 
    'x_format_bottleneck_report__mutmut_94': x_format_bottleneck_report__mutmut_94, 
    'x_format_bottleneck_report__mutmut_95': x_format_bottleneck_report__mutmut_95, 
    'x_format_bottleneck_report__mutmut_96': x_format_bottleneck_report__mutmut_96, 
    'x_format_bottleneck_report__mutmut_97': x_format_bottleneck_report__mutmut_97, 
    'x_format_bottleneck_report__mutmut_98': x_format_bottleneck_report__mutmut_98, 
    'x_format_bottleneck_report__mutmut_99': x_format_bottleneck_report__mutmut_99, 
    'x_format_bottleneck_report__mutmut_100': x_format_bottleneck_report__mutmut_100, 
    'x_format_bottleneck_report__mutmut_101': x_format_bottleneck_report__mutmut_101, 
    'x_format_bottleneck_report__mutmut_102': x_format_bottleneck_report__mutmut_102, 
    'x_format_bottleneck_report__mutmut_103': x_format_bottleneck_report__mutmut_103, 
    'x_format_bottleneck_report__mutmut_104': x_format_bottleneck_report__mutmut_104
}

def format_bottleneck_report(*args, **kwargs):
    result = _mutmut_trampoline(x_format_bottleneck_report__mutmut_orig, x_format_bottleneck_report__mutmut_mutants, args, kwargs)
    return result 

format_bottleneck_report.__signature__ = _mutmut_signature(x_format_bottleneck_report__mutmut_orig)
x_format_bottleneck_report__mutmut_orig.__name__ = 'x_format_bottleneck_report'
